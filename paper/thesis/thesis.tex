% Thesis guidelines:
% - 40 pages including graphs, graphics and pictures, TOC/appendix/bibliography not counted
% - 1.5 line-spacing
\documentclass[12pt,a4paper,english]{article} %document type and language

\usepackage[onehalfspacing]{setspace}
% \linespread{1.5}

\usepackage[utf8]{inputenc}	% set character set to support some UTF-8
\usepackage{babel} 	% multi-language support
% \usepackage{sectsty}	% allow redefinition of section command formatting
\usepackage{tabularx}	% more table options
\usepackage{titling}	% allow redefinition of title formatting
\usepackage{imakeidx}	% create and index of words
\usepackage{xcolor}	% more color options
\usepackage{enumitem}	% more list formatting options
\usepackage{tocloft}	% redefine table of contents, new list like objects

\usepackage[centering,noheadfoot,left=3cm, right=2cm, top=2cm, bottom=2cm]{geometry}

%set TOC margins
\setlength{\cftbeforesecskip}{15pt} % skip in TOC

% remove paragraph white space and modify space between list items
\usepackage{parskip}

% Set font globally
\usepackage{lmodern}                % load Latin modern fonts
\usepackage[defaultsans]{cantarell} % cantarell fonts

% HACK: https://tex.stackexchange.com/questions/58087/how-to-remove-the-warnings-font-shape-ot1-cmss-m-n-in-size-4-not-available
\usepackage{anyfontsize}

% set LaTeX global font
\renewcommand{\familydefault}{\sfdefault}
\renewcommand{\sfdefault}{lmss}

% set styling headings
%\allsectionsfont{\usefont{OT1}{phi}{b}{n}}

\usepackage{float} 	% floats
\usepackage{graphicx}	% Graphics
\usepackage{amsmath}	% extensive math options
\usepackage{amssymb}	% special math symbols
\usepackage[Gray,squaren,thinqspace,thinspace]{SIunits} % elegant units
\usepackage{listings}                                   % source code

% Custom Operators
%% Expectation symbol
\DeclareMathOperator*{\E}{\mathbb{E}}
\DeclareMathOperator*{\Cov}{\mathrm{Cov}}
\DeclareMathOperator*{\Var}{\mathrm{Var}}

% missing math commands
\providecommand{\abs}[1]{\left\lvert#1\right\rvert}                    % |.|
\providecommand{\br}[1]{\left(#1\right)}                               % (.)
\providecommand{\sbr}[1]{\left[#1\right]}                              % [.]
\providecommand{\ddfrac}[2]{\frac{\displaystyle #1}{\displaystyle #2}}
% use \math rm{d} to include math differential

% independence symbol
% https://tex.stackexchange.com/questions/79434/double-perpendicular-symbol-for-independence
\newcommand{\indep}{\perp\!\!\!\!\perp}


% options for listings
\lstset{
  breaklines=true,
  postbreak=\raisebox{0ex}[0ex][0ex]{\ensuremath{\color{red}\hookrightarrow\space}},
  numbers=left,
  numbersep=5pt,
  numberstyle=\tiny\color{gray},
  basicstyle=\footnotesize\ttfamily
}

% NEEDS to be before hyperref, cleveref and autonum
% number figures, tables and equations within the sections
\numberwithin{equation}{section}
\numberwithin{figure}{section}
\numberwithin{table}{section}

% references and annotation, citations
\usepackage[small,bf,hang]{caption}        % captions
\usepackage{subcaption}                    % adds sub figure & sub caption
\usepackage{sidecap}                       % adds side captions
\usepackage{hyperref}                      % add hyperlinks to references
\usepackage[noabbrev,nameinlink]{cleveref} % better references than default~\ref
% Hack:https://tex.stackexchange.com/questions/285950/package-autonum-needs-the-obsolete-etex-package
\expandafter\def\csname ver@etex.sty\endcsname{3000/12/31}
\let\globcount\newcount
% Deactivate for now to avoid issues with equation* environment.
% \usepackage{autonum}                       % only number referenced equations
\usepackage{url}                           % URLs
\usepackage{cite}                          % well formed numeric citations

% biblatex for references
% \usepackage{biblatex}
% \addbibresource{literature.bib}
% csquotes recommended: https://tex.stackexchange.com/questions/229638/package-biblatex-warning-babel-polyglossia-detected-but-csquotes-missing
% \usepackage{csquotes}

% format hyperlinks
\colorlet{linkcolour}{black}
\colorlet{urlcolour}{blue}
\hypersetup{colorlinks=true,
            linkcolor=linkcolour,
            citecolor=linkcolour,
            urlcolor=urlcolour}

%\usepackage{todonotes} % add to do notes
\usepackage{epstopdf}  % process eps-images
\usepackage{float}     % floats
\usepackage{fancyhdr}  % header and footer
% HACK: https://tex.stackexchange.com/questions/664532/fancyhr-warning-footskip-is-too-small
\setlength{\footskip}{15pt}

% default path for figures
\graphicspath{{figures/}}

% If we have multiple directories, specify them like this: \graphicspath{{figures_ch1/}{figures_ch2/}}.

% For rendering tikz
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{decorations.pathreplacing} % Load the library for drawing braces


% Define some math environments
% \usepackage{amsthm}

% \newtheorem{theorem}{Theorem}[section]
% \newtheorem{corollary}{Corollary}[theorem]
% \newtheorem{lemma}[theorem]{Lemma}

% \theoremstyle{definition}
% \newtheorem{definition}{Definition}[section]

% \theoremstyle{remark}
% \newtheorem*{remark}{Remark}

% https://tex.stackexchange.com/questions/24840/use-courier-font-inline-on-text
\usepackage{courier}

% https://tex.stackexchange.com/questions/639234/how-to-put-braces-over-certain-parts-of-matrices
\usepackage{nicematrix}


% set header and footer
\pagestyle{fancy}
\fancyhf{}                           % clear all header and footer fields
\cfoot{\thepage}                     % add page number
\renewcommand{\headrulewidth}{0pt} % add horizontal line of 0.4pt thick

\title{Statistical Inference in Partially Identified Marginal Treatment Effect Models}
\author{Julian Budde}
\date{\today}
\begin{document}

\maketitle

\begin{abstract}
	I study inference in the marginal treatment effect model when used to partially-identify a target parameter of interest.
	First, I demonstrate the invalidity of the bootstrap in a simple, but common binary IV model.
	In specific settings, the bounds of the identified set have an easy to calculate solution.
	These solutions exhibits points of non-differentiability when viewed as functions of the point-identified parameters rendering the nonparametric bootstrap invalid.
	Second, I discuss potential solutions proposed in the literature and show how they might be adjusted to typical use-cases where bounds have to be calculated by the means of linear programming.
	Third, for a number of empirically relevant settings I conduct novel simulation studies for the MTE model comparing competing inference methods.
\end{abstract}

\clearpage
\newpage

\tableofcontents

\clearpage
\newpage

\listoftables

\clearpage
\newpage

\listoffigures

\clearpage
\newpage

\section{Introduction}

\section{Bootstrap Invalidity: A Simple Example}
I first illustrate the invalidity of the bootstrap in a simple example with a binary instrument where we only use the point-identified LATE for the instrument-compliers as the basis for extrapolation.
While this will not best we can do in this case --- there are more moments in the data we can point-identify --- it helps to illustrate the type of inference issues that can also arise in more complex setups.
In particular, we can calculate explicit solutions for the upper and lower as functions of the point-identified complier LATE.\@

\subsection{Setup}
We first introduce some basic notation and key ingredients of the MTE model.
A complete statement of the model corresponding to~\cite{mogstad2018using} can be found in Section~\ref{sec:general_mte}.

The outcome is denoty $Y_i$ and has bounded support, which we generally assume to be in $[0,1]$ for convenience.
The instrument $Z_i$ is binary with realizatons in $\{0,1\}$.
Binary treatment $D_i$ is determined by a selection model with
\begin{equation}\label{eq:treatment}
  D_i = I\{p(Z_i) \geq U_i\}
\end{equation}
where unobserved heterogeneity $U_i$ is normalized to follow a $U(0,1)$ distribution, $U_i \indep Z_i$ and $I\{A\}$ denotes the indicator function for event $A$.
Hence, $p(z) \equiv P(D_i=1|Z_i=z)$ is the propensity score. We define the instrument $Z_i$ such that $p(0)<p(1)$.

As shown by~\cite{vytlacil2002independence}, under an appropriate set of assumptions stated more formally in Section~\ref{sec:general_mte}, this setup is equivalent to the potential outcome framework in~\cite{imbens_angrist1994ecma}.
Hence, we can point-identify a local average treatment effect for the instrment-compliers, namely those individuals, who take up the treatment if and only if $Z_i=1$.
In the selection model above, the complier sub-population is those with $U_i$ realizations in $[p(0), p(1)]$.
Hence, in the notation of the model, we can point-identify,
\begin{equation}
  \beta_s = \E[Y(1) - Y(0)|U_i\in[p(0), p(1)]].
\end{equation}
Note that individuals with realizations $u \leq p(0)$ will always select into treatment, while individuals with $u \geq p(1)$ will never select into treatment.
To keep the notation light, I will refers to these subpopulations as \textit{complier}, \textit{never-taker} and \textit{always-taker}.

Our goal is to extrapolate from this identified complier-LATE to a larger subpopulation that also includes some never-takers.
[Example for interpretation]. Now we are interested in a policy-change which we would believe to increase the propensity score.
We denote this new target by
\begin{equation}
  \beta^* = \E[Y(1) - Y(0)|U_i \in[p(0), p(1) + \overline{u}]],
\end{equation}
where $\overline{u}\in[0, 1-p(1)]$.

Without very strong restrictions, $\beta^*$ will not be point-identified.
Instead, we will only generally be able to identify a set of values for the target that are both consistent with the data and theoretical assumptions we want to impose.
Usually, this \textit{identified set} will be a closed interval on the real line and we will denote it by
\begin{equation*}
  \beta^* \in [\underline{\beta^*}(\beta_s), \overline{\beta^*}(\beta_s)],
\end{equation*}
where we emphasize the dependence of the upper and lower bound of the identified set on the point-identified parameter $\beta_s$.

\subsection{Solution: Non-parametric Bounds}
While the identified set is usually computed via a linear program, in the simple case considered here with a single point-identified parameter the identified set has an intuitive and easy to compute form.
Note the following solutions are referred to as \textit{non-parametric} in~\cite{mogstad2018using}, that is, they do not impose any parametric assumptions on the underlying MTR functions.
The corresponding linear program solution can be achieved by using constant splines defined on the grid $[0, p(0), p(1), p(1) + \overline{u}, 1]$.

Denote by $\beta_{\overline{u}}$ the LATE for the population of never-takers for which we want to extend the treatment, i.e.
\begin{equation*}
  \beta_{\overline{u}} \equiv E[Y(1) - Y(0) | u \in [p(1), p(1) + \overline{u}]].
\end{equation*}
Then $\beta^*$ is a weighted average of the two LATES:\@
\begin{equation*}
  \beta^* = \omega\beta_s + (1-\omega)\beta_{\overline{u}}
\end{equation*}
where $\omega = \frac{p(1) - p(0)}{\overline{u} + p(1) - p(0)}$ is the relative share of compliers in the target population.
Finding the identified set then amounts to finding bounds on $\beta_{\overline{u}}$, where different restrictions imply different bounds.



\paragraph{Non-parametric Bounds}
Without any further restrictions, the only bounds we can give follow from the support of $Y_i$.
Since $Y_i\in[0,1]$ we have $\beta_{\overline{u}} \in [0,1]$ and hence
\begin{equation*}
  \beta^* \in [\omega\beta_s - (1 - \omega), \omega\beta_s + (1 - \omega)]
\end{equation*}
We can already see that this is a continuous and differentiable function of $\beta_s$.
Hence, if estimation for $\beta_s$ and $\omega$ is standard, the usual delta method could be applied.
This is the case here, since $\beta_s$ can be consistently estimated using the Wald estimator which --- assuming no weak identification issues --- is asymptotically normal.
$\omega$ is a function of sample moments and can be estimated consistently.
Therefore, also the non-parametric bootstrap is valid in this case.
\footnote{The only issue would arise at the boundary of the parameter space, that is when $\underline{\beta^*}(\beta_s)=-1$ or $\overline{\beta^*}(\beta_s)=1$.
These cases are of less interest practically, hence I disregard them. However, a similar issue arises under shape restrictions discussed below.}

\paragraph{Monotone Marginal Treatment Effect}
We can impose the assumption, that the treatment effects $E[Y(1) - Y(0)|U_i=u]$ are monotone in $u$.
In the language of~\cite{mogstad2018using}, the MTE function is monotone.
The most interesting assumption is a \textit{decreasing} MTE function, since treatment take-up is decreasing in $u$.

If the MTE function is decreasing, we know $\beta_{\overline{u}} \leq \beta_s$ and hence the identified set shrinks at the upper bound:
\begin{equation*}
  \beta^* \in [\omega\beta_s - (1 - \omega), \beta_s]
\end{equation*}
In this case, inference is again standard. Note we have point identification when $\beta_s=-1$.
\footnote{Convergence to point-identification can pose problems to the type of confidence intervals considered by~\cite{imbens2004confidence} and considered later here.
In particular, coverage does not hold uniformly over parameter sequence implying convergence to point-identification.
While~\cite{imbens2004confidence} propose a fix to this, later on I focus on cases ``far enough'' away from point-identification.}


\paragraph{Monotone Treatment Response}
Under monotone treatment response, it is assumed that all individuals respond either negative or positive to the treatment.
An implication for the marginal treatmnt effect is that it is either positive or negative everywhere.
This again tightens the set, but it also restrict the parameter space of $\beta_s$ that is consistent with the model.
In particular, any $\beta_s \leq 0$ is inconsistent with the assumption of positive treatment responses.
In addition, the lower bound will be tighter:
\begin{equation*}
  \beta^* \in \begin{cases}
    [\omega\beta_s, \beta_s + (1 - \omega)] & \beta_s \geq 0 \\
    \emptyset & \beta_s < 0
  \end{cases}
\end{equation*}
Here, inference becomes non-standard at the boundary of the parameter space.
This case is close to the stylized example analyzed in~\cite{andrews1999estimation}, where interest lies in estimating an expectation which is known to be positive.
[Explain, why not focusing on this in the following.]

\paragraph{Decreasing Marginal Treatment Responses}
A last set of restrictions considered by~\cite{mogstad2018using} are monotonocity restrictions on the MTR functions $m_d(u) = E[Y(d)|U=u]$.
We consider the case of decreasing MTR functions also considered in their paper.
Imposing restrictions on these functions is also the general approach discussed in Section~\ref{sec:general_mte}, since this allows to exploit other moments for identification.
For example, in our model also $E[Y(1)|\text{always-taker}]$ and $E[Y(0)|\text{never-taker}]$ are identified and can help in tightening the identified set.

While generally not impossible, with the restriction that $m_0(u), m_1(u)$ are decreasing in $u$, the computation of the identified set becomes less immediate.
Figure [REF] illustrates the solution using constant spline basic functions defined separately over the intervals implied by $[0, p(0), p(1), p(1) + \overline{u}, 1]$.
Essentially, since both the identified and target parameter are averages over the MTR (or MTE) function with constant weights on each subinterval, taking constant splines on these subintervals as basis functions does not impose any restrictions.

The solution in this case is given by:
\begin{equation}
	\overline{\beta^*}(\beta_s)=
	\begin{cases}
		\omega \beta_s + (1 - \omega),& \quad \text{if } \beta_s \geq 0\\
		\beta_s + (1 - \omega),              & \quad \text{if } \beta_s < 0,
	\end{cases}
\end{equation}
and
\begin{equation}
	\underline{\beta^*}(\beta_s)=
	\begin{cases}
		\beta_s - (1 - \omega),& \quad \text{if } \beta_s \geq 0\\
		\omega \beta_s - (1 - \omega),              & \quad \text{if } \beta_s < 0.
	\end{cases}
\end{equation}

Hence, both bounds exhibit a kink at $\beta_s=0$.
In these cases, as was shown in~\cite{dumbgen1993nondifferentiable} and later~\cite{fang2019infdirdiff}, non-differentiability at the kink renders the non-parametric bootstrap invalid.
However, other methods have been deviced for this case. For example,~\cite{fang2019infdirdiff} and~\cite{hong2018numerical} extend the delta method to the case of functions that are only directionally differentiable, such as the ones with a kink shown above.
Further, subsampling is theoretically justified as long as the bounds of the identified set have \textit{any} limiting distribution.
I consider these methods as well as the non-parametric bootstrap in more detail in Section~\ref{sec:inference_methods} and provide simulation evidence in a realistic setting in Section~\ref{sec:simulation_studies}.

\subsection{Solutions}

\paragraph{Problem I:\@ Non-differentiability}

\paragraph{Problem II:\@ Parameter at the Boundary}

\section{General MTE Setup}\label{sec:general_mte}

\subsection{Setup}

\subsection{Linear Program}

\subsection{Restrictions}

\section{Inference Methods}\label{sec:inference_methods}

\subsection{Resampling Methods: Non-parametric Bootstrap and Subsampling}

\subsection{Confidence Intervals}

\subsection{Adjusted Delta Methods}

\paragraph{Analytical Delta Method}

\paragraph{Numerical Delta Method}

\section{Simulation Studies}\label{sec:simulation_studies}

\subsection{Model Setting}

\subsection{Simulation Design}

\subsection{Simulation Results}


\bibliographystyle{plain}
\bibliography{refs.bib}

\appendix

\section{Linear Programs}
This section presents the exact formulation of the linear programs corresponding to how most solvers specify linear programs.
I illustrate the actual programs in the case of binary IV model with an identified complier LATE/IV slope coefficient.
[Add programs for sharp identified set?]

\subsection{Identification}

The program for the upper bound is formulated as follows in~\cite{mogstad2018using}:
\begin{align}
  \overline{\beta}^* \equiv \sup_{(\theta_0, \theta_1)\in\Theta} \sum_{k=1}^{K_0}\theta_{0k}\Gamma^*_0(b_{0k}) + \sum_{k=1}^{K_1}\theta_{1k}\Gamma^*_1(b_{1k}) \\
  \text{subject to} \qquad \sum_{k=1}^{K_0}\theta_{0k}\Gamma_{0s}(b_{0k}) + \sum_{k=1}^{K_1}\theta_{1k}\Gamma_{1s}(b_{1k}) = \beta_s \text{ for } s \in \mathcal{S}.
\end{align}

In words: We maximize the target parameter, where we maximize over a basis function approximation with coefficients in a set $\Theta$.
The first summation term corresponds to $m_0$, the second to $m_1$.
We can separately parametrize $m_0$ and $m_1$ as reflected by the potentially different number of basis coefficients $K_0$ and $K_1$.
$\Theta$ can implicitly incorporate other constraints, like shape restrictions on $m_0, m_1$ which will be made explicit below.
It will at least entail the restrictions that $m_0, m_1$ are bounded by the support of the outcome $Y$, which we take to be $[0,1]$ without loss of generality.

Explicitly stated in the second line are the restrictions that for each point-identified estimand $s\in\mathcal{S}$ the maximizers $(\theta_0, \theta_1)$ must recover the point-identified parameter.

\paragraph{Matrix Form}
I next explicitly define the matrices typically used to describe a linear program.
In the so-called \textit{standard form}, all linear programs are described by:
(1) A \textit{maximization} objective; (2) inequality constraints with upper bounds; (3) non-negativity constraints on the choice variables.
Hence, we have:

\begin{align}
  \max_x c'x \\
  \text{ subject to } Ax\leq b, x\geq0
\end{align}

To keep the notation lighter, I instead reformulate the program in terms of:
(1) A \textit{minimization} objective; (2) inequality constraints with upper bounds; (3) equality constraints; (4) lower and upper bounds on the choice variables.
In this case, the program is:

\begin{align}
  \min_x c'x \\
  \text{ subject to } A_{ub}x \leq b_{ub}, A_{eq} = b_{eq}, l\leq x\leq u.
\end{align}

This is, for example, how the linear program is inputted into the \texttt{scipy} \texttt{linprog} solver.
The program can easily be transformed to standard form by: (1) taking the negative of the objective, (2) reformulating equalities as two inequalities, and (3) expressing variables without positivity constraints as differences of two non-negative variables.
\texttt{Scipy}, for example, performs these operations in addition to other pre-solve checks for potential simplifications in the background.
It then passes the standard form program to a solver and post-processes the result to a solution of the original problem.

For simplicity, in the following we take $K_0 = K_1 = K$. This is the case for non-parametric exact bounds using constant splines and also parametric cases there seems little reason to have different numbers of basis polynomials.
Further, it simplifies the formulation of shape restrictions.

The choice variables are the basis function coefficients
\begin{equation*}
  x =
  \begin{bmatrix}
     \theta_{01} & \cdots & \theta_{0K} & \theta_{11} & \cdots & \theta_{1K}
  \end{bmatrix}',
\end{equation*}
and the coefficients are the individual contributions to the target parameter
\begin{equation*}
  c =
  \begin{bmatrix}
     \Gamma_0^*(b_{01}) & \cdots & \Gamma_0^*(b_{0K}) & \Gamma_1^*(b_{11}) & \cdots & \Gamma_1^*(b_{1K})
  \end{bmatrix}'.
\end{equation*}
Both of these are vectors of length $2K$.

Next, we have an equality constraint for each point-identified estimand $s\in\mathcal{S}$.
\begin{equation*}
  a_{eq}^s =
  \begin{bmatrix}
     \Gamma_{0s}(b_{01}) & \cdots & \Gamma_{0s}(b_{0K}) & \Gamma_{1s}(b_{11}) & \cdots & \Gamma_{1s}(b_{1K})
  \end{bmatrix}.
\end{equation*}
Note that we changed the map from $\Gamma^*_d$ to $\Gamma_{ds}$. Stacking these row-vectors gives the equality constraint matrix:
\begin{equation*}
  A_{eq}^s =
  \begin{bmatrix}
    a_{eq}^1 \\
    \vdots \\
    a_{eq}^\mathcal{S} \\
  \end{bmatrix},
\end{equation*}
which is a matrix of dimension $(S\times 2K)$, where, for brevity, we denote the \textit{number} of identified estimands $|\mathcal{S}| = S$.
The constraint values are given by the point-identified estimands:
\begin{equation*}
  b_{eq} =
  \begin{bmatrix}
    \beta_1, \ldots, \beta_\mathcal{S}
  \end{bmatrix}'.
\end{equation*}


A set of restrictions to achieve a minimal degree of identification is $y_l \leq m_d \leq y_d$ for $d=0,1$ and $y_l, y_u$ the lower and uppre bound of the support of $Y$.
Taking $y_l = 0, y_u=1$, for typical shape constraints --- constant splines or Bernstein polynomials --- this amounts to the restrictions $0\leq \theta_{dj} \leq 1$ for $d=0,1, j=1,\ldots,K$.
Hence, we have the restrictions
\begin{equation*}
  l = \mathbf{0} \leq x \leq \mathbf{1} = u,
\end{equation*}
where $l$ and $u$ are vectors of length $2K$ each.

Finally, a number of shape restrictions may be added in the form of inequality bounds $A_{ub} \leq b_{ub}$.
The following applies to constant splines and Bernstein polynomials, but might not hold for all possible types of basis functions.

\paragraph{MTR Monotonicity}
To enforce a \textit{decreasing} MTR function $m_d(u)$ for constant splines, it is immediate that $\theta_{d1} \geq \theta_{d2} \geq \cdots \geq \theta_{dK}$, given that the functions are ordered from left to right over the partition of $u$, which we always assume to be the case.
For Bernstein polynomials this also holds, since they obey a monotonicity property: Whenever the basis coefficients $\theta_{dk}$ are decreasing over $k$, so is the resulting Bernstein polynomial (and the same for increasing).

Hence, to enforce a decreasing MTR function for $d=0$ we add the following constraints:
% \begin{equation*}
%   A_{eq}^s =
%   \begin{bmatrix}
%     -1 & 1 & 0 & \cdots & 0 & 0\\
%     0 & -1 & 1 & \cdots & 0 & 0\\
%     \vdots & & & \vdots \\
%     0 & 0 & 0 & \cdots & -1 & 1\\
%   \end{bmatrix} + \mathbf{0}
% \end{equation*}

\begin{equation*}
  A_{ub} =
  \begin{bNiceArray}{ccccccc}
    -1 & 1 & 0 & \cdots & 0 & 0 & \Block{4-1}<\Large>{\mathbf{0}}\\
    0 & -1 & 1 & \cdots & 0 & 0\\
    \vdots & & & \vdots \\
    0 & 0 & 0 & \cdots & -1 & 1\\
  \end{bNiceArray} \leq \mathbf{0} = b_{ub}.
\end{equation*}
Here, the first block is of dimension $(K-1) \times K$ reflecting the inequalities on the basis functions for $m_0(u)$.
The second block are zeros of dimension $(K-1) \times K$ on the basis functions for $m_1(u)$.
To add a similar restrictions on $m_1(u)$ we can switch the blocks.
To enforce \textit{increasing} MTR functions $A_{ub}$ above can be multiplied by $-1$ to flip the inequalities.
$b_{ub}$ has length $(K-1)$.

\paragraph{MTE Monotonicity}
To enforce a \textit{decreasing} MTE function (the difference $m_1(u) - m_0(u)$) we require
$\theta_{11} - \theta_{01} \geq \theta_{12} - \theta_{02} \geq \cdots \geq \theta_{1K} - \theta_{0K}$.
For both constant splines and Bernstein polynomials this is immediate, since the difference $m_1(u) - m_0(u)$ implies a set of basis functions with the above differences as coefficients, which, hence, obey the same monotonicity conditions.

\begin{equation*}
  A_{ub} =
  \begin{bmatrix}
    \ldots
  \end{bmatrix} \leq \mathbf{0} = b_{ub}
\end{equation*}

\paragraph{Monotone Treatment Response}
Monotone treatment response requires $m_1(u) - m_0(u) \geq 0$ (positive) or $m_1(u) - m_0(u) \leq 0$ (negative) for all $u$.
Hence, we require $\theta_{1j} - \theta_{0j} \geq 0$ for all $j=1,\ldots,K$ for positive responses and reversed for negative.
\begin{equation*}
  A_{ub} =
  \begin{bmatrix}
    \ldots
  \end{bmatrix} \leq \mathbf{0} = b_{ub}
\end{equation*}

\subsection{Estimation}
Estimation is more involved and features solving two linear programs.
~\cite{mogstad2018using} propose a two-step estimator.
The reason is, that in sample we might not always expect all equality constraints for the identified estimands to be satisfied.
To allow for some sampling uncertainty, in a first step we compute the minimal achievable deviations from all point-identified estimands (as measured by the $l_1$ norm).
In the second step, we then restrict the set of allowed MTR functions to maximize over to those achieving this minimal deviation plus some tolerance.
In this way, the program always has a solution, meaning our estimators always exists.

This approach is stated in equation (27) of~\cite{mogstad2018using}:
\begin{align}
  & \hat{\overline{\beta}^*} \equiv \sup_{m\in \mathcal{M}}\hat{\Gamma}^*(m) \\
  & \text{ subject to } \sum_{s\in\mathcal{S}}|\hat{\Gamma}_s(m) - \hat{\beta}_s| \leq \inf_{m'\in \mathcal{M}} \sum_{s\in\mathcal{S}}|\hat{\Gamma}_s(m') - \hat{\beta}_s| + \kappa_n
\end{align}

Note we have a single constraint for the $l_1$ norm.
The $\inf$ on the right-hand side is the first-step linear program and we allow for tolerance $\kappa_n$ to enlarge the set of possible solutions.

\subsubsection{First Step Linear Program}
The first step program is given by
\begin{equation*}
  \inf_{m'\in \mathcal{M}} \sum_{s\in\mathcal{S}}|\hat{\Gamma}_s(m') - \hat{\beta}_s|
\end{equation*}

We again rewrite it in the matrix form stated above.
Note that $\mathcal{M}$ again implicitly encodes any bounds and other shape restrictions on the MTR functions.
Further, as pointed out in~\cite{mogstad2018using}, we need to introduce dummy variables to mimic the absolute values in the objective.

\paragraph{Absolute Values in the Objective}
With $S = 1$, that is a single point-identified estimand, we have an objective of the following form:
\begin{equation*}
  \inf_m |X_1|
\end{equation*}

where $X_1 = \hat{\Gamma}_1(m) - \hat{\beta}_1$.
We now add a dummy variable $X_1'$ and two constraints to the model:
\begin{align*}
  X_1 \leq X_1', \\
  -X_1 \leq X_1'.
\end{align*}
In the objective we replace the absolute value with the dummy:
\begin{equation*}
  \inf X_1'.
\end{equation*}
By going through all possible cases ($X_1=0, X_1<0, X_1>0$) it is easy to show,
that $X_1'$ will behave like $|X_1|$.

Having $|S|>1$ is then simply a matter of adding another dummy and two constraints per point-identified estimand.

Plugging in the original expression for $X_1$ we get
\begin{align*}
  \inf X_1' \\
  \text{ subject to }\\
  \hat{\Gamma}_1(m) - \hat{\beta}_1 \leq X_1', \\
  -(\hat{\Gamma}_1(m) - \hat{\beta}_1) \leq X_1',
\end{align*}
where minimization is now over the basis coefficients (appearing in the constraints) and the dummy variables (appearing in the objective).
Hence, we now have $2K + S$ choice variables.

The vector of choice variables is given by
\begin{equation*}
  x^{fs} =
  \begin{bmatrix}
     \theta_{01} & \cdots & \theta_{0K} & \theta_{11} & \cdots & \theta_{1K} & X_1' & \cdots & X_{S}
  \end{bmatrix}',
\end{equation*}
with corresponding coefficients

\begin{equation*}
  c^{fs} =
  \begin{bmatrix}
     \mathbf{0}', \mathbf{0}', \mathbf{1}'
  \end{bmatrix}',
\end{equation*}
where the zeros on the basis functions are of length $K$ each and the $1$ on the dummies of length $S$.
Both of these are vectors of length $2K + S$.


To encode the first type of inequality for all $S$ we get the following inequality constraint matrix:
\begin{equation*}
  A_{ub, 1}^{fs} =
  \begin{bmatrix}
    \hat{\Gamma}_{01}(b_{01}) & \cdots & \hat{\Gamma}_{01}(b_{0K}) & \hat{\Gamma}_{11}(b_{11}) & \cdots & \hat{\Gamma}_{11}(b_{1K}) & -1 & 0 & \cdots & 0 \\
    \vdots & & \vdots & \vdots &  & \vdots & \\
    \hat{\Gamma}_{0S}(b_{01}) & \cdots & \hat{\Gamma}_{0S}(b_{0K}) & \hat{\Gamma}_{1S}(b_{11}) & \cdots & \hat{\Gamma}_{1S}(b_{1K}) & 0 & 0 & \cdots & -1 \\
  \end{bmatrix}
\end{equation*}
which is of dimension $S\times (2K + S)$. The upper bounds are the estimated point-identified estimands:

\begin{equation*}
  b_{ub, 1}^{fs} =
  \begin{bmatrix}
    \hat{\beta}_1 & \cdots & \hat{\beta}_S
  \end{bmatrix}'.
\end{equation*}

To encode the second type of constraint we multiply the first $2K$ columns of $A_{ub, 1}^{fs}$ as well as the upper bounds by $-1$.

In the first step program, $c$ is a constant, while $A_{ub}^{fs}$ and $b_{ub}^{fs}$ need to be estimated from the data.

\subsubsection{Second Step Linear Program}
Denote the minimal deviation from the first step program, that is the value function, by $\mu$ (or $\hat{\mu}$, to emphasize the dependence on the data).

The choice variables in the second-step program are again only the basis function coefficients:
\begin{equation*}
  x^{ss} =
  \begin{bmatrix}
     \theta_{01} & \cdots & \theta_{0K} & \theta_{11} & \cdots & \theta_{1K} & X_1' & \cdots & X_S'
  \end{bmatrix}',
\end{equation*}
and the coefficients are the estimated individual contributions to the target parameter
\begin{equation*}
  c^{ss} =
  \begin{bmatrix}
     \hat{\Gamma}_0^*(b_{01}) & \cdots & \hat{\Gamma}_0^*(b_{0K}) & \hat{\Gamma}_1^*(b_{11}) & \cdots & \hat{\Gamma}_1^*(b_{1K}) & \mathbf{0}
  \end{bmatrix}'.
\end{equation*}
Both of these are vectors of length $2K + S$.

Note that the problem again features the $l_1$ norm, this time in the constraints.
Because we have a sum over multiple absolute values, we again introduce a dummy variable and two constraints that behaves like the absolute value for each point-identified estimand.
The only difference to the first-step constraint matrix is then the additional presence of the (relaxed) constraint on the $l_1$ norm:

\begin{equation*}
  a_{ub}^{ss, 1} = \begin{bmatrix}
    \mathbf{0} & \mathbf{1}
  \end{bmatrix}
  \leq \hat{\mu} + \kappa_n.
\end{equation*}
Here the first vector of zeros is of length $2K$ (the basis functions) and the vector of ones of length $S$ (the dummy variables).

The full second-step upper ound matrix is then given by
\begin{equation*}
  A_{ub}^{ss} = \begin{bmatrix}
    a_{ub}^{ss, 1} \\
    \mathbf{A_{ub,1}^{fs}} \\
    \mathbf{A_{ub,2}^{fs}} \\
  \end{bmatrix}
  \leq
  \begin{bmatrix}
    \hat{\mu} + \kappa_n \\
    \mathbf{\hat{\beta}} \\
    - \mathbf{\hat{\beta}} \\
  \end{bmatrix}
  = b_{ub}^{ss}.
\end{equation*}

The full program is then
\begin{align}
  & \min_x \hat{c}^{ss'}x^{ss} \text{ subject to }\\
  & \hat{A}_{ub}^{ss} \leq \hat{b}_{ub}^{ss} \\
  & \mathbf{0} \leq x^{ss} \leq \mathbf{1}.
\end{align}
where $\hat{\cdot}$ is used to emphasize, which parts of the problem are estimated from the data.

\subsubsection{Shape Constraints}
Shape constraints need to be added to both the first and second step program.
All shape constraints are similar to the identification part, except for additional zero columns in $A_{ub}$ in the first step program due to the dummy variables.
In particular, all of these constraints are non-random.


\subsection{Illustration: Binary IV Model with Point-Identified LATE}


\subsection{Illustration: Binary IV Model with Sharp Identified Set}


\end{document}
