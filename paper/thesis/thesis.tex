% Thesis guidelines:
% - 40 pages including graphs, graphics and pictures, TOC/appendix/bibliography not counted
% - 1.5 line-spacing
\documentclass[12pt,a4paper,english]{article} %document type and language

\usepackage[onehalfspacing]{setspace}
% \linespread{1.5}

\usepackage[utf8]{inputenc}	% set character set to support some UTF-8
\usepackage{babel} 	% multi-language support
% \usepackage{sectsty}	% allow redefinition of section command formatting
\usepackage{tabularx}	% more table options
\usepackage{titling}	% allow redefinition of title formatting
\usepackage{imakeidx}	% create and index of words
\usepackage{xcolor}	% more color options
\usepackage{enumitem}	% more list formatting options
\usepackage{tocloft}	% redefine table of contents, new list like objects

\usepackage[centering,noheadfoot,left=3cm, right=2cm, top=2cm, bottom=2cm]{geometry}

%set TOC margins
\setlength{\cftbeforesecskip}{15pt} % skip in TOC

% remove paragraph white space and modify space between list items
\usepackage{parskip}

% Set font globally
\usepackage{lmodern}                % load Latin modern fonts
\usepackage[defaultsans]{cantarell} % cantarell fonts

% HACK: https://tex.stackexchange.com/questions/58087/how-to-remove-the-warnings-font-shape-ot1-cmss-m-n-in-size-4-not-available
\usepackage{anyfontsize}

% set LaTeX global font
\renewcommand{\familydefault}{\sfdefault}
\renewcommand{\sfdefault}{lmss}

% set styling headings
%\allsectionsfont{\usefont{OT1}{phi}{b}{n}}

\usepackage{float} 	% floats
\usepackage{graphicx}	% Graphics
\usepackage{amsmath}	% extensive math options
\usepackage{amssymb}	% special math symbols
\usepackage[Gray,squaren,thinqspace,thinspace]{SIunits} % elegant units
\usepackage{listings}                                   % source code

% Custom Operators
%% Expectation symbol
\DeclareMathOperator*{\E}{\mathbb{E}}
\DeclareMathOperator*{\Cov}{\mathrm{Cov}}
\DeclareMathOperator*{\Var}{\mathrm{Var}}

% missing math commands
\providecommand{\abs}[1]{\left\lvert#1\right\rvert}                    % |.|
\providecommand{\br}[1]{\left(#1\right)}                               % (.)
\providecommand{\sbr}[1]{\left[#1\right]}                              % [.]
\providecommand{\ddfrac}[2]{\frac{\displaystyle #1}{\displaystyle #2}}
% use \math rm{d} to include math differential

% independence symbol
% https://tex.stackexchange.com/questions/79434/double-perpendicular-symbol-for-independence
\newcommand{\indep}{\perp\!\!\!\!\perp}


% options for listings
\lstset{
  breaklines=true,
  postbreak=\raisebox{0ex}[0ex][0ex]{\ensuremath{\color{red}\hookrightarrow\space}},
  numbers=left,
  numbersep=5pt,
  numberstyle=\tiny\color{gray},
  basicstyle=\footnotesize\ttfamily
}

% NEEDS to be before hyperref, cleveref and autonum
% number figures, tables and equations within the sections
\numberwithin{equation}{section}
\numberwithin{figure}{section}
\numberwithin{table}{section}

% references and annotation, citations
\usepackage[small,bf,hang]{caption}        % captions
\usepackage{subcaption}                    % adds sub figure & sub caption
\usepackage{sidecap}                       % adds side captions
\usepackage{hyperref}                      % add hyperlinks to references
\usepackage[noabbrev,nameinlink]{cleveref} % better references than default~\ref
% Hack:https://tex.stackexchange.com/questions/285950/package-autonum-needs-the-obsolete-etex-package
\expandafter\def\csname ver@etex.sty\endcsname{3000/12/31}
\let\globcount\newcount
% Deactivate for now to avoid issues with equation* environment.
% \usepackage{autonum}                       % only number referenced equations
\usepackage{url}                           % URLs
% Biblatex throws error when cite is used.
% Similar warning is given by natbib.
% \usepackage{cite}                          % well formed numeric citations

% % biblatex for references
% \usepackage{biblatex}
% \addbibresource{literature.bib}
% % csquotes recommended: https://tex.stackexchange.com/questions/229638/package-biblatex-warning-babel-polyglossia-detected-but-csquotes-missing
% \usepackage{csquotes}
% \addbibresource{refs.bib}

% https://tex.stackexchange.com/questions/144764/author-year-citation-in-latex
\usepackage[round]{natbib}

% Avoid space before footnotes when \footnote{...} is on next line.
% https://tex.stackexchange.com/questions/94563/new-line-for-footnote-without-blank-space
\usepackage{xpatch}
\xpretocmd{\footnote}{\unskip}{}{}

% format hyperlinks
\colorlet{linkcolour}{black}
\colorlet{urlcolour}{blue}
\hypersetup{colorlinks=true,
            linkcolor=linkcolour,
            citecolor=linkcolour,
            urlcolor=urlcolour}

%\usepackage{todonotes} % add to do notes
\usepackage{epstopdf}  % process eps-images
\usepackage{float}     % floats
\usepackage{fancyhdr}  % header and footer
% HACK: https://tex.stackexchange.com/questions/664532/fancyhr-warning-footskip-is-too-small
\setlength{\footskip}{15pt}

% default path for figures
\graphicspath{{figures/}}

% If we have multiple directories, specify them like this: \graphicspath{{figures_ch1/}{figures_ch2/}}.

% For rendering tikz
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{decorations.pathreplacing} % Load the library for drawing braces


% Define some math environments
% \usepackage{amsthm}

% \newtheorem{theorem}{Theorem}[section]
% \newtheorem{corollary}{Corollary}[theorem]
% \newtheorem{lemma}[theorem]{Lemma}

% \theoremstyle{definition}
% \newtheorem{definition}{Definition}[section]

% \theoremstyle{remark}
% \newtheorem*{remark}{Remark}

% https://tex.stackexchange.com/questions/24840/use-courier-font-inline-on-text
\usepackage{courier}

% https://tex.stackexchange.com/questions/639234/how-to-put-braces-over-certain-parts-of-matrices
\usepackage{nicematrix}


% set header and footer
\pagestyle{fancy}
\fancyhf{}                           % clear all header and footer fields
\cfoot{\thepage}                     % add page number
\renewcommand{\headrulewidth}{0pt} % add horizontal line of 0.4pt thick

\title{Statistical Inference in Partially Identified Marginal Treatment Effect Models}
\author{Julian Budde}
\date{\today}
\begin{document}

\maketitle

\begin{abstract}
	I study inference in the marginal treatment effect model when used to partially-identify a target parameter of interest.
	First, I demonstrate the invalidity of the bootstrap in a simple, but common binary IV model.
	In specific settings, the bounds of the identified set have an easy to calculate solution.
	These solutions exhibit points of non-differentiability when viewed as functions of the point-identified parameters rendering the non-parametric bootstrap invalid.
	Second, I discuss potential solutions proposed in the literature and show how they might be adjusted to typical use-cases where bounds have to be calculated by the means of linear programming.
	Third, for a number of empirically relevant settings I conduct novel simulation studies for the MTE model comparing competing inference methods.
\end{abstract}

\clearpage
\newpage

\tableofcontents

\clearpage
\newpage

\listoftables

\clearpage
\newpage

\listoffigures

\clearpage
\newpage

\section{Introduction}

\section{Bootstrap Invalidity: A Simple Example}
I first illustrate the invalidity of the bootstrap in a simple example with a binary instrument where we only use the point-identified LATE for the instrument-compliers as the basis for extrapolation.
While this will not best we can do in this case --- there are more moments in the data we can point-identify --- it helps to illustrate the type of inference issues that can also arise in more complex setups.
In particular, we can calculate explicit solutions for the upper and lower viewed as functions of the point-identified complier LATE.\@

\subsection{Setup}
We first introduce some basic notation and key ingredients of the MTE model.
A complete statement of the model corresponding to~\cite{mogstad2018using} can be found in Section~\ref{sec:general_mte}.

The outcome is denoted $Y_i$ and has bounded support, which we generally assume to be in $[0,1]$ for convenience.
The instrument $Z_i$ is binary with realizations in $\{0,1\}$.
Binary treatment $D_i$ is determined by a selection model with
\begin{equation}\label{eq:treatment}
  D_i = I\{p(Z_i) \geq U_i\}
\end{equation}
where unobserved heterogeneity $U_i$ is normalized to follow a $U(0,1)$ distribution, $U_i \indep Z_i$ and $I\{A\}$ denotes the indicator function for event $A$.
Hence, $p(z) \equiv P(D_i=1|Z_i=z)$ is the propensity score. We define the instrument $Z_i$ such that $p(0)<p(1)$.

As shown by~\cite{vytlacil2002independence}, under an appropriate set of assumptions stated more formally in Section~\ref{sec:general_mte}, this setup is equivalent to the potential outcome framework in~\cite{imbens_angrist1994ecma}.
Hence, we can point-identify a local average treatment effect for the instrument-compliers, namely those individuals, who take up the treatment if and only if $Z_i=1$.
In the selection model above, the complier subpopulation is those with $U_i$ realizations in $[p(0), p(1)]$.
Hence, in the notation of the model, we can point-identify,
\begin{equation}
  \beta_s = \E[Y(1) - Y(0)|U_i\in[p(0), p(1)]].
\end{equation}
Note that individuals with realizations $u \leq p(0)$ will always select into treatment, while individuals with $u \geq p(1)$ will never select into treatment.
To keep the notation light, I will refer to these subpopulations as \textit{complier}, \textit{never-taker} and \textit{always-taker}.

Our goal is to extrapolate from this identified complier-LATE to a larger subpopulation that also includes some never-takers.
[Example for interpretation]. Now we are interested in a policy-change which we would believe to increase the propensity score.
We denote this new target by
\begin{equation}
  \beta^* = \E[Y(1) - Y(0)|U_i \in[p(0), p(1) + \overline{u}]],
\end{equation}
where $\overline{u}\in[0, 1-p(1)]$.

Without very strong restrictions, $\beta^*$ will not be point-identified.
Instead, we will only generally be able to identify a set of values for the target that are both consistent with the data and theoretical assumptions we want to impose.
Usually, this \textit{identified set} will be a closed interval on the real line, and we will denote it by
\begin{equation*}
  \beta^* \in [\underline{\beta^*}(\beta_s), \overline{\beta^*}(\beta_s)],
\end{equation*}
where we emphasize the dependence of the upper and lower bound of the identified set on the point-identified parameter $\beta_s$.

\subsection{Solution: Non-parametric Bounds}
While the identified set is usually computed via a linear program, in the simple case considered here with a single point-identified parameter the identified set has an intuitive and easy to compute form.
Note the following solutions are referred to as \textit{non-parametric} in~\cite{mogstad2018using}, that is, they do not impose any parametric assumptions on the underlying MTR functions.
The corresponding linear program solution can be achieved by using constant splines defined on the grid $[0, p(0), p(1), p(1) + \overline{u}, 1]$.

Denote by $\beta_{\overline{u}}$ the LATE for the population of never-takers for which we want to extend the treatment, i.e.
\begin{equation*}
  \beta_{\overline{u}} \equiv E[Y(1) - Y(0) | u \in [p(1), p(1) + \overline{u}]].
\end{equation*}
Then $\beta^*$ is a weighted average of the two LATEs:\@
\begin{equation*}
  \beta^* = \omega\beta_s + (1-\omega)\beta_{\overline{u}}
\end{equation*}
where $\omega = \frac{p(1) - p(0)}{\overline{u} + p(1) - p(0)}$ is the relative share of compliers in the target population.
Finding the identified set then amounts to finding bounds on $\beta_{\overline{u}}$, where different restrictions imply different bounds.



\paragraph{Non-parametric Bounds}
Without any further restrictions, the only bounds we can give follow from the support of $Y_i$.
Since $Y_i\in[0,1]$ we have $\beta_{\overline{u}} \in [0,1]$ and hence
\begin{equation*}
  \beta^* \in [\omega\beta_s - (1 - \omega), \omega\beta_s + (1 - \omega)]
\end{equation*}
We can already see that this is a continuous and differentiable function of $\beta_s$.
Hence, if estimation for $\beta_s$ and $\omega$ is standard, the usual delta method could be applied.
This is the case here, since $\beta_s$ can be consistently estimated using the Wald estimator which --- assuming no weak identification issues --- is asymptotically normal.
$\omega$ is a function of sample moments and can be estimated consistently.
Therefore, also the non-parametric bootstrap is valid in this case.
\footnote{The only issue would arise at the boundary of the parameter space, that is when $\underline{\beta^*}(\beta_s)=-1$ or $\overline{\beta^*}(\beta_s)=1$.
These cases are of less interest practically, hence I disregard them. However, a similar issue arises under shape restrictions discussed below.}

\paragraph{Monotone Marginal Treatment Effect}
We can impose the assumption, that the treatment effects $E[Y(1) - Y(0)|U_i=u]$ are monotone in $u$.
In the language of~\cite{mogstad2018using}, the MTE function is monotone.
The most interesting assumption is a \textit{decreasing} MTE function, since treatment take-up is decreasing in $u$.

If the MTE function is decreasing, we know $\beta_{\overline{u}} \leq \beta_s$ and hence the identified set shrinks at the upper bound:
\begin{equation*}
  \beta^* \in [\omega\beta_s - (1 - \omega), \beta_s]
\end{equation*}
In this case, inference is again standard. Note we have point identification when $\beta_s=-1$.
\footnote{Convergence to point-identification can pose problems to the type of confidence intervals considered by~\cite{imbens2004confidence} and considered later here.
In particular, coverage does not hold uniformly over parameter sequence implying convergence to point-identification.
While~\cite{imbens2004confidence} propose a fix to this, later on I focus on cases ``far enough'' away from point-identification.}


\paragraph{Monotone Treatment Response}
Under monotone treatment response, it is assumed that all individuals respond either negative or positive to the treatment.
An implication for the marginal treatment effect is that it is either positive or negative everywhere.
This again tightens the set, but it also restricts the parameter space of $\beta_s$ that is consistent with the model.
In particular, any $\beta_s \leq 0$ is inconsistent with the assumption of positive treatment responses.
In addition, the lower bound will be tighter:
\begin{equation*}
  \beta^* \in \begin{cases}
    [\omega\beta_s, \beta_s + (1 - \omega)] & \beta_s \geq 0 \\
    \emptyset & \beta_s < 0
  \end{cases}
\end{equation*}
Here, inference becomes non-standard at the boundary of the parameter space.
This case is close to the stylized example analyzed in~\cite{andrews1999estimation}, where interest lies in estimating an expectation which is known to be positive.
[Explain, why not focusing on this in the following.]

\paragraph{Decreasing Marginal Treatment Responses}
A last restriction considered by~\cite{mogstad2018using} are monotonicity restrictions on the MTR functions $m_d(u) = E[Y(d)|U=u]$.
We consider the case of decreasing MTR functions also considered in their paper.
Imposing restrictions on these functions is also the general approach discussed in Section~\ref{sec:general_mte}, since this allows to exploit other moments for identification.
For example, in our model also $E[Y(1)|\text{always-taker}]$ and $E[Y(0)|\text{never-taker}]$ are identified and can help in tightening the identified set.

While generally not impossible, with the restriction that $m_0(u), m_1(u)$ are decreasing in $u$, the computation of the identified set becomes less immediate.
Figure [REF] illustrates the solution using constant spline basic functions defined separately over the intervals implied by $[0, p(0), p(1), p(1) + \overline{u}, 1]$.
Essentially, since both the identified and target parameter are averages over the MTR (or MTE) function with constant weights on each sub-interval, taking constant splines on these sub-intervals as basis functions does not impose any restrictions.

The solution in this case is given by:
\begin{equation}
	\overline{\beta^*}(\beta_s)=
	\begin{cases}
		\omega \beta_s + (1 - \omega),& \quad \text{if } \beta_s \geq 0\\
		\beta_s + (1 - \omega),              & \quad \text{if } \beta_s < 0,
	\end{cases}
\end{equation}
and
\begin{equation}
	\underline{\beta^*}(\beta_s)=
	\begin{cases}
		\beta_s - (1 - \omega),& \quad \text{if } \beta_s \geq 0\\
		\omega \beta_s - (1 - \omega),              & \quad \text{if } \beta_s < 0.
	\end{cases}
\end{equation}

Hence, both bounds exhibit a kink at $\beta_s=0$.
In these cases, as was shown in~\cite{dumbgen1993nondifferentiable} and later~\cite{fang2019infdirdiff}, non-differentiability at the kink renders the non-parametric bootstrap invalid.
However, other methods have been devised for this case. For example,~\cite{fang2019infdirdiff} and~\cite{hong2018numerical} extend the delta method to the case of functions that are only directionally differentiable, such as the ones with a kink shown above.
Further, subsampling is theoretically justified as long as the bounds of the identified set have \textit{any} limiting distribution.
I consider these methods as well as the non-parametric bootstrap in more detail in Section~\ref{sec:inference_methods} and provide simulation evidence in a realistic setting in Section~\ref{sec:simulation_studies}.

\subsection{Solutions: Bounds with Bernstein Polynomials}
In most applications, the non-parametric identified set resulting from constant splines will feature solutions, i.e. MTR functions, that exhibit jump discontinuities.
[Maybe put a solution in the appendix similar to Bernstein figures?].
The appeal of the MST framework is, that the MTR functions have direct economic interpretations that allow to place theoretical restrictions about which meaningful discussions are possible.
In particular, in most cases a certain degree of smoothness in these functions seems reasonable.

To implement this in a tractable manner,~\cite{mogstad2018using} propose to use Bernstein polynomials as basis polynomials for the MTR functions, because they exhibit many computational advantages.
For example, shape restrictions, including monotonicity of the MTR or MTE functions as well as monotone treatment response, can be easily implemented in a linear program.
\footnote{For details on the implementation of shape restrictions see Appendix Section~\ref{app_sec:linear_programs}.}

The Bernstein polynomial of degree $k$ is given by the sum over $k+1$ basis polynomials:
\begin{equation*}
  B_k(x) := \sum_{v=0}^n c_v b_{v,n}(x) \qquad \text{ where } b_{v,n}(x) := \binom{n}{v} x^v(1-x)^{n-v}, \qquad v = 0, \ldots, k.
\end{equation*}

For example, for $k=2$, we have
\begin{equation*}
  B_2(x) = c_0 (1-x)^2 + c_1 2x(1-x) + c_2 x^2.
\end{equation*}

Using these basis polynomials, it is easy to construct a linear program such that the implied MTR functions are smooth over $[0,1]$ and have images in $[0,1]$ (or any other interval, for that matter).
However, the value function of the linear program, that is the upper or lower bound of the identified set, can still be \textit{non-smooth} and feature kinks.

To illustrate, I return to the binary-IV example discussed above for non-parametric bounds.
Because the analytical solution is non-trivial to characterize for polynomials of degree $\geq2$, I instead repeatedly solve the linear program over a range of parameter values for the compliers LATE $\beta_s$.
\footnote{For a precise statement of the problem see Section~\ref{sec:general_mte} and Appendix Section~\ref{app_sec:linear_programs} for the linear program.}
I report results for a lower-order, meaning more restrictive, polynomial of degree $2$ as well as a $11$th order polynomial similar to the numerical example in~\cite{mogstad2018using}.
\footnote{
  To construct the solutions, I first use DGPs with a constant spline MTR function defined on the sub-intervals of $[0, p(0), p(1), p(1) + \overline{u}, 1]$.
  To vary $\beta_s$ I change the coefficients on the constant spline on the complier sub-interval $[p(0), p(1)]$.
  While in principle this DGP is inconsistent with a Bernstein polynomial used in identification,
  note that the MTR functions are only used to compute $\beta_s$ and play no role otherwise.
}

Figure~\ref{fig:id_set_binary_iv_bernstein} plots the solution to the lower bound $\underline{\beta}^*(\beta_s)$ as a function of the identified late.
In addition to the lower bound, the figure also shows the basis function coefficients that solve the linear program separately for $m_0$ and $m_1$.
The corresponding solution for the upper bound is symmetric, since no other shape restrictions are placed on the problem. Hence, I focus on the lower bound only for clarity.

Notably, while still continuous, these solutions again exhibit kinks, this time even without any shape restrictions.
While also visually notable, I estimate the position of the kinks by checking for discontinuities in estimated second derivatives.
These positions are indicated by gray lines and align well with the visual kinks.
From the lower subplots it is easy to see, that the kinks are associated with corner solutions of the linear programs, where coefficients start to increase from 0 or decrease from 1.
This feature is present for both reported degrees, $k=3$ and $k=11$.

Notably, increasing the degree of the polynomial has three effects:
First, as expected, the lower bounds become smaller, since the space of possible solutions is larger.
Second, the solution exhibits more kinks. In fact, the number of kinks equals the number of basis polynomials.
Lastly, increasing the number of polynomials sharply increases the slope of the solution close to 1, that is the boundary of the parameter space.
If we think of an increasing sequence $k$, this could indicate a failure of Lipschitz continuity close to the boundary.


\begin{figure}

  \caption{Identified Sets for the Binary-IV Model with Bernstein Polynomial MTRs}\label{fig:id_set_binary_iv_bernstein}

  \centering
  \begin{subfigure}[b]{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../../bld/figures/binary_iv/identification_with_bernstein_2.png}
      \caption{Degree $k=2$}\label{fig:id_set_binary_iv_bernstein_k_2}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../../bld/figures/binary_iv/identification_with_bernstein_11.png}
      \caption{Degree $k=11$}\label{fig:id_set_binary_iv_bernstein_k_11}
  \end{subfigure}
\end{figure}

\paragraph{Shape Constraints}
When adding shape constraints, the qualitative features of the solution that can pose problems for inference do not change.
[Construct Appendix Figures under shape constraints.]
However, adding further shape constraints typically reduces the solution space and hence can increase

To summarize, in this section I considered a simple binary IV model which already exhibits some problems for inference in the MTE model:
First, many solutions to the MTE problem exhibit \textit{kinks} when viewed as a function of the point-identified parameters.
Kinks are present under certain shape restrictions for the non-parametric identified set using constant splines and are a general feature when using Bernstein polynomials.
Non-differentiability generally renders typical inference approaches like the non-parametric bootstrap inconsistent, as observed by~\cite{dumbgen1993nondifferentiable}.
Second, for Bernstein polynomials, the slope of the solutions close to the boundary of the parameter space exhibits increasing slopes over the degree $k$.
This could potentially indicate a failure of Lipschitz continuity.
Lastly, introducing shape restrictions, in particular monotonicity and sign constraints on the implied MTE function $m_1(u) - m_0(u)$, can significantly reduce the parameter space consistent with the model.
This can lead to parameter at the boundary issues, as, for example, discussed in~\cite{andrews1999estimation}.

After presenting the general MTE model setup, the rest of the paper is concerned with presenting inference approaches, that can potentially address these concerns (Section~\ref{sec:inference_methods}) and comparing their performance in a Monte Carlo simulation.
\section{General MTE Setup}\label{sec:general_mte}

\subsection{Setup}

\subsection{Linear Program}

\subsection{Restrictions}

\section{Inference Methods}\label{sec:inference_methods}

\subsection{Resampling Methods: Non-parametric Bootstrap and Subsampling}

\subsection{Confidence Intervals}

\subsection{Adjusted Delta Methods}

\paragraph{Analytical Delta Method}

\paragraph{Numerical Delta Method}

\section{Simulation Studies}\label{sec:simulation_studies}

\subsection{Model Setting}

\subsection{Simulation Design}

\subsection{Simulation Results}

\bibliographystyle{abbrvnat}
\bibliography{refs.bib}

% \printbibliography % use this for biblatex

\appendix

\section{Linear Programs}\label{app_sec:linear_programs}
This section presents the exact formulation of the linear programs corresponding to how most solvers specify linear programs.
I illustrate the actual programs in the case of binary IV model with an identified complier LATE/IV slope coefficient.
[Add programs for sharp identified set?]

\subsection{Identification}

The program for the upper bound is formulated as follows in~\cite{mogstad2018using}:
\begin{align}
  \overline{\beta}^* \equiv \sup_{(\theta_0, \theta_1)\in\Theta} \sum_{k=1}^{K_0}\theta_{0k}\Gamma^*_0(b_{0k}) + \sum_{k=1}^{K_1}\theta_{1k}\Gamma^*_1(b_{1k}) \\
  \text{subject to} \qquad \sum_{k=1}^{K_0}\theta_{0k}\Gamma_{0s}(b_{0k}) + \sum_{k=1}^{K_1}\theta_{1k}\Gamma_{1s}(b_{1k}) = \beta_s \text{ for } s \in \mathcal{S}.
\end{align}

In words: We maximize the target parameter, where we maximize over a basis function approximation with coefficients in a set $\Theta$.
The first summation term corresponds to $m_0$, the second to $m_1$.
We can separately parametrize $m_0$ and $m_1$ as reflected by the potentially different number of basis coefficients $K_0$ and $K_1$.
$\Theta$ can implicitly incorporate other constraints, like shape restrictions on $m_0, m_1$ which will be made explicit below.
It will at least entail the restrictions that $m_0, m_1$ are bounded by the support of the outcome $Y$, which we take to be $[0,1]$ without loss of generality.

Explicitly stated in the second line are the restrictions that for each point-identified estimand $s\in\mathcal{S}$ the maximizers $(\theta_0, \theta_1)$ must recover the point-identified parameter.

\paragraph{Matrix Form}
Next, I explicitly define the matrices typically used to describe a linear program.
In the so-called \textit{standard form}, all linear programs are described by:
(1) A \textit{maximization} objective; (2) inequality constraints with upper bounds; (3) non-negativity constraints on the choice variables.
Hence, we have:

\begin{align}
  \max_x c'x \\
  \text{ subject to } Ax\leq b, x\geq0
\end{align}

To keep the notation lighter, I instead reformulate the program in terms of:
(1) A \textit{minimization} objective; (2) inequality constraints with upper bounds; (3) equality constraints; (4) lower and upper bounds on the choice variables.
In this case, the program is:

\begin{align}
  \min_x c'x \\
  \text{ subject to } A_{ub}x \leq b_{ub}, A_{eq} = b_{eq}, l\leq x\leq u.
\end{align}

This is, for example, how the linear program is inputted into the \texttt{SciPy} \texttt{linprog} solver.
The program can easily be transformed to standard form by: (1) taking the negative of the objective, (2) reformulating equalities as two inequalities, and (3) expressing variables without positivity constraints as differences of two non-negative variables.
\texttt{SciPy}, for example, performs these operations in addition to other pre-solve checks for potential simplifications in the background.
It then passes the standard form program to a solver and post-processes the result to a solution of the original problem.

For simplicity, in the following we take $K_0 = K_1 = K$. This is the case for non-parametric exact bounds using constant splines and also parametric cases there seems little reason to have different numbers of basis polynomials.
Further, it simplifies the formulation of shape restrictions.

The choice variables are the basis function coefficients
\begin{equation*}
  x =
  \begin{bmatrix}
     \theta_{01} & \cdots & \theta_{0K} & \theta_{11} & \cdots & \theta_{1K}
  \end{bmatrix}',
\end{equation*}
and the coefficients are the individual contributions to the target parameter
\begin{equation*}
  c =
  \begin{bmatrix}
     \Gamma_0^*(b_{01}) & \cdots & \Gamma_0^*(b_{0K}) & \Gamma_1^*(b_{11}) & \cdots & \Gamma_1^*(b_{1K})
  \end{bmatrix}'.
\end{equation*}
Both of these are vectors of length $2K$.

Next, we have an equality constraint for each point-identified estimand $s\in\mathcal{S}$.
\begin{equation*}
  a_{eq}^s =
  \begin{bmatrix}
     \Gamma_{0s}(b_{01}) & \cdots & \Gamma_{0s}(b_{0K}) & \Gamma_{1s}(b_{11}) & \cdots & \Gamma_{1s}(b_{1K})
  \end{bmatrix}.
\end{equation*}
Note that we changed the map from $\Gamma^*_d$ to $\Gamma_{ds}$. Stacking these row-vectors gives the equality constraint matrix:
\begin{equation*}
  A_{eq}^s =
  \begin{bmatrix}
    a_{eq}^1 \\
    \vdots \\
    a_{eq}^\mathcal{S} \\
  \end{bmatrix},
\end{equation*}
which is a matrix of dimension $(S\times 2K)$, where, for brevity, we denote the \textit{number} of identified estimands $|\mathcal{S}| = S$.
The constraint values are given by the point-identified estimands:
\begin{equation*}
  b_{eq} =
  \begin{bmatrix}
    \beta_1, \ldots, \beta_\mathcal{S}
  \end{bmatrix}'.
\end{equation*}


A set of restrictions to achieve a minimal degree of identification is $y_l \leq m_d \leq y_d$ for $d=0,1$ and $y_l, y_u$ the lower and upper bound of the support of $Y$.
Taking $y_l = 0, y_u=1$, for typical shape constraints --- constant splines or Bernstein polynomials --- this amounts to the restrictions $0\leq \theta_{dj} \leq 1$ for $d=0,1, j=1,\ldots,K$.
Hence, we have the restrictions
\begin{equation*}
  l = \mathbf{0} \leq x \leq \mathbf{1} = u,
\end{equation*}
where $l$ and $u$ are vectors of length $2K$ each.

Finally, a number of shape restrictions may be added in the form of inequality bounds $A_{ub} \leq b_{ub}$.
The following applies to constant splines and Bernstein polynomials, but might not hold for all possible types of basis functions.

\paragraph{MTR Monotonicity}
To enforce a \textit{decreasing} MTR function $m_d(u)$ for constant splines, it is immediate that $\theta_{d1} \geq \theta_{d2} \geq \cdots \geq \theta_{dK}$, given that the functions are ordered over the partition of $u$, which we always assume to be the case.
For Bernstein polynomials this also holds, since they obey a monotonicity property: Whenever the basis coefficients $\theta_{dk}$ are decreasing over $k$, so is the resulting Bernstein polynomial (and the same for increasing).

Hence, to enforce a decreasing MTR function for $d=0$ we add the following constraints:
% \begin{equation*}
%   A_{eq}^s =
%   \begin{bmatrix}
%     -1 & 1 & 0 & \cdots & 0 & 0\\
%     0 & -1 & 1 & \cdots & 0 & 0\\
%     \vdots & & & \vdots \\
%     0 & 0 & 0 & \cdots & -1 & 1\\
%   \end{bmatrix} + \mathbf{0}
% \end{equation*}

\begin{equation*}
  A_{ub} =
  \begin{bNiceArray}{ccccccc}
    -1 & 1 & 0 & \cdots & 0 & 0 & \Block{4-1}<\Large>{\mathbf{0}}\\
    0 & -1 & 1 & \cdots & 0 & 0\\
    \vdots & & & \vdots \\
    0 & 0 & 0 & \cdots & -1 & 1\\
  \end{bNiceArray} \leq \mathbf{0} = b_{ub}.
\end{equation*}
Here, the first block is of dimension $(K-1) \times K$ reflecting the inequalities on the basis functions for $m_0(u)$.
The second block are zeros of dimension $(K-1) \times K$ on the basis functions for $m_1(u)$.
To add a similar restriction on $m_1(u)$ we can switch the blocks.
To enforce \textit{increasing} MTR functions $A_{ub}$ above can be multiplied by $-1$ to flip the inequalities.
$b_{ub}$ has length $(K-1)$.

\paragraph{MTE Monotonicity}
To enforce a \textit{decreasing} MTE function (the difference $m_1(u) - m_0(u)$) we require
$\theta_{11} - \theta_{01} \geq \theta_{12} - \theta_{02} \geq \cdots \geq \theta_{1K} - \theta_{0K}$.
For both constant splines and Bernstein polynomials this is immediate, since the difference $m_1(u) - m_0(u)$ implies a set of basis functions with the above differences as coefficients, which, hence, obey the same monotonicity conditions.

\begin{equation*}
  A_{ub} =
  \begin{bmatrix}
    \ldots
  \end{bmatrix} \leq \mathbf{0} = b_{ub}
\end{equation*}

\paragraph{Monotone Treatment Response}
Monotone treatment response requires $m_1(u) - m_0(u) \geq 0$ (positive) or $m_1(u) - m_0(u) \leq 0$ (negative) for all $u$.
Hence, we require $\theta_{1j} - \theta_{0j} \geq 0$ for all $j=1,\ldots,K$ for positive responses and reversed for negative.
\begin{equation*}
  A_{ub} =
  \begin{bmatrix}
    \ldots
  \end{bmatrix} \leq \mathbf{0} = b_{ub}
\end{equation*}

\subsection{Estimation}
Estimation is more involved and features solving two linear programs.
~\cite{mogstad2018using} propose a two-step estimator.
The reason is, that in sample we might not always expect all equality constraints for the identified estimands to be satisfied.
To allow for some sampling uncertainty, in a first step we compute the minimal achievable deviations from all point-identified estimands (as measured by the $l_1$ norm).
In the second step, we then restrict the set of allowed MTR functions to maximize over to those achieving this minimal deviation plus some tolerance.
In this way, the program always has a solution, meaning our estimators always exists.

This approach is stated in equation (27) of~\cite{mogstad2018using}:
\begin{align}
  & \hat{\overline{\beta}^*} \equiv \sup_{m\in \mathcal{M}}\hat{\Gamma}^*(m) \\
  & \text{ subject to } \sum_{s\in\mathcal{S}}|\hat{\Gamma}_s(m) - \hat{\beta}_s| \leq \inf_{m'\in \mathcal{M}} \sum_{s\in\mathcal{S}}|\hat{\Gamma}_s(m') - \hat{\beta}_s| + \kappa_n
\end{align}

Note we have a single constraint for the $l_1$ norm.
The $\inf$ on the right-hand side is the first-step linear program, and we allow for tolerance $\kappa_n$ to enlarge the set of possible solutions.

\subsubsection{First Step Linear Program}
The first step program is given by
\begin{equation*}
  \inf_{m'\in \mathcal{M}} \sum_{s\in\mathcal{S}}|\hat{\Gamma}_s(m') - \hat{\beta}_s|
\end{equation*}

We again rewrite it in the matrix form stated above.
Note that $\mathcal{M}$ again implicitly encodes any bounds and other shape restrictions on the MTR functions.
Further, as pointed out in~\cite{mogstad2018using}, we need to introduce dummy variables to mimic the absolute values in the objective.

\paragraph{Absolute Values in the Objective}
With $S = 1$, that is a single point-identified estimand, we have an objective of the following form:
\begin{equation*}
  \inf_m |X_1|,
\end{equation*}

where $X_1 = \hat{\Gamma}_1(m) - \hat{\beta}_1$.
We now add a dummy variable $X_1'$ and two constraints to the model:
\begin{align*}
  X_1 \leq X_1', \\
  -X_1 \leq X_1'.
\end{align*}
In the objective we replace the absolute value with the dummy:
\begin{equation*}
  \inf X_1'.
\end{equation*}
By going through all possible cases ($X_1=0, X_1<0, X_1>0$) it is easy to show,
that $X_1'$ will behave like $|X_1|$.

Having $|S|>1$ is then simply a matter of adding another dummy and two constraints per point-identified estimand.

Plugging in the original expression for $X_1$ we get
\begin{align*}
  \inf X_1' \\
  \text{ subject to }\\
  \hat{\Gamma}_1(m) - \hat{\beta}_1 \leq X_1', \\
  -(\hat{\Gamma}_1(m) - \hat{\beta}_1) \leq X_1',
\end{align*}
where minimization is now over the basis coefficients (appearing in the constraints) and the dummy variables (appearing in the objective).
Hence, we now have $2K + S$ choice variables.

The vector of choice variables is given by
\begin{equation*}
  x^{fs} =
  \begin{bmatrix}
     \theta_{01} & \cdots & \theta_{0K} & \theta_{11} & \cdots & \theta_{1K} & X_1' & \cdots & X_{S}
  \end{bmatrix}',
\end{equation*}
with corresponding coefficients

\begin{equation*}
  c^{fs} =
  \begin{bmatrix}
     \mathbf{0}', \mathbf{0}', \mathbf{1}'
  \end{bmatrix}',
\end{equation*}
where the zeros on the basis functions are of length $K$ each and the $1$ on the dummies of length $S$.
Both of these are vectors of length $2K + S$.


To encode the first type of inequality for all $S$ we get the following inequality constraint matrix:
\begin{equation*}
  A_{ub, 1}^{fs} =
  \begin{bmatrix}
    \hat{\Gamma}_{01}(b_{01}) & \cdots & \hat{\Gamma}_{01}(b_{0K}) & \hat{\Gamma}_{11}(b_{11}) & \cdots & \hat{\Gamma}_{11}(b_{1K}) & -1 & 0 & \cdots & 0 \\
    \vdots & & \vdots & \vdots &  & \vdots & \\
    \hat{\Gamma}_{0S}(b_{01}) & \cdots & \hat{\Gamma}_{0S}(b_{0K}) & \hat{\Gamma}_{1S}(b_{11}) & \cdots & \hat{\Gamma}_{1S}(b_{1K}) & 0 & 0 & \cdots & -1 \\
  \end{bmatrix}
\end{equation*}
which is of dimension $S\times (2K + S)$. The upper bounds are the estimated point-identified estimands:

\begin{equation*}
  b_{ub, 1}^{fs} =
  \begin{bmatrix}
    \hat{\beta}_1 & \cdots & \hat{\beta}_S
  \end{bmatrix}'.
\end{equation*}

To encode the second type of constraint we multiply the first $2K$ columns of $A_{ub, 1}^{fs}$ as well as the upper bounds by $-1$.

In the first step program, $c$ is a constant, while $A_{ub}^{fs}$ and $b_{ub}^{fs}$ need to be estimated from the data.

\subsubsection{Second Step Linear Program}
Denote the minimal deviation from the first step program, that is the value function, by $\mu$ (or $\hat{\mu}$, to emphasize the dependence on the data).

The choice variables in the second-step program are again only the basis function coefficients:
\begin{equation*}
  x^{ss} =
  \begin{bmatrix}
     \theta_{01} & \cdots & \theta_{0K} & \theta_{11} & \cdots & \theta_{1K} & X_1' & \cdots & X_S'
  \end{bmatrix}',
\end{equation*}
and the coefficients are the estimated individual contributions to the target parameter
\begin{equation*}
  c^{ss} =
  \begin{bmatrix}
     \hat{\Gamma}_0^*(b_{01}) & \cdots & \hat{\Gamma}_0^*(b_{0K}) & \hat{\Gamma}_1^*(b_{11}) & \cdots & \hat{\Gamma}_1^*(b_{1K}) & \mathbf{0}
  \end{bmatrix}'.
\end{equation*}
Both of these are vectors of length $2K + S$.

Note that the problem again features the $l_1$ norm, this time in the constraints.
Because we have a sum over multiple absolute values, we again introduce a dummy variable and two constraints that behaves like the absolute value for each point-identified estimand.
The only difference to the first-step constraint matrix is then the additional presence of the (relaxed) constraint on the $l_1$ norm:

\begin{equation*}
  a_{ub}^{ss, 1} = \begin{bmatrix}
    \mathbf{0} & \mathbf{1}
  \end{bmatrix}
  \leq \hat{\mu} + \kappa_n.
\end{equation*}
Here the first vector of zeros is of length $2K$ (the basis functions) and the vector of ones of length $S$ (the dummy variables).

The full second-step upper bound matrix is then given by
\begin{equation*}
  A_{ub}^{ss} = \begin{bmatrix}
    a_{ub}^{ss, 1} \\
    \mathbf{A_{ub,1}^{fs}} \\
    \mathbf{A_{ub,2}^{fs}} \\
  \end{bmatrix}
  \leq
  \begin{bmatrix}
    \hat{\mu} + \kappa_n \\
    \mathbf{\hat{\beta}} \\
    - \mathbf{\hat{\beta}} \\
  \end{bmatrix}
  = b_{ub}^{ss}.
\end{equation*}

The full program is then
\begin{align}
  & \min_x \hat{c}^{ss'}x^{ss} \text{ subject to }\\
  & \hat{A}_{ub}^{ss} \leq \hat{b}_{ub}^{ss} \\
  & \mathbf{0} \leq x^{ss} \leq \mathbf{1},
\end{align}
where $\hat{\cdot}$ is used to emphasize, which parts of the problem are estimated from the data.

\subsubsection{Shape Constraints}
Shape constraints need to be added to both the first and second step program.
All shape constraints are similar to the identification part, except for additional zero columns in $A_{ub}$ in the first step program due to the dummy variables.
In particular, all of these constraints are non-random.


\subsection{Illustration: Binary IV Model with Point-Identified LATE}


\subsection{Illustration: Binary IV Model with Sharp Identified Set}


\end{document}
