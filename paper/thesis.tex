% Thesis guidelines:
% - 40 pages including graphs, graphics and pictures, TOC/appendix/bibliography not counted
% - 1.5 line-spacing
\documentclass[12pt,a4paper,english]{article} %document type and language

\usepackage[onehalfspacing]{setspace}
% \linespread{1.5}

\usepackage[utf8]{inputenc}	% set character set to support some UTF-8
\usepackage{babel} 	% multi-language support
% \usepackage{sectsty}	% allow redefinition of section command formatting
\usepackage{tabularx}	% more table options
\usepackage{titling}	% allow redefinition of title formatting
\usepackage{imakeidx}	% create and index of words
\usepackage{xcolor}	% more color options
\usepackage{enumitem}	% more list formatting options
\usepackage{tocloft}	% redefine table of contents, new list like objects

\usepackage[centering,noheadfoot,left=3cm, right=2cm, top=2cm, bottom=2cm]{geometry}

%set TOC margins
\setlength{\cftbeforesecskip}{15pt} % skip in TOC

% remove paragraph white space and modify space between list items
\usepackage{parskip}

% Set font globally
\usepackage{lmodern}                % load Latin modern fonts
\usepackage[defaultsans]{cantarell} % cantarell fonts

% HACK: https://tex.stackexchange.com/questions/58087/how-to-remove-the-warnings-font-shape-ot1-cmss-m-n-in-size-4-not-available
\usepackage{anyfontsize}

% set LaTeX global font
\renewcommand{\familydefault}{\sfdefault}
\renewcommand{\sfdefault}{lmss}

% set styling headings
%\allsectionsfont{\usefont{OT1}{phi}{b}{n}}

\usepackage[capposition=top]{floatrow}

\usepackage{float} 	% floats
\usepackage{graphicx}	% Graphics
\usepackage{amsmath}	% extensive math options
\usepackage{amssymb}	% special math symbols
\usepackage[Gray,squaren,thinqspace,thinspace]{SIunits} % elegant units
\usepackage{listings}                                   % source code

% Custom Operators
%% Expectation symbol
\DeclareMathOperator*{\E}{\mathbb{E}}
\DeclareMathOperator*{\Cov}{\mathrm{Cov}}
\DeclareMathOperator*{\Var}{\mathrm{Var}}

% missing math commands
\providecommand{\abs}[1]{\left\lvert#1\right\rvert}                    % |.|
\providecommand{\br}[1]{\left(#1\right)}                               % (.)
\providecommand{\sbr}[1]{\left[#1\right]}                              % [.]
\providecommand{\ddfrac}[2]{\frac{\displaystyle #1}{\displaystyle #2}}
% use \math rm{d} to include math differential

% independence symbol
% https://tex.stackexchange.com/questions/79434/double-perpendicular-symbol-for-independence
\newcommand{\indep}{\perp\!\!\!\!\perp}


% options for listings
\lstset{
  breaklines=true,
  postbreak=\raisebox{0ex}[0ex][0ex]{\ensuremath{\color{red}\hookrightarrow\space}},
  numbers=left,
  numbersep=5pt,
  numberstyle=\tiny\color{gray},
  basicstyle=\footnotesize\ttfamily
}

% NEEDS to be before hyperref, cleveref and autonum
% number figures, tables and equations within the sections
\numberwithin{equation}{section}
% \numberwithin{figure}{section}
% \numberwithin{table}{section}

% references and annotation, citations
\usepackage[small,bf,hang]{caption}        % captions
\usepackage{subcaption}                    % adds sub figure & sub caption
\usepackage{sidecap}                       % adds side captions
\usepackage{hyperref}                      % add hyperlinks to references
\usepackage[noabbrev,nameinlink]{cleveref} % better references than default~\ref
% Hack:https://tex.stackexchange.com/questions/285950/package-autonum-needs-the-obsolete-etex-package
\expandafter\def\csname ver@etex.sty\endcsname{3000/12/31}
\let\globcount\newcount
% Deactivate for now to avoid issues with equation* environment.
% \usepackage{autonum}                       % only number referenced equations
\usepackage{url}                           % URLs
% Biblatex throws error when cite is used.
% Similar warning is given by natbib.
% \usepackage{cite}                          % well formed numeric citations

% % biblatex for references
% \usepackage{biblatex}
% \addbibresource{literature.bib}
% % csquotes recommended: https://tex.stackexchange.com/questions/229638/package-biblatex-warning-babel-polyglossia-detected-but-csquotes-missing
% \usepackage{csquotes}
% \addbibresource{refs.bib}

% https://tex.stackexchange.com/questions/144764/author-year-citation-in-latex
\usepackage[round]{natbib}

% Avoid space before footnotes when \footnote{...} is on next line.
% https://tex.stackexchange.com/questions/94563/new-line-for-footnote-without-blank-space
\usepackage{xpatch}
\xpretocmd{\footnote}{\unskip}{}{}

% format hyperlinks
\colorlet{linkcolour}{black}
\colorlet{urlcolour}{blue}
\hypersetup{colorlinks=true,
            linkcolor=linkcolour,
            citecolor=linkcolour,
            urlcolor=urlcolour}

%\usepackage{todonotes} % add to do notes
\usepackage{epstopdf}  % process eps-images
\usepackage{float}     % floats
\usepackage{fancyhdr}  % header and footer
% HACK: https://tex.stackexchange.com/questions/664532/fancyhr-warning-footskip-is-too-small
\setlength{\footskip}{15pt}

% default path for figures
\graphicspath{{figures/}}

% If we have multiple directories, specify them like this: \graphicspath{{figures_ch1/}{figures_ch2/}}.

% For rendering tikz
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{decorations.pathreplacing} % Load the library for drawing braces


% Define some math environments
\usepackage{amsthm}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\theoremstyle{remark}
\newtheorem*{remark}{Remark}

\theoremstyle{plain}
\newtheorem{assumption}{Assumption}

% https://tex.stackexchange.com/questions/24840/use-courier-font-inline-on-text
\usepackage{courier}

% https://tex.stackexchange.com/questions/639234/how-to-put-braces-over-certain-parts-of-matrices
\usepackage{nicematrix}


% set header and footer
\pagestyle{fancy}
\fancyhf{}                           % clear all header and footer fields
\cfoot{\thepage}                     % add page number
\renewcommand{\headrulewidth}{0pt} % add horizontal line of 0.4pt thick

\title{Statistical Inference in Partially Identified Marginal Treatment Effect Models}
\author{Julian Budde}
\date{\today}
\begin{document}

\begin{titlepage}
	\newfont{\smc}{cmcsc10 at 12pt}
	\newgeometry{left=3cm,right=3cm,bottom=2cm,top=2cm}
	\linespread{1.5}

\begin{center}
$~~$\\
$~~$\\
{\Large
Statistical Inference in Partially Identified \\[-0.5em]
Marginal Treatment Effect Models \\[3.5cm]
\large Master Thesis Presented to the \\
 Department of Economics at the \\
 Rheinische Friedrich-Wilhelms-Universit√§t Bonn \\[1cm]
 In Partial Fulfillment of the Requirements for the degree of \\
 Master of Science (M.Sc.) \\[6.5cm]
 Supervisor: Prof.\ Dr.\ Joachim Freyberger \\[2cm]
\begin{singlespacing}
 Submitted in November 2024 by: \\
 Julian Budde \\
 Matriculation Number: 50099232
 \end{singlespacing}}
\end{center}

\restoregeometry
\end{titlepage}

\maketitle

\begin{abstract}
	I study inference in the marginal treatment effect model when used to partially-identify a target parameter of interest.
	First, I demonstrate the invalidity of the nonparametric bootstrap in a simple, but common binary IV model.
	In specific settings, the bounds of the identified set have an easy to calculate solution.
	These solutions exhibit points of non-differentiability when viewed as functions of the point-identified parameters hence rendering the nonparametric bootstrap invalid.
	Similar issues arise when a parametric approximation to the MTR functions is used.
  Second, I discuss potential solutions proposed in the literature and show how they might be adjusted to typical use-cases where bounds have to be calculated by the means of linear programming.
	Third, for a number of empirically relevant settings I conduct novel simulation studies for the MTE model comparing competing inference methods.
  Lastly, I suggest a conservative inference approach to the value of a linear program by a strict convex relaxation of the constraint set.
  I illustrate the performance of this method in a simple Monte Carlo experiment.
\end{abstract}

\clearpage
\newpage

\tableofcontents

\clearpage
\newpage

% \listoftables

% \clearpage
% \newpage

\listoffigures

\clearpage
\newpage

\section{Introduction}\label{sec:introduction}
Causal inference using instrumental variables is part of the toolkit of almost all applied economists.
One particular prominent result is that of~\cite{imbens_angrist1994ecma}, who show that instruments can identify a certain local average treatment effect, namely for those people who respond to the instrument by shifting their treatment status.
While this result clarifies the interpretation of estimates typically reported in applied work, it provides no guidance for formally reasoning about other parameters of interest, like the average treatment effect or treatment effects for other subpopulations.
These parameters are generally not point-identified without making assumptions beyond those required for the local average treatment effect result.
There are several intuitive approaches to make progress, that can be found throughout the literature, but many of them are unappealing, at least on their own:
Homogeneity assumptions are likely to be wrong. After all, some individuals did not select into treatment and one reason might be knowledge about a negative individual treatment effect.
Using only bounds on the outcome for partial identification typically leaves wide intervals and implies unappealing discontinuities in the treatment response.
Other assumptions might be possible to enforce on a case-by-case basis, but this can quickly get unwieldy.

~\cite{mogstad2018using} seek to remedy this problem by providing a unified framework that allows to transparently combine
information in the data and theoretical assumptions into an identified interval for many target parameters of interest.
In particular, they rely on a characterization of the underlying population by a latent variable describing unobserved heterogeneity or, in economic terms, resistance to treatment.
The model can then be reformulated in terms of \textit{marginal} treatment effects for a particular resistance to treatment.
This characterization allows to combine many assumptions previously imposed in the literature (like a decreasing treatment effect or a monotone treatment response), but also to incorporate smoothness and parametric assumptions.

The usefulness of any such method for applied researchers also depends on the ease with which statistical uncertainty can be accounted for.
That is, how difficult it is to construct valid confidence intervals.
However, inference seems to have gained less attention in the existing literature, possibly due to the lack of theoretical results and the relative difficulty of implementation compared to traditional methods.
While the working paper version~\cite{mogstad2017using_nber} contains a potential procedure for constructing confidence intervals, it is not implemented in their own later implementation \citep{shea2023ivmte} and no simulation results are available.
\footnote{The steps necessary for the inference procedure are described on p. 87 of~\cite{mogstad2017using_nber}.
In addition to constructing the usual sample analogue estimators it involves: (1) Solving two mixed integer linear programs for each bootstrap draw for tuning parameter choice, followed by (2) solving a bilinear maximization problem for each bootstrap draw.}
Instead,~\cite{shea2023ivmte} implement several resampling techniques including the bootstrap, but report no simulations.

This study seeks to fill this gap by conducting Monte Carlo experiments and provides some suggestions on potential inference approaches other than the bootstrap.
I consider a delta method for inference on directionally differentiable functions suggested in~\cite{fang2019infdirdiff} and~\cite{hong2018numerical} for a simple example with explicit solutions.
For more complicated settings, the performance of the nonparametric bootstrap as well as subsampling are studied.
I also suggest a different approach to the ones found in the literature, which relies on appropriately relaxing the constraints of the linear program used to compute the identified set to a larger, but strictly convex feasible set.

The paper is structured as follows. Section~\ref{sec:general_mte} introduces the framework of~\cite{mogstad2018using} and briefly describes potential inference methods.
Section~\ref{sec:simple_example} is concerned with inference in a simple model, for which explicit solutions are available, while Section~\ref{sec:simulation_studies} focuses on a setting with flexible polynomial restrictions closer to actual applications.
Lastly, Section~\ref{sec:convex_relaxation} discusses an idea for a conservative inference approach based on relaxing the constraint set and provides a simple Monte Carlo simulation.

\section{Background: MTE Model and Inference Methods}\label{sec:general_mte}
As a background for the results in the following sections, I restate the MTE approach presented in~\cite{mogstad2018using} (MST) as well as different approaches to inference.
For inference, I focus on the nonparametric bootstrap as a benchmark, adjustments of the delta method to non-differential functions proposed in~\cite{fang2019infdirdiff} as well as subsampling.

\subsection{MTE Model}
\paragraph{Setup}
The method applies to settings with a binary treatment, an instrument and a selection equation that is weakly separable and features one dimension of heterogeneity.
To keep the notation light, I restate the problem without additional exogenous covariates, which will also be the setting for the simulation studies.
The outcome is generated by the outcome equation
\begin{equation}\label{eq:outcome}
  Y = DY_1 + (1-D) Y_0,
\end{equation}
where $Y_d$ is the outcome if $D$ was exogenously set to $d\in\{0,1\}$.
Treatment status is determined by the selection equation
\begin{equation}\label{eq:selection}
  D = I\{p(Z) \geq U\}.
\end{equation}
Here, we directly assume $U$ to be uniformly distributed on $[0,1]$, although any continuous distribution is in fact sufficient since we can transform the problem to quantiles.
In this case, under the assumptions stated below, $p(Z)$ will be the propensity score, that is, the probability $p(z) = P(D=1|Z=z)$.
$p(z)$ is generally unknown and needs to be estimated.
\footnote{The case of a \textit{known} propensity score in (stratified) experiments is not applicable here, since we focus on an instrumental variable framework with non-compliance.
That is, where we can think of $Z$ as treatment offer and $D$ as realized treatment with $D\neq Z$ possible.}

The following set of assumptions is a slightly simplified version of those by MST, which guarantee a ``valid'' instrument.

\begin{assumption}{Instrument Validity, MST Assumptions I}
\begin{itemize}
  \item[1.1] $U$ is statistically independent of $Z$, that is, $U \indep Z$.
  \item[1.2] $E[Y_d|Z,U] = E[Y_d|U]$ and $E[Y_d^2]<\infty$ for $d\in\{0,1\}$.
  \item[1.3] $U$ follows a Uniform distribution on $[0,1]$.
\end{itemize}
\end{assumption}
~\cite{vytlacil2002independence} showed that the additive separable selection equation~\ref{eq:selection} in combination with these assumptions is equivalent to the model of~\cite{imbens_angrist1994ecma}.
As pointed out by MST, the above assumptions allow for a rich degree of choice heterogeneity and selection into the treatment.
For example, dependence between $U$ and $(Y_0, Y_1)$ could arise from selection on the individual treatment effect $Y_1-Y_0$, which might be known to individuals.

The general goal of the approach is to make inference on a target parameter of interest given a set of point-identified parameters and theoretical assumptions.
Inference, here, is to be understood as constructing an identified set in the population.
For example, we might be interested in constructing an identified set for the average treatment effect, given the information provided by the instrument and a potentially further theoretical assumptions, like non-negative treatment effects.

An important object in this model is the \textit{marginal treatment effect} (MTE) for a given realization of $U$,
\begin{equation}\label{eq:mte}
  MTE(u) = \E[Y_1 - Y_0|U=u],
\end{equation}
which can be written as the difference of the two \textit{marginal treatment response} (MTR) functions
\begin{equation}\label{eq:mtr}
  m_d(u) \equiv \E[Y_d|U=u] \qquad \text{ for } d\in\{0,1\}.
\end{equation}

The key observation of MST is that both point-identified parameters and target parameters of interest are \textit{linear functionals} of the underlying MTR functions.
Parametrizing the MTR functions by a finite-dimensional basis then allows to compute the identified set using a linear program.

For target parameters $\beta^*$, we have that
\begin{equation}\label{eq:target}
  \beta^* \equiv \E\left[\int_0^1m_0(u)\omega^*_0(u,Z)du\right] + \E\left[\int_0^1m_1(u)\omega^*_1(u,Z)du\right].
\end{equation}
In Sections~\ref{sec:simple_example} and~\ref{sec:simulation_studies} we will consider as the target the local average treatment effect for the subpopulation $[p(0), p(1) + \overline{u}]$, for which the weights are
\begin{equation*}
  \omega^*_1(u, z) = \frac{I[u\in(p0), p(1) + \overline{u}]}{p(1) + \overline{u} - p(0)} = -\omega^*_0(u,z).
\end{equation*}
A range of other target parameters and their associated weights can be found in Table I of MST, including those with different integrating measures.

Crucially, many point-identified parameters can also be written as similar averages.
As shown in Proposition 1 of MST, we have under Assumptions 1 and for a known $s: \{0,1\}\times \mathbb{R}^{d_z} \to \mathbb{R}$, that
\begin{equation}\label{eq:identified_s}
  \beta_s \equiv \E\left[\int_0^1m_0(u)\omega_{0s}(u,Z)du\right] + \E\left[\int_0^1m_1(u)\omega_{1s}(u,Z)du\right],
\end{equation}
where $\omega_{0s}(u,z) \equiv s(0, z)I[u > p(z)]$ and $\omega_{1s}(u,z) \equiv s(1,z) I[u\leq p(z)]$.
The identification result for these point-identified estimands is that $\beta_s = \E[s(D,Z)Y]$.
Table II of MST lists a number of these so-called \textit{IV-like} estimands.
For example, the IV slope coefficient, which identified the local average treatment effect for compliers with a binary $Z$ \citep{imbens_angrist1994ecma}, is given by the specification
\begin{equation*}
  s(d,z) = \frac{z - E[Z]}{Cov(D,Z)}.
\end{equation*}
Other potential estimands include the OLS slope coefficient and 2SLS components.

\paragraph{Identification: Theory}
The main theoretical identification result is given in Proposition 2 in MST.\@
Denote a collection of IV-like estimands, that is functions $s$ that generate by $\mathcal{S}$.
Denote the parameter space for all considered MTR functions $(m_0, m_1)$ by $\mathcal{M}$, where this can incorporate any restrictions a researcher wants to impose on $(m_0, m_1)$.
We can then define the space of MTR functions that could have \textit{generated the point-identified parameters} by
\begin{equation*}
  \mathcal{M}_{\mathcal{S}} \equiv \{m\in \mathcal{M}: \Gamma_s(m) = \beta_s \text{ for all } s\in\mathcal{S}\},
\end{equation*}
where $\Gamma_s(m_0, m_1)$ is a linear functional corresponding to Equation~\ref{eq:identified_s}.

We can define a similar linear functional $\Gamma^*(m_0, m_1)$ for the target parameter, which then implies the \textit{identified set}
\begin{equation}\label{eq:identified_set}
  \mathcal{B}^*_\mathcal{S} \equiv \{b\in\mathbf{R}: b = \Gamma^*(m_0, m_1) \qquad \text{for some} \qquad m \in \mathcal{M}_\mathcal{S}\}.
\end{equation}

Proposition 2 in MST shows that $\mathcal{B}_\mathcal{S}^*$ is an interval in $\mathbb{R}$ as long as $\mathcal{M}$ is convex, except if $\mathcal{M}_\mathcal{S}$ is empty in which case the identified set is also empty.
In particular, the interval bounds are obtained as $\underline{\beta}^* \equiv \inf_{m\in\mathcal{M}_\mathcal{S}}\Gamma^*(m_0, m_1)$ and $\overline{\beta}^* \equiv \sup_{m\in\mathcal{M}_\mathcal{S}}\Gamma^*(m_0, m_1)$.


\paragraph{Identification: Computation}
Generally, the infinite-dimensional parameter space $\mathcal{M}$ precludes computation of the identified set.
MST instead propose to use a finite-dimensional basis spline approximation $\mathcal{M}_{fd}\subseteq{\mathcal{M}}$, for example a collection of order $k$ polynomials.
Parametrizing in terms of basis functions allows to restate the problem in terms of a finite number of basis function coefficients in a (Euclidean) space $\Theta$.
Since the maps $\Gamma^*, \Gamma_s$ are linear, the problem simplifies to a linear program.

The program, here for the upper bound, can be stated as follows:
\begin{align}\label{eq:lp_identification}
  &\overline{\beta}^* \equiv \sup_{(\theta_0, \theta_1)\in\Theta} \sum_{k=1}^{K_0}\theta_{0k}\Gamma^*_0(b_{0k}) + \sum_{k=1}^{K_1}\theta_{1k}\Gamma^*_1(b_{1k}) \\
  &\text{subject to} \qquad \sum_{k=1}^{K_0}\theta_{0k}\Gamma_{0s}(b_{0k}) + \sum_{k=1}^{K_1}\theta_{1k}\Gamma_{1s}(b_{1k}) = \beta_s \text{ for } s \in \mathcal{S}.
\end{align}
For a translation into a standard matrix form including an explanation for handling shape constraints, see Appendix~\ref{app_sec:linear_programs}.

Proposition 4 in MST shows that the identified set can be recovered exactly in cases where the $Z$ is discrete, and the target weights are piecewise constant in $u$.
Then, partitioning $[0,1]$ into a finite number of intervals defined by the discontinuities in the weights and the support of the propensity score, constant splines on these intervals are sufficient to recover the exact identified set.
This will be referred to as the \textit{nonparametric} bounds, since no further parametric assumptions are placed on $\mathcal{M}$.
Apart from constant splines, Sections~\ref{sec:simple_example} and~\ref{sec:simulation_studies} will consider Bernstein polynomials as basis functions, which are recommended by MST and allow to easily enforce certain shape restrictions.

Lastly, a \textit{sharp} identified set, in the sense of exploiting all information in the conditional means of $Y$, can be constructed (see Proposition 3 in MST).
For this, we can take $\mathcal{S} = \{I\{d=d', z=z'\} \text{ for } z'\in Supp_Z, d'\in\{0,1\}\}$.
This is also the sense in which \textit{sharp} is used throughout this study.
See the discussion following Proposition 3 for an application for continuous $Z$ and~\cite{marx2024sharp} for identified sets, that use other features of the data beyond the conditional first moments.

\paragraph{Estimation}
Estimation is complicated by the fact, that the equality constraints in~\ref{eq:lp_identification} might not hold with probability one in finite samples.
To address this, MST propose a two-way estimator that is guaranteed to exist.

The first step program is given by
\begin{equation*}\label{eq:lp_estimation_fs}
  \inf_{m'\in \mathcal{M}} \sum_{s\in\mathcal{S}}|\hat{\Gamma}_s(m') - \hat{\beta}_s|.
\end{equation*}
Denote the optimal value of the first step program by $\hat{\mu}$.
Then the second step program solves the following problem, here again for the upper bound:

\begin{align}\label{eq:lp_estimation}
  & \hat{\overline{\beta}^*} \equiv \sup_{m\in \mathcal{M}}\hat{\Gamma}^*(m) \\
  & \text{ subject to } \sum_{s\in\mathcal{S}}|\hat{\Gamma}_s(m) - \hat{\beta}_s| \leq \hat{\mu} + \kappa_n,
\end{align}
where $\kappa_n$ is a tolerance chosen by the researcher, that needs to converge to $0$ with the sample size.

Note, that in many applications, like the one in Section~\ref{sec:simple_example}, $\hat{\mu} = 0$, since there are more free parameters than identified estimands.
In particular, with $4\times2=8$ constant splines for the partition $U = [0, p(0), p(1), p(1) + \overline{u}]$, we can match the four point-identified moments $\gamma_1^{at}, \gamma_1^c, \gamma_0^c, \gamma_0^{nt}$ exactly, at least without any further shape restrictions.
With shape restrictions, like decreasing MTR or MTE functions, this might not generally be true.
In this case, $\hat{\mu}$ can be used for specification tests as pointed out by MST as well as~\cite{shea2023ivmte}.


\subsection{Inference Methods}\label{sec:inference_methods}
Our goal is to construct a random interval $CI$, such that it covers the true parameter of interest with a pre-specified probability $1-\alpha$, at least asymptotically.

As pointed out in the literature, there is a difference between constructing confidence intervals for the identified set and confidence intervals for the parameter of interest.
Intuitively, coverage for the true parameter is less demanding, since the true parameter can never be at the lower and upper bound \textit{simultaneously}.

As shown by~\cite{imbens2004confidence} it might hence be sufficient to construct the confidence interval from one-sided critical values of level $\alpha$, as opposed to $\alpha/2$, which is typically required for a $(1-\alpha)\%$ coverage.
~\cite{imbens2004confidence} also, however, note a uniformity issue with this confidence interval in their example: When the identified set converges to a point, using one-sided critical values will undercover.
They hence propose to use adjust critical values that take into account the ``degree'' of point-identification.
I will consider this in more detail when discussing coverage rates in the simulations in Section~\ref{sec:simulation_studies}.

% Formally, we seek a random interval $CI$, which, for any $\beta^* \in \mathbb{R}$, satisfies
% \begin{equation}
%   \lim_{n\to\infty} P(\beta^* \in CI) \geq 1-\alpha.
% \end{equation}
% If the above holds with equality we say the confidence interval has asymptotically exact coverage, if the inequality is strict we say it is conservative.
% [Briefly discuss differences to uniform coverage?]

If we knew the asymptotic distribution of $\hat{\underline{\beta}^*}$ and $\hat{\overline{\beta}^*}$ as defined by the two-step estimator in~\ref{eq:lp_estimation}, we would use
\begin{equation*}
  CI = \left[\hat{\underline{\beta}^*} - \frac{\underline{t}_{1-\alpha}}{r_n}, \hat{\overline{\beta}^*} - \frac{\overline{t}_{\alpha}}{r_n}\right]
\end{equation*}
where $\underline{t}_\alpha$ and $\overline{t}_\alpha$ refers to the $\alpha$ quantiles of the (scaled) upper and lower bound distributions and $r_n$ is an appropriate scaling sequence.

% \paragraph{Special Case: Plug-in Estimator}
% In special cases, as discussed in Section~\ref{sec:simple_example} explicit forms of the solution might be available.
% For example, with only $\beta_s = LATE(p(0), p(1))$ as the point-identified estimand and without any shape restrictions, $\overline{\beta}^* = \omega \beta_s + (1-\omega)$.
% Hence, we can easily derive the asymptotic distribution of the plug-in estimator using a sample analogue estimator for $\omega$ and, say, 2SLS for $\beta_s$ in combination with Slutsky's theorem.

% In particular, we know under the usual assumptions, that
% \begin{equation*}
%   \sqrt{n} \left(\hat{\omega}\hat{\beta}_s + (1-\hat{\omega}) - \overline{\beta}^*\right) \to_d N(0, \sigma^2),
% \end{equation*}
% where $\sigma^2$ is easy to estimate.
% Hence, after appropriate standardization we can use the quantiles of $\Phi$, that standard normal CDF, to construct a confidence interval.

% However, in those cases where $\overline{\beta_s}^*(\beta_s, \omega)$ exhibits kinks, the asymptotic distribution depends on the unknown point-identified parameter $\beta_s$, in the sense that the slope of the true upper bound, and hence the plug-in estimator, changes at $\beta_s=0$.
% While this might be easily addressed by a pre-test for $\beta_s=0$, as explained earlier, it would be desirable to have inference methods that do not depend on knowing the form of $\overline{\beta}^*(\beta_s, \omega)$ explicitly.
% The point of the linear program procedure is to compute identified sets with non-obvious solutions.
% The following sections survey some potentially applicably inference methods.

\paragraph{Resampling Methods: Nonparametric Bootstrap}

A typical procedure applied in situations, where the asymptotic distribution is known to be normal, but the asymptotic variance is hard to characterize explicitly, is the nonparametric bootstrap.
The qualification nonparametric (or sometimes ``standard'') derives from the fact, that the empirical distribution function is used to generate the bootstrap samples, as opposed to say a normal parametrization with an estimated mean and variance.
While widely applicable, and even potentially advantageous in some situations, the bootstrap has many known limitations, which has been shown at various places in the literature.
Some relevant limitations are outlined below.

The nonparametric bootstrap in our case would proceed as follows.
Our goal is to approximate the asymptotic distribution of
\begin{equation*}
  \sqrt{n}\left(\hat{\overline{\beta}^*} - \overline{\beta}^*\right),
\end{equation*}
which we denote by $\overline{G}$. Here, we assume both that $\overline{G}$ exists and that $\sqrt{n}$ is the correct rate.
\footnote{
  MST provide no distributional results. We conjecture the existence and rate based on examples in~\ref{sec:simple_example}, where the solution can be derived explicitly and inherits the rate from the underlying parameters.
  These are a finite number of sample moments and in our setting all converge at $\sqrt{n}$.
  This will also be true for more complicated settings, where we do not change estimation of the underlying parameters, but simply add more constraints to the linear program.
}
Given a sample of size $N$, we create $B$ resamples, also of size $N$, by drawing with replacement from the sample at hand.
We then compute the upper bound, i.e.\ solve the linear program, on each of these bootstrap samples.
Denote the resulting bootstrap estimates by $\left\{\hat{\overline{\beta}}_b^*\right\}_{b=1}^B$.

We then approximate the distribution of interest above, by the bootstrap distribution of
\begin{equation*}
  \sqrt{n}\left(\hat{\overline{\beta}^*_b} - \hat{\overline{\beta}^*}\right),
\end{equation*}
where the centering is now around the estimate on the original sample.

The goal is then to show, that under appropriate conditions, this distribution is in a certain sense asymptotically equivalent to our distribution of interest.
Hence, we can use the quantiles of the bootstrap distribution to conduct inference.
If $B\to\infty$, we can find these quantiles exactly.

% [Discuss usual general conditions? For example check Andrews papers or Lehman and Romano.]

\paragraph{Differentiability and the Bootstrap}
It is typically easy to establish the bootstrap consistency for estimators of the point-identified parameters $\hat{\beta}_s$ and nuisance parameters $\hat{\omega}$ (like the propensity score), at least if we ignore potential weak identification issues in the context of IV estimation.
\footnote{
  The inconsistency of the bootstrap for weakly-identified 2SLS is discussed in more detail in~\cite{andrews2010applications}.
  Inference for weakly-identified models in the MTE context would be another interesting direction for future research.
}
However, this does not generally imply the bootstrap consistency of a \textit{functional} $\phi(\hat{\beta_s}, \hat{\omega})$.
For example,~\cite{andrews2000inconsistency} shows that the bootstrap in inconsistent in the case where $\phi(x) = \max\{x, 0\}$.
In fact,~\cite{fang2019infdirdiff} show that, more generally, the standard bootstrap in these cases is valid for approximating the distribution of $\phi(\hat{\beta}_s, \hat{\omega})$ if and only if $\phi$ is fully differentiable at a conjectured $\theta_0$.
\footnote{
  This equivalence holds under a set of assumptions. Loosely, speaking, it is required that
  (1) $\phi$ is well-defined and Hadamard directionally differentiable,
  (2) the estimator $\hat{\theta}$ has some asymptotic distribution $\mathbb{G}_0$, and finally,
  (3) the bootstrap is consistent for the law of $\hat{\theta}$.
}

However, if the function is non-differentiable we might still estimate the asymptotic distribution based on a delta method for directionally differentiable functions.
The key difficulty is then finding an appropriate estimator of this directional derivative.
~\cite{fang2019infdirdiff} propose to construct estimators motivated by known analytical forms of the directional derivative, while~\cite{hong2018numerical} suggest constructing numerical approximations.
Section~\ref{sec:simple_example} shows how to apply these methods in a simple MTE model.
However, the analytical delta method has the downside of requiring explicit forms of the directional derivative, which is not immediate when the functional is the solution of a linear program.
The numerical delta method, on the other hand, requires specifying a step-size to constructing the derivative, a tuning parameter for which no guidance except high-level rate requirements seem to be available.
Hence, for the model in Section~\ref{sec:simulation_studies}, where no explicit solutions are available, I instead provide additional simulation results using subsampling.

\paragraph{Subsampling}
Subsampling is based on the idea, that given a dataset of size $N$, we can mimic the situation of sampling from a population, by drawing samples of size $B<N$ from the original dataset.
So while the bootstrap creates samples of size $N$ from the empirical distribution function (amounting to sampling \textit{with replacement}), subsampling takes samples of size $B$ from the actual distribution function, by sampling \textit{without replacement} from the dataset.
So both methods provide different approximations to drawing samples of size $N$ from the true distribution function.

However, it can be shown (e.g. Chapter 18.7 of~\cite{lehmann_romano2022}), that the requirements on subsampling are much weaker, than for the nonparametric bootstrap.
In fact, the most essential assumption is that the estimator of interest has \textit{some} asymptotic distribution.
Consistency holds, when the subsampling size diverges, but not faster than the sample size. That is, $\frac{B}{N}\to 0$, while $B\to \infty$, as $N\to\infty$.
So while potentially consistent in the cases considered in the following Sections where the nonparametric bootstrap is invalid, the main difficulty is justifying the subsample size $B$.
Despite, I study the performance of subsampling in the more complicated setting in Section~\ref{sec:simulation_studies} and explore the effect of different subsample sizes.

\paragraph{Other Approaches}
There are several other approaches beyond the scope of the current study, but that could be potentially applicable.
~\cite{bei2023inference} suggests a projection based approach that applies to the MTE model.
Projection approaches construct confidence sets for the solution of the linear program and then project them onto the optimal value space.
Generally, projection approaches tend to be conservative, which, for example, can be addressed by appropriately adjusting the critical values of the underlying test statistic.
Confidence sets for the solution of linear programs are also derived in~\cite{hsieh2022inference}, which might be used in a projection approach.
Lastly,~\cite{freyberger2015identification} and~\cite{bhattacharya2009inferring} derive asymptotic results for the optimal value.

\section{Bootstrap Invalidity: A Simple Example}\label{sec:simple_example}
I first illustrate the invalidity of the bootstrap in a simple example with a binary instrument where we only use the point-identified LATE for the instrument-compliers as the basis for extrapolation.
While smaller identified sets can be constructed using other point-identified estimands, it helps to illustrate the type of inference issues that can also arise in more complex setups.
In particular, we can calculate explicit solutions for the upper and lower bound viewed as functions of the point-identified complier LATE.\@
These will also point to potential adjustments for inference.

\subsection{Setup}
The outcome is denoted $Y_i$ with bounded support in $[0,1]$ for convenience.
The instrument $Z_i$ is binary with realizations in $\{0,1\}$.
Binary treatment $D_i$ is determined by the selection equation~\ref{eq:selection} and the assumptions for instrument validity hold.
We define the instrument $Z_i$ such that $p(0)<p(1)$.

In this model, we can point-identify a local average treatment effect for the instrument-compliers, namely those individuals, who take up the treatment if and only if $Z_i=1$.
In the selection model above, the complier subpopulation is those with $U_i$ realizations in $[p(0), p(1)]$.
Hence, in the notation of the model, we can point-identify,
\begin{equation}
  \beta_s = \E[Y(1) - Y(0)|U_i\in[p(0), p(1)]].
\end{equation}
Note that individuals with realizations $u \leq p(0)$ will always select into treatment, while individuals with $u \geq p(1)$ will never select into treatment.
To keep the notation light, I will refer to these subpopulations as \textit{complier}, \textit{never-taker} and \textit{always-taker}, or short, groups $g\in\{c, at, nt\}$.

Our goal will be to extrapolate from this identified complier-LATE to a larger subpopulation that also includes some never-takers.
This could correspond to introducing a redesigned policy with increased incentives for treatment take-up.
We denote the new target by
\begin{equation}
  \beta^* = \E[Y(1) - Y(0)|U_i \in[p(0), p(1) + \overline{u}]],
\end{equation}
where $\overline{u}\in[0, 1-p(1)]$.

Without very strong restrictions, $\beta^*$ will not be point-identified.
Instead, we will only generally be able to identify a set of values for the target that are both consistent with the data and theoretical assumptions we want to impose.
Denote the identified set by
\begin{equation*}
  \beta^* \in [\underline{\beta^*}(\beta_s, \omega), \overline{\beta^*}(\beta_s, \omega)],
\end{equation*}
where we emphasize the dependence of the upper and lower bound of the identified set on the point-identified parameter $\beta_s$.
Additionally, there might be a target specific nuisance parameter $\omega$, see below.

\subsection{Solution: Nonparametric Bounds}
In the simple case considered here with a single point-identified parameter, the solution to the linear program has an intuitive and easy to compute form.
Note the following solutions are referred to as \textit{nonparametric} in~\cite{mogstad2018using}, that is, they do not impose any parametric assumptions on the underlying MTR functions.
The corresponding linear program solution can be achieved by using constant splines defined on the grid $[0, p(0), p(1), p(1) + \overline{u}, 1]$.

Denote by $\beta_{\overline{u}}$ the LATE for the population of never-takers for which we want to extend the treatment, i.e.
\begin{equation*}
  \beta_{\overline{u}} \equiv E[Y(1) - Y(0) | u \in [p(1), p(1) + \overline{u}]].
\end{equation*}
Then $\beta^*$ is a weighted average of the two LATEs:\@
\begin{equation*}
  \beta^* = \omega\beta_s + (1-\omega)\beta_{\overline{u}}
\end{equation*}
where $\omega = \frac{p(1) - p(0)}{\overline{u} + p(1) - p(0)}$ is the relative share of compliers in the target population.
Finding the identified set then amounts to finding bounds on $\beta_{\overline{u}}$, where different restrictions imply different bounds.



\paragraph{Nonparametric Bounds}
Without any further restrictions, the only bounds we can give follow from the support of $Y_i$.
Since $Y_i\in[0,1]$ we have $\beta_{\overline{u}} \in [0,1]$ and hence
\begin{equation*}
  \beta^* \in [\omega\beta_s - (1 - \omega), \omega\beta_s + (1 - \omega)]
\end{equation*}
We can already see that this is a continuous and differentiable function of $\beta_s$.
Hence, if estimation for $\beta_s$ and $\omega$ is standard, the usual delta method could be applied.
This is the case here, since $\beta_s$ can be consistently estimated using the Wald estimator which --- assuming no weak identification issues --- is asymptotically normal.
$\omega$ is a function of sample moments and can be estimated consistently.
Therefore, also the nonparametric bootstrap is valid in this case.
\footnote{The only issue would arise at the boundary of the parameter space, that is when $\underline{\beta^*}(\beta_s, \omega)=-1$ or $\overline{\beta^*}(\beta_s, \omega)=1$.
These cases are of less interest practically, hence I disregard them. However, a similar issue arises under shape restrictions discussed below.}

\paragraph{Monotone Marginal Treatment Effect}
We can impose the assumption, that the treatment effects $E[Y(1) - Y(0)|U_i=u]$ are monotone in $u$.
In the language of~\cite{mogstad2018using}, the MTE function is monotone.
The economically most interesting assumption is a \textit{decreasing} MTE function, since treatment take-up is decreasing in $u$.

If the MTE function is decreasing, we know $\beta_{\overline{u}} \leq \beta_s$ and hence the identified set shrinks at the upper bound:
\begin{equation*}
  \beta^* \in [\omega\beta_s - (1 - \omega), \beta_s]
\end{equation*}
In this case, inference is again standard. Note we have point identification when $\beta_s=-1$.
\footnote{Convergence to point-identification can pose problems to the type of confidence intervals considered by~\cite{imbens2004confidence} and considered later here.
In particular, coverage does not hold uniformly over parameter sequence implying convergence to point-identification.
~\cite{imbens2004confidence} propose a simple adjustment that I discuss in more detail in Section~\ref{sec:simulation_studies}.}


\paragraph{Monotone Treatment Response}
Under monotone treatment response, it is assumed that all individuals respond either negative or positive to the treatment.
An implication for the marginal treatment effect is that it is either positive or negative everywhere.
This again tightens the set, but it also restricts the parameter space of $\beta_s$ that is consistent with the model.
In particular, any $\beta_s \leq 0$ is inconsistent with the assumption of positive treatment responses.
In addition, the lower bound will be tighter:
\begin{equation*}
  \beta^* \in \begin{cases}
    [\omega\beta_s, \beta_s + (1 - \omega)] & \beta_s \geq 0 \\
    \emptyset & \beta_s < 0
  \end{cases}
\end{equation*}
Here, inference becomes non-standard at the boundary of the parameter space.
This case is close to the stylized example analyzed in~\cite{andrews1999estimation}, where interest lies in estimating an expectation which is known to be positive.
% [TODO - Explain, why not focusing on this in the following.]

\paragraph{Monotone Marginal Treatment Responses}
A last restriction considered by~\cite{mogstad2018using} are monotonicity restrictions on the MTR functions $m_d(u) = E[Y(d)|U=u]$.
We consider the case of increasing MTR functions.
% Imposing restrictions on these functions is also the general approach discussed in Section~\ref{sec:general_mte}, since this allows to exploit other moments for identification.
% For example, in our model also $E[Y(1)|g=at]$ and $E[Y(0)|g=nt]$ are identified and can help in tightening the identified set.

With the restriction that $m_0(u), m_1(u)$ are increasing in $u$, the computation of the identified set becomes less immediate.
% Figure [MAYBEDO REF] illustrates the solution using constant spline basic functions defined separately over the intervals implied by $[0, p(0), p(1), p(1) + \overline{u}, 1]$.

The solution in this case is given by:
\begin{equation}\label{eq:solution_cs_increasing_mtr_upper}
	\overline{\beta^*}(\beta_s, \omega)=
	\begin{cases}
		\omega \beta_s + (1 - \omega),& \quad \text{if } \beta_s \geq 0\\
		\beta_s + (1 - \omega),              & \quad \text{if } \beta_s < 0,
	\end{cases}
\end{equation}
and
\begin{equation}\label{eq:solution_cs_increasing_mtr_lower}
	\underline{\beta^*}(\beta_s, \omega)=
	\begin{cases}
		\beta_s - (1 - \omega),& \quad \text{if } \beta_s \geq 0\\
		\omega \beta_s - (1 - \omega),              & \quad \text{if } \beta_s < 0.
	\end{cases}
\end{equation}

Hence, both bounds exhibit a kink at $\beta_s=0$.
In these cases, as was shown in~\cite{dumbgen1993nondifferentiable} and later~\cite{fang2019infdirdiff}, non-differentiability at the kink renders the nonparametric bootstrap invalid.
However, other methods have been devised for this case. For example,~\cite{fang2019infdirdiff} and~\cite{hong2018numerical} extend the delta method to the case of functions that are only directionally differentiable, such as the ones with a kink shown above.
Further, subsampling is theoretically justified as long as the bounds of the identified set have \textit{any} limiting distribution.

\subsection{Solution: Bounds with Bernstein Polynomials}
In most applications, the nonparametric identified set resulting from constant splines will feature solutions, i.e. MTR functions, that exhibit jump discontinuities.
% [MAYBEDO - Maybe put a solution in the appendix similar to Bernstein figures?].
The appeal of the MST framework is, that the MTR functions have direct economic interpretations that allow to place theoretical restrictions about which meaningful discussions are possible.
In particular, in most cases a certain degree of smoothness in these functions seems reasonable.

To implement this in a tractable manner,~\cite{mogstad2018using} propose to use Bernstein polynomials as basis polynomials for the MTR functions, because they exhibit many computational advantages.
For example, shape restrictions, including monotonicity of the MTR or MTE functions as well as monotone treatment response, can be easily implemented in a linear program.
\footnote{For details on the implementation of shape restrictions see Appendix Section~\ref{app_sec:linear_programs}.}

The Bernstein polynomial of degree $k$ is given by the sum over $k+1$ basis polynomials:
\begin{equation*}
  B_k(x) := \sum_{v=0}^n c_v b_{v,n}(x) \qquad \text{ where } b_{v,n}(x) := \binom{n}{v} x^v(1-x)^{n-v}, \qquad v = 0, \ldots, k.
\end{equation*}

For example, for $k=2$, we have
\begin{equation*}
  B_2(x) = c_0 (1-x)^2 + c_1 2x(1-x) + c_2 x^2.
\end{equation*}

Using these basis polynomials, it is easy to construct a linear program such that the implied MTR functions are smooth over $[0,1]$ and have images in $[0,1]$ (or any other interval, for that matter).
However, as shown in the following, the value function of the linear program, that is the upper or lower bound of the identified set, can still exhibit points of non-differentiability.

To illustrate, I return to the binary-IV example discussed above for nonparametric bounds.
Because the analytical solution is difficult to characterize explicitly for higher-order polynomials, I instead repeatedly solve the linear program over a range of parameter values for the complier LATE $\beta_s$.
\footnote{For a precise statement of the problem see Section~\ref{sec:general_mte} and Appendix Section~\ref{app_sec:linear_programs} for the linear program.}
I report results for a lower-order, and hence more restrictive polynomial of degree $2$ as well as an $11$th order polynomial similar to the numerical example in~\cite{mogstad2018using}.
\footnote{
  To construct the solutions, I first use DGPs with a constant spline MTR function defined on the sub-intervals of $[0, p(0), p(1), p(1) + \overline{u}, 1]$.
  To vary $\beta_s$ I change the coefficients on the constant spline on the complier sub-interval $[p(0), p(1)]$.
  While in principle this DGP is inconsistent with a Bernstein polynomial used in identification,
  note that the MTR functions are only used to compute $\beta_s$ and play no role otherwise.
}

Figure~\ref{fig:id_set_binary_iv_bernstein} plots the solution to the lower bound $\underline{\beta}^*(\beta_s, \omega)$ as a function of $\beta_s$.
In addition to the lower bound, the figure also shows the basis function coefficients that solve the linear program separately for $m_0$ and $m_1$.
The corresponding solution for the upper bound is symmetric, since no other shape restrictions are placed on the problem. Hence, I focus on the lower bound only for clarity.

While still continuous, these solutions again exhibit kinks, this time even without any shape restrictions.
While also visually notable, I attempt to heuristically estimate the position of the kinks by checking for discontinuities in approximations to the second derivatives.
These positions are indicated by gray lines and align well with the visual kinks.
% Think about this - solutions are always at corner! Equality constraints cut through space --> that's why some coefficients change.
% Kink is generated by cuts changing smoothly, but cutting through spaces which imply different slopes.
From the lower subplots, it is easy to see, that the kinks are associated with one basis function coefficient hitting a corner of the $[0,1]^d$ box.
At the points of non-differentiability, different basis function coefficients are then used to satisfy the changing equality constraint, but they have different slopes in the objective function.
% From the lower subplots it is easy to see, that the kinks are associated with corner solutions of the linear programs, where coefficients start to increase from 0 or decrease from 1.
This feature is present for both reported degrees, $k=3$ and $k=11$.

Notably, increasing the degree of the polynomial has three effects:
First, as expected, the lower bounds become smaller, since the space of possible solutions is larger.
Second, the solution exhibits more kinks. In fact, the number of kinks equals the number of basis polynomials.
Lastly, increasing the number of polynomials sharply increases the slope of the solution close to 1, that is the boundary of the parameter space.
If we think of an increasing sequence $k$, this could indicate a failure of Lipschitz continuity close to the boundary.

\begin{figure}

  \caption{Identified Sets for the Binary-IV Model with Bernstein Polynomial MTRs}\label{fig:id_set_binary_iv_bernstein}

  \centering
  \begin{subfigure}[b]{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../../bld/figures/binary_iv/id_bernstein_2_none_late.png}
      \caption{Degree $k=2$}\label{fig:id_set_binary_iv_bernstein_k_2}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../../bld/figures/binary_iv/id_bernstein_11_none_late.png}
      \caption{Degree $k=11$}\label{fig:id_set_binary_iv_bernstein_k_11}
  \end{subfigure}
\end{figure}

\paragraph{Shape Constraints}
When adding shape constraints, the qualitative features of the solution that can pose problems for inference do not change, see Appendix Figure~\ref{app_fig:id_set_binary_iv_bernstein_shape_restrictions}.
However, adding further shape constraints typically reduces the solution space and hence may increase boundary issues.
It also shifts the area featuring many kinks to settings of potentially higher empirical relevance.

\paragraph{Sharp Identified Set}
While only using $\beta_s$ is illuminating for illustration, in practice we would always use specifications with more point-identified parameters.
For example, we can use all specifications $E[YI\{Z=z\}I\{D=d\}]$, which is the sharp identified set in the sense of~\cite{mogstad2018using}.
While it can be shown that this avoids kink issues in the constant splines model, the Bernstein polynomial model still exhibits notable kinks, as can be seen in Appendix Figure~\ref{app_fig:id_set_binary_iv_bernstein_shape_restrictions_sharp}.
\footnote{For example, the bounds in~\ref{eq:solution_cs_increasing_mtr_upper} do not exhibit a discontinuity because using all conditional moments not only fixes the difference $\gamma_1^c - \gamma_0^c$ but their levels.}

In this setting, the parameter space for the point-identified estimands is $[-1,1]^4$, not counting the nuisance parameter $\omega$.
The relevant parameters are the conditional means of the potential outcomes for each group, in particular $\gamma_1^{at}, \gamma_1^c, \gamma_0^c$ and $\gamma_0^{nt}$ where $\gamma_d^g := E[Y_d|g(U) = g]$.
To construct the figure I fix two parameters --- $\gamma_1^{at}$ and $\gamma_0^{nt}$ --- and vary the complier responses such that
$\gamma_1^c = 0.5 + \beta_s/2$ and $\gamma_0^c = 0.5 - \beta_s/2$. Hence, we can plot the lower bound again as a function of $\beta_s \in [-1,1]$.

Note that kinks in this plot also correspond to points of non-differentiability in the parameter space $[-1,1]^d$.
That is, approaching a point of non-differentiability from the left and the right when viewed in terms of $\beta_s$ implies two sequences in $[-1, 1]^4$ that result in different slopes of the value function.


\paragraph{Summary}
In this section I considered a simple binary IV model which already exhibits some problems for inference in the MTE model:
First, many solutions to the MTE problem exhibit \textit{kinks} when viewed as a function of the point-identified parameters.
Kinks are present under certain shape restrictions for the nonparametric identified set using constant splines and are a general feature when using Bernstein polynomials.
Non-differentiability generally renders typical inference approaches like the nonparametric bootstrap inconsistent, as observed by~\cite{dumbgen1993nondifferentiable}.
Second, for Bernstein polynomials, the slope of the solutions close to the boundary of the parameter space exhibits increasing slopes over the degree $k$.
This could potentially indicate a failure of Lipschitz continuity.
Lastly, introducing shape restrictions, in particular monotonicity and sign constraints on the implied MTE function $m_1(u) - m_0(u)$, can significantly reduce the parameter space consistent with the model.
This can lead to parameter at the boundary issues, as, for example, discussed in~\cite{andrews1999estimation}.
% MAYBEDO Is Lipschitz continuity important?
% TODO Say we are not focusing on the boundary issues.


\subsection{Simulation: Bootstrap and Adjusted Delta Methods}
This section presents some simple Monte Carlo evidence on the failure of the nonparametric bootstrap in the case of constant splines (``nonparametric'' bounds) with the restriction of increasing MTR functions.
\footnote{All simulations in the present paper where performed on the Marvin High-Performance-Computing Cluster at the University of Bonn.
Code for MTE model simulations can be found at \url{https://github.com/buddejul/thesis}.}
I take the target $LATE(0.4, 0.8)$ and a binary instrument $Z$ with propensity scores $p(0) = 0.4 < 0.6 = p(1)$ and $P(Z=0)=P(Z=1)=0.5$.
The outcome data is generated as $Y(d) = \gamma_d^g + \varepsilon$ where $\varepsilon \sim N(0,\sigma^2)$ with $\sigma=0.1$ and $g=g(u)$ refers to the three subpopulations (complier, never-taker, always-taker).
We treat only the complier late $\beta_s = \gamma_1^c - \gamma_0^c$ as point-identified and the remaining parameters are chosen such that the true parameter is at the upper bound of the identified set.

I construct confidence intervals by estimating the quantiles of the asymptotic distribution of the estimated upper bound denoted by
\begin{equation}
  \sqrt{N}(\hat{\overline{\beta}^*} - \overline{\beta}^*) \to_d \overline{G},
\end{equation}
where $\overline{\beta}^*$ is the sample analogue of Equation~\ref{eq:solution_cs_increasing_mtr_upper}, and analogously for the lower bound.
In the current case, given the closed form solution it is easy to show using a CLT and Slutsky's theorem, that this estimator is asymptotically normal.
In particular, for the Wald estimator we have $\sqrt{N}(\hat{\beta_s} - \beta_s) \to_d N(0, \sigma^2_s)$ and $\hat{\omega} = \frac{\hat{p(1)} - \hat{p(0)}}{\hat{p(1)} + \overline{u} - \hat{p(0)}} \to_p \omega$.
Hence, $\sqrt{N}(\hat{\overline{\beta}^*} - \overline{\beta}^*) \to_d N(0, \overline{\sigma}^2)$ where $\overline{\sigma}^2 = I\{\beta_s \geq 0\}\omega^2\sigma^2_s + I\{\beta_s < 0\}\sigma^2_s$.
Clearly, this distribution discontinuously depends on the unknown true parameter $\beta_s$.

I consider estimates of the quantiles of $\overline{G}$ using the non-parametric bootstrap.
A more detailed description of the construction is given in Section~\ref{sec:inference_methods}.
In addition, I construct confidence intervals using the analytical delta method suggested by~\cite{fang2019infdirdiff} and the numerical delta method~\cite{hong2018numerical}.
Instead of directly bootstrapping $\hat{\overline{\beta^*}}(\beta_s, \omega)$, these methods bootstrap $\hat{\beta_s}$ (and $\hat{\omega}$) and then apply different estimators of the directional derivative.

\paragraph{Analytical delta method} The analytical delta method constructs an estimator based on the analytical form of the derivative.
Our case is in fact very similar to Example 1 considered in~\cite{fang2019infdirdiff}, who consider $f(\theta) = \max(0, \theta)$, which has a kink at $\theta=0$.
The directional derivative for our case when $\omega$ is known is given by
\begin{align}
  \overline{\beta^*}'(h) =
  \begin{cases}
    \omega h  &\text{if } \beta_s > 0 \\
    \max\{\omega h, -h\} & \text{if } \beta_s = 0 \\
    h         & \text{if } \beta_s < 0\\
  \end{cases}.
\end{align}
Hence, the slope is $\omega$ to the right and $1$ to the left of the kink.
When approaching the kink, the derivative depends on the sign of $h$, i.e.\ the direction:
When $\beta_s = 0$ and we are at the kink, approaching it from the right ($h>0$) gives slope $\omega$, approaching from the left ($h<0$) gives slope $1$.

This motivates a simple estimator, which effectively boils down to a pre-test, since we have an estimate of $\beta_s$.
In particular, consider the estimator
\begin{align*}
  \hat{\overline{\beta^*}}'(h) =
  \begin{cases}
    \hat{\omega} h  &\text{if } \sqrt{N}\frac{\hat{\beta_s}}{\hat{\sigma_s}} > \kappa_n \\
    \max\{\hat{\omega} h, -h\} & \text{if } |\sqrt{N}\frac{\hat{\beta_s}}{\hat{\sigma_s}}| \leq \kappa_n \\
    h         & \text{if } \sqrt{N}\frac{\hat{\beta_s}}{\hat{\sigma_s}} < -\kappa_n\\
  \end{cases},
\end{align*}
where $\hat{\sigma_s}$ is an estimator of the asymptotic standard deviation of $\hat{\beta_s}$.
Here, $\kappa_n$ is a tuning parameter that diverges at a rate slower than $\sqrt{N}$.
Then, whenever $\beta_s > 0$, $\sqrt{N}\frac{\hat{\beta_s}}{\hat{\sigma_s}}$ diverges more quickly than $\kappa_n$, hence we select the first condition with probability approaching one (equivalently when $\beta_s < 0$ we select the third condition).
When $\beta_s = 0$, $\sqrt{N}\frac{\hat{\beta_s}}{\hat{\sigma_s}}$ is stochastically bounded hence we select the middle equation with probability approaching one.
Importantly, tuning parameter choice in this context can be motivated by a type-1 error control, which separates this approach from many other approaches discussed later on (e.g.\ the numerical delta method and subsampling).
In the simulations below I take $\kappa_n = \sqrt{\ln(n)}$ which gives $\kappa_n(1000) = 2.63$ and $\kappa_n(10000) = 3.03$ resulting in slightly conservative error control.

This estimator, however, does not take into account the estimation error in using $\hat{\omega}$.
Hence, I construct the following estimator which takes into account both variability in estimating $\beta_s$ and $\omega$:
\begin{align*}
  \hat{\overline{\beta^*}}'(h, w) =
  \begin{cases}
    \hat{\omega} h + \hat{\beta_s} \omega + w &\text{if } \sqrt{N}\frac{\hat{\beta_s}}{\hat{\sigma_s}} > \kappa_n \\
    I\{h > 0\}(\hat{\omega} h + \hat{\beta_s} w) + I\{h<0\} h + w & \text{if } |\sqrt{N}\frac{\hat{\beta_s}}{\hat{\sigma_s}}| \leq \kappa_n \\
    h + w         & \text{if } \sqrt{N}\frac{\hat{\beta_s}}{\hat{\sigma_s}} < - \kappa_n\\
  \end{cases}.
\end{align*}
The distribution of interest $\overline{G}$ is then approximated by $\hat{\overline{\beta^*}}'(H^*, W^*)$ where $H^*,W^*$ are the (consistent) bootstrap estimates of the asymptotic distributions of $\hat{\beta_s}, \hat{\omega}$.

\paragraph{Numerical Delta Method}
~\cite{hong2018numerical} instead propose to approximate the derivative using a numerical estimator.
In particular, we use
\begin{equation*}
  \hat{\overline{\beta^*}}'_{s_n}(h, w) = \frac{1}{s_n} \{\overline{\beta^*}(\hat{\beta_s} + s_n h, \hat{\omega} + s_n w) - \overline{\beta^*}({\hat{\beta_s}, \hat{\omega}})\}.
\end{equation*}
We can then again approximate $\overline{G}$ by $\hat{\overline{\beta^*}}'_{s_n}(H^*, W^*)$.
The tuning parameter in this case is the step size $s_n$, which needs to converge to $0$ more slowly than $\sqrt{n}$.
Other than that, there seems to be no more guidance on tuning parameter choice in this case.
For the simulations I performed, $s_n = \frac{1}{\sqrt{n}}$ eventually worked best, although this convergence is slightly too fast.
Notably,~\cite{hong2018numerical} find several instances in their simulations where tuning parameter choices violating asymptotic requirements perform best, pointing to the general difficulty of justifying tuning parameters in their method.

\paragraph{Results}
Figure~\ref{fig:sims_simple_model} presents the resulting coverage probability for the three inference approaches and $N\in\{1000, 10000\}$ over a grid $\beta_s$.
I focus on the coverage and length around the kink. To construct a 95\% confidence interval for the parameter I construct one-sided confidence intervals for the upper and lower bound.
Since we are interested in coverage for the true parameter, I use one-sided critical values, i.e.\ using $\alpha=0.05$ instead of $\alpha=0.025$.

The nonparametric bootstrap clearly exhibits a dip in the coverage probability at the kink $\beta_s = 0$, also for the large sample size $N=10000$.
The two adjusted delta methods --- analytical and numerical --- on the other hand are conservative, with coverage probabilities exceeding the nominal level, at least around the kink.
Looking at the average length of the confidence interval, for $N=1000$ these methods result in much larger intervals.
For $N=10000$, however, they come close to the length of the bootstrap interval, which is consistent off the kink, while appropriately increasing the length of the confidence interval at the kink.

% MAYBEDO Provide some discussion. How useful is this? --> Analytical delta method for small cases.


\begin{figure}

  \caption{Monte Carlo Simulation: Simple Kink Example}\label{fig:sims_simple_model}

  \centering
  \begin{subfigure}[b]{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../../bld/simple_model/figures/coverage.png}
      \caption{Coverage}\label{fig:sims_simple_model_coverage}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../../bld/simple_model/figures/length.png}
      \caption{Length}\label{fig:sims_simple_model_length}
  \end{subfigure}

  % \begin{subfigure}[b]{0.49\textwidth}
  %     \centering
  %     \includegraphics[width=\textwidth]{../../bld/figures/binary_iv/id_bernstein_2_mte_monotone_late.png}
  %     \caption{Decreasing MTE ($k=2$)}\label{fig:id_set_binary_iv_bernstein_k_2_mte_monotone}
  % \end{subfigure}
  % \hfill
  % \begin{subfigure}[b]{0.49\textwidth}
  %     \centering
  %     \includegraphics[width=\textwidth]{../../bld/figures/binary_iv/id_bernstein_11_mte_monotone_late.png}
  %     \caption{Decreasing MTE ($k=11$)}\label{fig:id_set_binary_iv_bernstein_k_11_mte_monotone}
  % \end{subfigure}


\end{figure}

\section{Simulation Studies: Polynomial Bases and Shape Restrictions}\label{sec:simulation_studies}
% MAYBEDO Proofread
This section provides a simulation study for confidence interval construction with a setting close to actual applications.
That is, we focus on the sharp identified set (in the sense of~\cite{mogstad2018using}), use a flexible polynomial to enforce smoothness of the MTR functions, and impose various shape restrictions.
In this case, solving the linear program algorithmically is necessary, and no explicit solutions are available.
Hence, as a baseline for inference I for now only consider the performance of the nonparametric bootstrap and subsampling.
These resampling procedures are also the ones implemented by~\cite{shea2023ivmte}, although no further discussion or simulation results are provided.


\subsection{Simulation Design}
The baseline setup is the same as in Section~\ref{sec:simple_example}:
The instrument $Z$ is binary with support $\{0,1\}$ and has PMF $P(Z=1) = P(Z=0) = 0.5$.
The propensity score I use is $p(0) = 0.4 < p(1) = 0.6$.
Hence, the IV slope coefficient identifies the LATE for the subpopulation with $u\in(0.4, 0.6]$.
The target parameter of interest is an extrapolation to the never-taker subpopulation up to $u=0.8$.
There are two difficulties when designing these simulations: First, we need to use DGPs that are consistent with the assumptions we enforce, for example shape restrictions.
Second, the parameter needs to be at the boundary of the identified set such that we study the most difficult case for inference.

I follow MSTs notion of sharpness and parametrize the simulation in terms of the conditional moments of $Y$, or more precisely, those moments that are identified by the conditional moments.
In particular, I specify the following four moments, which can be identified from the data: $\gamma_1^{at}, \gamma_1^{c}, \gamma_0^{c}, \gamma_0^{nt}$, where again $\gamma_d^g = E[Y_d|g(U)=g]$.
Additionally, there are two more moments that cannot be identified but are required to parametrize the model: $\gamma_0^{at}, \gamma_1^{nt}$.


\paragraph{MTR Functions: Constant Splines}
In terms of the parametrization from above, the constant spline MTR functions are
\begin{equation*}
  m_d(u; \gamma) \equiv \sum_{g\in\{at, c, nt\}} I\{g(U) = g\} \gamma_{d}^g.
\end{equation*}
To mimic a bounded outcome in $[0,1]$, I take $\gamma_d^g\in[0,1]$.
The least favorable parameter position in terms of coverage is at the boundary of the identified set.
Without any further restrictions, this would occur at $\gamma_1^{nt} - \gamma_0^{nt} = 1$ for the upper, and $\gamma_1^{nt} - \gamma_0^{nt} = -1$ for the lower bound.
However, if we are using Bernstein polynomials, it is unclear, which values of the non-identified parameters imply parameter positions on the boundary as well as comply with any other shape restrictions.

\paragraph{MTR Functions: Bernstein Polynomials}
The goal is to find two Bernstein polynomials of the specified order $k$ for $m_0, m_1$, that
(1) are consistent with the point-identified moments $\gamma_1^{at}, \gamma_1^c, \gamma_0^c, \gamma_0^{nt}$,
(2) imply a parameter at the boundary of the identified set, and
(3) are consistent with any other shape restrictions.

To this end, I first solve the linear program for the identification problem using the desired Bernstein polynomial as the basis functions.
The values of the point-identified parameters in this problem are computed based on the constant spline MTR functions.
In this way, we can specify the DGP in terms of the parameters $\gamma_d^g$ (using constant splines) while generating Bernstein polynomial MTR functions.
The basis function coefficient vector that solves the linear program can then be used to construct the desired MTR functions as Bernstein polynomials.
By construction, it generates the point-identified parameters, implies a parameter at the boundary (either the upper or lower bound depending on whether we use the upper or lower bound linear program solution), and satisfies any shape restrictions that are imposed.
Note, however, that the linear program might not have a solution and hence no DGP satisfying these requirements exists.

\paragraph{Data}
Having constructed the MTR functions, I generate the data $\{Y_i, D_i, Z_i\}_{i=1}^N$ in the following way:
\begin{itemize}
  \item[1.] Draw $Z_i$ according to $P(Z_i=z) = 0.5$ and $U_i$ from $\text{Uniform}(0,1)$.
  \item[2.] Generate the treatment using the selection equation $D_i = I\{p(Z_i) \geq U_i\}$.
  \item[3.] Generate the outcome by $Y = D_i m_1(u; \gamma) + (1 - D_i) m_0(u; \gamma) + \varepsilon_i$,
  where $m_1, m_0$ are the parametrized MTR functions (see below), and $\varepsilon\sim N(0, \sigma^2)$ with $\sigma=0.1$ is additional, independent noise.
\end{itemize}

\paragraph{Parameter Grid}
Simulation studies are conducted over a grid of parameters to study performance around kinks.
I again use
\begin{align}
  \gamma_1^c = \beta_s / 2 + 0.5, \quad \gamma_0^c = -\beta_s / 2 + 0.5,
\end{align}
over a grid $[-1, 1]$ for the complier LATE $\beta_s$.
For the remaining point-identified parameters I take $\gamma_1^{at} = 0.75$ and $\gamma_0^{nt} = 0.2$.
This allows for DGPs consistent with decreasing MTR functions, a decreasing MTE, and a positive response everywhere.
\footnote{In the extreme, if we were to set $\gamma_1^{at} = 0$, the largest complier LATE consistent with a decreasing MTE function would be $0$, namely when $\gamma_0^{at}=0$.}

\paragraph{Shape Restrictions}
I provide simulation results for a model without shape restrictions, a model with positive monotone treatment response (a positive MTE function everywhere), and a model with a decreasing treatment effect (a decreasing MTE function).

\paragraph{Identified Set}
I focus on the case of the \textit{sharp} identified set in the sense of MST, which is likely the specification most used in practice.
This is the set that uses all conditional moments of $Y$.
Hence, in our case this will make use of $\gamma_1^{at}, \gamma_1^{c}, \gamma_0^{c}$ and $\gamma_0^{nt}$.

% TODO: I don't think we need this section?
% \paragraph{Inference Methods}
% Section~\ref{sec:simple_example} already provided some results on inference in a simple case with one point-identified parameter and constant splines.
% However, a prominent feature for of the MST approach is to allow for parametric, but flexible specifications of the MTR functions.
% When combined with shape restrictions, these problems are difficult to solve analytically and no explicit form of the solution will be available.
% Hence, it is also more difficult to construct estimators of the directional derivative that would be required for the analytical delta method by~\cite{fang2019infdirdiff}.
% \footnote{Some results might be available from sensitivity analysis in mathematical programming.~\cite{shapiro1991asymptotic} derives the Hadamard (directional) derivative for the optimal value functions of convex programs, that does not seem to apply here though \citep{bhattacharya2009inferring}.
% ~\cite{de_wolf_smeers2021generalized} derive a particular differential of the optimal value with respect to matrix coefficients in the presence of potentially non-unique solutions.
% This is not pursued any further here, but might be a fruitful direction.}

% Instead, for the Bernstein case I focus on resampling techniques in the following section.
% As a baseline, I investigate performance of the nonparametric bootstrap.
% Additionally, I also provide results on subsampling, which is typically employed in cases where the nonparametric bootstrap might be inconsistent, but some asymptotic distribution is believed to exist.

\subsection{Simulation Results}
Figure~\ref{fig:simulation_binary_iv_bernstein} reports the main simulation results for the default setting of one-sided critical values,  a subsampling share of $0.1\times N$ and a linear program tolerance $\kappa_n = \frac{1}{N}$.
\footnote{Simulations were performed using my own Python implementation of the MTE approach. See Appendix Section~\ref{app_sec:pyvmte} and \url{https://github.com/buddejul/pyvmte}.}
Nominal coverage is $95$\% throughout all simulations. The first row considers a model without shape constraints, while the second row imposes monotone treatment response and the third a decreasing MTE function.
The red line indicates the lower bound, which is also the true parameter position.
The second column for each row focuses on a subset of $\beta_s$ for which inference is most difficult.
Appendix Figure~\ref{app_fig:simulation_binary_iv_bernstein_means} plots the corresponding mean bound and confidence interval estimates.
\footnote{Note this illustrates the consistency of the bound estimators proven in~\cite{mogstad2018using}, but for which no simulation studies seem to be available.}

Looking at the results in Figure~\ref{fig:simulation_binary_iv_bernstein} immediately reveals, that in this setting the bootstrap as well as subsampling perform well for some $\beta_s$ but have bad finite sample coverage in large regions.
Coverage rates are generally close to the nominal level for smaller values of $\beta_s$ where the lower bound is close to linear with few kinks.
For larger $\beta_s$ coverage is typically far below the nominal level. This is true for both $N=1000$ and $N=10000$ as well as both bootstrap and subsampling.
Coverage generally deteriorates towards the boundary, but is increasing in the sample size.
This is also evident in the means reported in Figure~\ref{app_fig:simulation_binary_iv_bernstein_means}.
The estimators for both the upper and lower bound are close to the true bounds in most cases, especially in regions where the bounds are ``well-behaved''.
Generally, the estimator for the larger sample size is also on average closer.
However, when focusing on the boundary regions for large $\beta_s$, estimates of the lower bound for the largest $\beta_s$ tend to be further away from the true bounds on average.
The following paragraphs consider variations in some simulation parameters: The subsampling share, the linear program tolerance $\kappa_n$, and the way in which confidence intervals are constructed.

\begin{figure}

  \caption{Simulation Results for the Binary-IV Model with Bernstein Polynomial MTRs}\label{fig:simulation_binary_iv_bernstein}

  \centering
  \begin{subfigure}[b]{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../../bld/figures/pyvmte_sims/1n/sims_binary_iv_sharp_none_coverage.png}
      \caption{No Shape Restrictions}\label{fig:simulation_coverage_none}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../../bld/figures/pyvmte_sims/1n/sims_binary_iv_sharp_none_coverage_problematic_region.png}
      \caption{Problematic Region}\label{fig:simulation_coverage_none_problematic}
  \end{subfigure}

  \begin{subfigure}[b]{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../../bld/figures/pyvmte_sims/1n/sims_binary_iv_sharp_monotone_response_coverage.png}
      \caption{Monotone Response}\label{fig:simulation_coverage_monotone_response}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../../bld/figures/pyvmte_sims/1n/sims_binary_iv_sharp_monotone_response_coverage_problematic_region.png}
      \caption{Problematic Region}\label{fig:simulation_coverage_monotone_response_problematic}
  \end{subfigure}

  \begin{subfigure}[b]{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../../bld/figures/pyvmte_sims/1n/sims_binary_iv_sharp_mte_monotone_coverage.png}
      \caption{MTE Decreasing}\label{fig:simulation_coverage_mte_monotone}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../../bld/figures/pyvmte_sims/1n/sims_binary_iv_sharp_mte_monotone_coverage_problematic_region.png}
      \caption{Problematic Region}\label{fig:simulation_coverage_mte_monotone_problematic}
  \end{subfigure}

  \floatfoot{\textbf{Notes}: This figure reports simulated coverage probabilities for the true parameter for the binary-IV model with Bernstein polynomials of degree $k=11$ under various shape constraints.
  The true parameter is at the lower bound. Confidence intervals are constructed using one-sided $\alpha = 0.05$ critical values for a nominal coverage of $95$\%.
  Number of simulations = 2000, subsampling share = $0.1\times N$, $\kappa_n = \frac{1}{N}$.}
\end{figure}


\paragraph{Subsampling Size}
The tuning parameter for subsampling is the size of the subsample.
Beyond the conditions required for asymptotic theory, namely that the subsample size needs to diverge, but no faster than $N$ so that $B/N \to 0$, little further guidance seems to be available.
In our case, we are resampling a ratio estimator for $\beta_s$ which is known to have poor behavior in small samples.
Hence, using $s = \frac{B}{N}$ too small should yield worse coverage, since any sample of size $B$ is a poor approximation to a sample of size $N$.

Appendix Figure~\ref{app_fig:sims_add_subsample_share_tolerance} reports simulation results in the case without shape restrictions using different subsample share $s \in \{0.05, 0.1, 0.2\}$.
In general, the coverage probability seems to be increasing in $s$, with slightly worse coverage for $s=0.05$ and slightly better one for $s=0.2$ compared to the default.
However, the differences are only small and the subsample share hence cannot explain the coverage probabilities far below the nominal level.

\paragraph{Linear Program Tolerance $\kappa_n$}
~\cite{mogstad2018using} provide no discussion for the linear program tolerance $\kappa_n$ in their paper.
The only requirement stated in the main text is that $\kappa_n \to 0$ at an ``appropriate rate''.
The proof in the appendix is provided for a more general convex loss function and distinguishes between the rate of convergence of $\hat{\Gamma}, \hat{\beta}$ denoted $r_n$ and a tolerance parameter $\tilde{\kappa}_n$ (which I denote differently for clarity).
The requirement is $\frac{\tilde{\kappa}_n}{r_n} \to 0$ and $\tilde{\kappa_n}\to \infty$. Hence, if $r_n = \sqrt{n}$, $\tilde{\kappa}_n$ may diverge at $n^\delta$ for $\delta \in (0, 1/2)$.
Hence, $\kappa_n = \frac{1}{n^{1/2 - \delta}}$ and the upper bound is given by $\frac{1}{n^{1/2}}$.

~\cite{shea2023ivmte} take the tuning parameter as a multiple of the minimal deviations from the first step program, namely $\kappa_n = \sigma \hat{\mu}$. They set $\sigma = 10^{-4}$ as the default.
This does not comply with the asymptotic requirement since this converges at rate $r_n$ to zero.
Note that this sets the tolerance to zero whenever the minimal deviations are zero, that is, whenever there is an exact solution satisfying the equality constraints.
They also provide no further discussion on this choice.

In practice, for the simulations above I found $\kappa_n = \frac{1}{n}$ to perform reasonably well for estimation (cf. Appendix Figure~\ref{app_fig:simulation_binary_iv_bernstein_means}).
In addition, I also consider $\kappa_n = \frac{1}{\sqrt{n}}$ and $\kappa_n = \frac{1}{n^2}$.
For the larger tolerance $\frac{1}{\sqrt{n}}$, the set of feasible solutions is larger, hence we expect smaller estimates for the lower bound and larger estimates for the upper bound.
The reverse should hold for the smaller tolerance $\kappa_n = \frac{1}{n^2}$.
Results for simulations using these tolerances are reported in Appendix Figure~\ref{app_fig:sims_add_subsample_share_tolerance}.

Clearly, for the larger tolerance, the estimator performs considerably worse with mean estimates being much wider.
Hence, the resulting coverage probability is typically equal to one.
Using $\frac{1}{n^2}$, which comes close to the default tolerance of $0$ implicit in~\cite{shea2023ivmte}, no systematic changes to using $\frac{1}{n}$ are evident.
Hence, the coverage issue does not seem to result from choosing an inappropriate tuning parameter.

\paragraph{Critical Value}
A potential concern with the confidence intervals presented above is poor finite sample coverage whenever we are close to point-identification.
Clearly, with point-identification using $c_\alpha$ instead of the usual $c_{\alpha/2}$ will result in a too small confidence interval.
More generally, as pointed out by~\cite{imbens2004confidence}, using the one-sided critical value $c_\alpha$ has a uniformity issue that can translate into poor finite sample coverage.
In a missing data setting, they show that while for every fixed interval length, using $c_\alpha$ is asymptotically justified, for any fixed $N$ it is possible to find a DGP that results in finite sample coverage approaching $1-2\alpha$.

To remedy this issue they propose an adjusted critical value $\tilde{c}\in[c_{\alpha}, c_{\alpha/2}]$ that depends on the length of the confidence interval.
Let $\Delta$ denote the length of the identified set, $\sigma_l$ and $\sigma_u$ the asymptotic variances of the estimators for the upper and lower bound.
Then the proposed adjusted critical value $\tilde{c}$ is implicitly defined by
\begin{equation}\label{eq:imbens_manski_crit}
  \Phi\left(\tilde{c} + \sqrt{N}\frac{\hat{\Delta}}{\max(\hat{\sigma}_l, \hat{\sigma}_u)}\right) - \Phi\left(-\tilde{c}\right) = 1 - \alpha.
\end{equation}
Here, $\hat{\cdot}$ denote estimators of their corresponding population quantities.
~\cite{imbens2004confidence} provide conditions under which a confidence interval using this critical value has uniform coverage.
However, it is unclear whether their assumptions are holding in the current setting.
For example, their Assumption 1(i) requires the estimators for the lower and upper bounds to be jointly asymptotically normal, which, has not been established.
Further, as argued throughout, in our setting it is not clear how to derive consistent estimators for the asymptotic variance.
However, their construction still provides a way to derive quantiles for the bootstrap that lie somewhere between $\alpha$ and $\alpha/2$ and depend on the length of the confidence interval.

The idea is implemented as follows:
First, I estimate the interval length by $\hat{\Delta} = \hat{\overline{\beta}^*} - \hat{\underline{\beta}^*}$.
Similarly, the variances are estimated using the simulated standard deviation of the bootstrap distribution, $\hat{\sigma}_l^*$ and $\hat{\sigma}_u^*$.
\footnote{Clearly, these estimators are not consistent if the bootstrap is not consistent, but might have at least the right order of magnitude.}
I then solve equation~\ref{eq:imbens_manski_crit} for $\tilde{c}$.
Lastly, I take $\tilde{\alpha} = 1 - \Phi(\tilde{c})$ to determine the bootstrap percentiles for constructing the confidence interval.

Appendix Figure~\ref{app_fig:sims_add_alpha_im_crit} reports coverage probabilities using bootstrap percentiles based on one-sided critical values and Imbens-Manski critical values.
There is almost no difference in the coverage rates.
Looking at the implied (average) $\tilde{\alpha}$, this results from $\tilde{\alpha} \approx 0.05$.
To illustrate, the typical bootstrapped standard deviations are of order $1$ to $10$.
Hence, with $N = 10,000$ and taking $\max(\sigma_l, \sigma_u)=10$ (larger variances result in smaller $\tilde{\alpha}$), we have
\begin{equation*}
  \Phi\left(\tilde{c} + 100\Delta\right) - \Phi\left(-\tilde{c}\right) = 1 - 0.05.
\end{equation*}
For $\Delta=0.1$, the solution would be $\tilde{\alpha} = 0.046$, for $\Delta=0.01$, which is far smaller than any of the estimated lengths, we would get $\tilde{\alpha} = 0.028$.
Hence, based on this simple heuristic we shouldn't expect the coverage problems above to result from convergence to point-identification.

\section{Convex Relaxation}\label{sec:convex_relaxation}
Given the difficulty of inference evident in the simulation results of the previous section, we now provide some suggestions for a conservative inference approach.
The main idea is to provide a conservative approximation to the solutions of the linear program $\overline{\beta^*}, \underline{\beta^*}$ by solving a slightly relaxed, but nonlinear program.
In particular, we will replace the box constraint $x\in[0,1]^d$ on the basis coefficients by a larger and strictly convex set $\mathcal{A}$.
\footnote{In the following, I will drop the \textit{strict} qualifier and refer to this as convex relaxation. The set $[0,1]^d$ is of course also convex (but not strictly so).}

Solving a non-linear program has two potential advantages:
First, if the resulting value function is (fully) differentiable in the underlying parameters, then the bootstrap is consistent~\citep{fang2019infdirdiff}.
Second, if this is not the case, existing results on the asymptotic distribution of the value function derived in~\cite{shapiro1991asymptotic} can be used for inference on the value of the non-linear program.
The disadvantage is replacing the easily solvable linear program with a more difficult nonlinear one.
Asymptotic theory might be possible by letting the feasible region $\mathcal{A}$ shrink to $[0,1]^d$, probably at a sufficiently slow rate.
The rest of the section spells out the idea in more detail and provides a ``proof-of-concept'' simulation for a two-dimensional problem with box constraints.

\subsection{Illustration}

\paragraph{Convex Relaxation of Linear Program}
We want to perform inference on the value function of a linear program.
\begin{equation}
	\min_{x\in [0,1]^2} c'x.
\end{equation}

Here, $x$ are the choice variables, $c$ is a coefficient vector, and the constraints are implicit in the restriction $x \in [0,1]^2$.
In standard form these are four constraints, namely $x \leq 0, -x \leq 0$, where inequalities are component-wise.
Denote the value function as a function of the parameter $c$ by $\phi(c) := \min_{x\in[0,1]^d} c'x$ for slope vectors $c\in \mathbb{R}^2$.

For the linear program, the feasible region is a unit square centered at $(0.5, 0.5)$. Solutions are unique as long as $c$ has no $0$-entry.
If $c=\mathbf{0}$, all $x\in [0,1]^2$ are optimal. If $c_j = 0 \neq c_i$, there are infinitely many solutions with $c_j \in [0,1]$.
Whenever $c_1\neq0$ and $c_2\neq0$ we have a unique solution that appears at one of the corners of $[0,1]^2$ depending on the signs of $c_1, c_2$.

Assume $c_i, c_j \neq 0$ so we have a unique solution.
The first panel in Figure~\ref{fig:convex_relax_value} plots the value function for the linear program with a box constraint and several strictly convex constraints, to be described below.
We fix $c_2 = 1$ and plot the value function as a function of $c_1$ only.
Figures~\ref{fig:convex_relax_value} and~\ref{fig:convex_relax_solution} plot the value function and corresponding solution.
The associated value function exhibits a clear kink at $c_1 = 0$: Whenever $c_1 > 0$, $x = (0, 0)$ is optimal, whenever, $c_1 < 0$, $x=(1,0)$ is optimal.

Now the idea is, to slightly enlarge the box $[0,1]^2$ to a set $\mathcal{A}$ that is strictly convex.
A first natural idea is the circle centered at $(0.5, 0.5)$ with radius $r=0.5$.
This set is defined by $\mathcal{A} = \{x \in \mathbb{R}^2: (x_1 - 0.5)^2 + (x_2 - 0.5)^2 \leq 0.5\}$.

The resulting program is
\begin{equation}
	\min_{x\in\mathcal{A}} c'x,
\end{equation}
which is no longer a linear program, since the constraints are not linear.
However, this is a convex optimization problem for which optimization algorithms are available.
% \footnote{In fact, it has an easy to characterize analytical solution that can be derived using a Lagrangian.}

Figure~\ref{fig:convex_relax_value} $(k=2)$ shows the resulting value function.
Since the feasible region is larger, the value lies below the value of the linear program.
Also, the solution no longer features a kink.
Intuitively, solutions are now at points of tangency between the level curves of the objective and the border of the circle.
Since we smoothed out the edges, points of tangency now continuously change when $c$ changes.
This smooth transition is also evident in the solutions in Figure~\ref{fig:convex_relax_solution}.

However, the solution is visibly smaller (or in our interpretation: too conservative).
So maybe $\mathcal{A}$ is too large.
To construct a tighter strictly convex set containing $[0,1]^2$ we can take any other higher $l_p$ norm and recenter it.
In particular, we have
\begin{equation*}
	(x_1-0.5)^k + (x_2-0.5)^k \leq c
\end{equation*}
where $c = 2(1-0.5)^k$ so $[0,1]^d \subset \mathcal{A}$.
\footnote{An alternative is to use the intersection of multiple lower order polynomials with varying slopes on one of the coordinates. For example,
\begin{align}
	& s(x_1 - 0.5)^4 + (x_2 - 0.5)^4 \leq u(s), \\
	& (x_1 - 0.5)^4 + s(x_2 - 0.5)^4 \leq u(s), \\
\end{align}
where $s$ is now the tuning parameter with $s\to\infty$ implying $\mathcal{A} \to [0,1]^d$.}

Figure~\ref{fig:convex_relax} additionally plots the resulting solutions for $k\in\{4, 10, 20\}$.
The feasible region shrinks towards the unit square as $k$ increases.
Hence, the value function is also closer to the original linear program one, while still being smooth.

\paragraph{Monte Carlo}
I study the performance of the proposed method in a simple Monte Carlo experiment, using the setting as described above with $c_2=1$.
\footnote{The code to reproduce these simulations can be found under \url{https://github.com/buddejul/lp_relax}.
Simulations were again performed on the Marvin cluster.}
$c_1$ is treated as unknown, but we have noisy measurements $X_i \sim N(c_1, 1)$ so $\hat{c_1} = \frac{1}{N}\sum_i X_i$ is a consistent estimator.
The goal is to construct lower one-sided confidence intervals that cover the value function with a pre-specified probability.

To solve the optimization problems I tried several algorithms accessible via the \textit{optimagic} Python package~\citep{Gabler2024}.
The most suited to this small problem was found to be the \textit{SciPy}~\citep{scipy} implementation of the COBYLA algorithm~\citep{cobyla_powell1994direct}.
However, for problems with more dimensions other algorithms perform better, see the discussion below.
As an initial parameter guess for the nonlinear problems I use the linear program solution.

Confidence intervals are constructed based on the nonparametric bootstrap both for the linear program and for its convex relaxation.
For the linear program, the bootstrap is inconsistent at $c_1=0$ because the value function is not fully differentiable.
Hence, we would expect poor finite sample performance at and around the kink.
For the convex relaxation, the bootstrap is justified if the value function is smooth.
Of course, as $\mathcal{A}\to [0,1]^d$ we would expect similar issues to arise with the value function of the relaxed program.
If the value function of the relaxed problem is also only directionally differentiable, an alternative might be to perform inference based on a functional delta method derived by~\cite{shapiro1991asymptotic} (see below).

Figures~\ref{fig:convex_relax_ci_coverage} and~\ref{fig:convex_relax_ci_mean} report simulated coverage probabilities and mean confidence intervals.
I focus on the region $[-0.2, 0.2]$ around the kink and construct intervals with a nominal coverage of $95\%$.
First, the confidence interval based on the nonparametric bootstrap has poor finite sample coverage close to the kink. For $N=1000$, coverage is slightly smaller than $65$\%, while for $N=10000$ coverage only increases to about $85$\%.
Looking at the convex relaxation, for $k=2$ and $k=4$, inference is clearly too conservative as both confidence intervals have coverage probability one and are much larger than the bootstrap interval throughout.
\footnote{For the one-sided lower confidence interval I refer to the length in terms of the lower bound, where a larger lower bound means a smaller confidence interval.}
For tighter approximations $k=10$ and $k=20$, on the other hand, coverage probabilities are higher than for the bootstrap with only slightly larger average confidence intervals.
However, for $k=20$, the feasible region is potentially to close to the original linear program, and for $N=1000$ we again have coverage below the nominal level close to the kink.

\begin{figure}

  \caption{Convex Relaxation: Value Functions, Solutions, CI Coverage and Mean}\label{fig:convex_relax}

  \centering
  \begin{subfigure}[b]{0.49\textwidth}
      \centering
        \includegraphics[width=\textwidth]{../figures/relax/value_function_by_constraint_set_dim_2.png}
        \caption{Value Functions}\label{fig:convex_relax_value}
      \end{subfigure}
      \hfill
      \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../figures/relax/solutions_by_constraint_set_dim_2.png}
        \caption{Solutions}\label{fig:convex_relax_solution}
      \end{subfigure}

      \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../figures/relax/covers_lower_one_sided_by_method.png}
        \caption{One-Sided CI:\@ Coverage}\label{fig:convex_relax_ci_coverage}
      \end{subfigure}
      \hfill
      \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../figures/relax/lower_ci_one_sided_by_method.png}
      \caption{One-Sided CI:\@ Mean}\label{fig:convex_relax_ci_mean}
  \end{subfigure}

\end{figure}

\subsection{Discussion}
The foregone illustration already points to several of the weaknesses of this approach, which will be discussed in the following.

\paragraph{Inference on the Approximation}
The essential idea is to provide a (slightly) conservative approximation, for which we can perform inference.
However, even in this two-dimensional case, whenever $c_2=0$, the value function will feature a kink.
When $c_2=0$, level curves of the objective are parallel to the $x_2$-axis.
If $c_1 > 0$, the point of tangency is at the boundary of the feasible region where $x_1$ is minimal, for $c_1 < 0$ at the boundary where $x_1$ is maximal.
These points occur at $x_2=0.5$.
Call these values $x_1^{\min}, x_1^{\max}$.
The value function is then given by $I\{c_1 \leq 0\} c_1 x_1^{\max} + I\{c_1 > 0\} c_1 x_1^{\min} < 0$.
Since $x_1^{\max} > |x_1^{\min}|$, the value function has a kink at $c_0$.
However, $c = 0$ should be excluded in most applications of interest.
Specifically, in our case, choosing a relevant target parameter should always ensure that $c$ is bounded away from zero.
In the simple example, this would only include the situation where both $p(0) \approx p(1)$ and $\overline{u} \approx 0$, but the latter is chosen by the researcher.

More generally, even if the value of the nonlinear program features kinks, inference might still be possible based on a similar adjusted delta method as in Section~\ref{sec:simple_example}.
~\cite{shapiro1991asymptotic} Section 3 discusses several adjustments of the delta method for inference on the objective function in mathematical programming.
In particular, Theorem 3.1 provides the Hadamard directional derivative for the value function when only the objective has a stochastic component.
Further, it is noted that if the solution is unique, then this derivative is linear and the value function is hence \textit{fully} differentiable.
In that case, as expected, if the asymptotic distribution of the stochastic part of the value function is normal, so is the optimal value.
This would be the case in our example since $\sqrt{N}(\hat{c_1} - c_1) \to_d N(0,\sigma^2)$ then $\sqrt{N}(\hat{\phi}_n - \phi_0) \to_d N(0,\sigma^2)$.
For the result see Theorem 3.3 and the preceding discussion.
If we exclude $c=0$ from the parameter space, then the example problem above has a unique solution everywhere and hence the above theorem applies.
\footnote{We also require $\mathcal{A}$ to be compact, which is true since it is closed and bounded.
Further, (a) $g(x, \omega) = \hat{c_1}x_0$ is measurable for all $x \in \mathcal{A}$, (b) it has a finite second moment, and (c) there exists a dominating function since $g$ is linear in $x$.}
% [TODO Is this so relevant here?]
% Currently it reads: (1) There might be a kink at c = 0. We can exclude it. (2) If we don't want to then
% Maybe write: Not relevant here since c=0 is the only kink, but maybe we have other kinks in higher dimensions so might still want the derivative?

If all or some of the constraints are unknown, equivalent asymptotic results can be derived in settings with convex inequality constraints.
See Theorems 3.4 for the directional derivative and Theorem 3.5 for the asymptotic distribution.
Again, for a unique solution, this distribution is normal, with variance equal to the variance of the Lagrangian evaluated at the optimal solution.
Hence, this takes into account uncertainty both in the objective function and the constraints.
Since the $l_1$ norm in the MTE estimation linear program is also convex, replacing the box constraint by a convex relaxation would give such a program.

\paragraph{Choosing $\mathcal{A}$}
Figure~\ref{fig:convex_relax_solution} shows a potential problem with norm-based relaxations $\mathcal{A}$:
The solution is too far away from the linear solution away from the kink, but changes too quickly at the kink, especially for high $k$.

Even given a set $\mathcal{A}$ that has desirable properties, the main challenge is motivating the tuning parameter, e.g. $k$ above.
While asymptotic theory probably puts some requirements on the rate of convergence to points of non-differentiability (e.g.\ slow ``enough'' relative to estimation rates), it is unclear how to motivate this choice in finite sample.
How useful such an approach would be in practice hinges on the ability to motivate a tuning parameter choice, which, for example, is the main problem with subsampling.
One potential source of information could be solving the estimation problem at hand under increasing relaxations as well as comparing the degree of conservativeness to an initial (biased) bootstrap estimate of the standard error.

\paragraph{Optimization Problem}
Linear programs can be solved reliably and fast to global optimally, in particular in many relevant applications of the MTE method.
The nonlinear problem resulting from the convex relaxation is, however, more difficult to solve in general.
In particular, numerical instability can be a concern when $\mathcal{A}$ is defined by a large $k$.
Also setting up the problem correctly takes more care and different optimizers with different strategies have to be employed on a case-by-case basis.

To illustrate these difficulties, I consider one of the MTE problems studied in Section~\ref{sec:simple_example}, namely a Bernstein polynomial of degree 11 with no shape restrictions, implying $2\times12=24$ parameters.
Four equality constraints for the point-identified estimands reduce the number of free parameters to $20$; reparameterization is performed in the background by \textit{optimagic}.
The unit box $[0,1]^{24}$ is the only other constraint and I again relax it by the recentered norms of degree $k$.
Appendix Figure~\ref{app_fig:convex_relax_mte_bernstein_11} plots the resulting approximations to the lower bound.
Sub-figure (a) provides approximations for varying $k$, while sub-figure (b) considers solutions for $k=20$ with multiple algorithms.
Notably, to get a close enough approximation, $k=20$ or $k=40$ is required. However, the solution is visibly unstable.
Different optimization routines result in vastly different solutions, pointing to convergence issues, and with large $k$ numerical precision becomes a concern.
However, this is not meant to show, that numerical optimization in these higher dimensional, yet simple problems is impossible, but merely to point out that several additional difficulties are introduced by relaxing the constraint that are not present with a pure linear programming approach.


\section{Conclusion}
This study was concerned with statistical inference in the MTE model, when point-identified parameters and theoretical restrictions are used to partially identify a target parameter of interest.
While the study of inference methods is still incomplete, some conclusions can be drawn:
First, in the most simple models it might be worthwhile to derive explicit formulas for the identified set instead of blindly bootstrapping the linear program.
Depending on the exact setting, the solutions will feature kinks in the identified parameters and other boundary issues might arise.
For the cases where explicit solutions are known, inference might be adjusted based on forms of the directional derivative as suggested by~\cite{fang2019infdirdiff}.

If the setting is more complex, for example because we want to employ more point-identified parameters, and place flexible parametric as well as shape restrictions, more care seems to be required.
The simulation studies here show, that the bootstrap as well as subsampling can lead to coverage rates far below the nominal level whenever a flexible parametric specification is used.

However, the current simulations are also missing some promising inference methods:
~\cite{bei2023inference} proposes conservative inference based on a projection approach.~\cite{bhattacharya2009inferring} and~\cite{freyberger2015identification} provide asymptotic results on the value function of a linear program.
Ideas in~\cite{de_wolf_smeers2021generalized} might be used to approximate the gradient of the value function and use a delta method in the spirit of~\cite{shapiro1991asymptotic} and~\cite{fang2019infdirdiff}.
In the context of the model, it might be worthwhile thinking about heuristics for the choice of $\kappa_n$.
Slightly relaxing the tolerance $\frac{1}{n}$ but not as far as $\frac{1}{n^2}$, which was considered here, might provide conservative inference at the sake of a slightly slower convergence of the estimator.
Alternatively, it might be possible to make the convex relaxation approach work also for larger problems.
This would require a better construction for the relaxed feasible region and, more importantly so, guidance for the tuning parameter choice.
Lastly, inference could be considered for identification approaches using more information of the data, see~\cite{marx2024sharp} for ways to construct smaller identified sets, or models of multidimensional heterogeneity as considered in~\cite{dutz2021selection}.
\footnote{\cite{marx2024sharp} also provides a different characterization of the identified set, which might be useful for inference.}

\clearpage
\newpage


\bibliographystyle{abbrvnat_my_version}
\bibliography{refs.bib}

\clearpage
\newpage

% \printbibliography % use this for biblatex

\appendix
\counterwithin{figure}{section}

\section{Binary IV Model: Solutions}
\begin{figure}

  \caption{Identified Sets Binary-IV-Bernstein: Shape Restrictions (Complier LATE Only)}\label{app_fig:id_set_binary_iv_bernstein_shape_restrictions}

  \centering
  \begin{subfigure}[b]{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../../bld/figures/binary_iv/id_bernstein_2_monotone_response_late.png}
      \caption{Positive Treatment Response ($k=2$)}\label{app_fig:id_set_binary_iv_bernstein_k_2_monotone_response}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../../bld/figures/binary_iv/id_bernstein_11_monotone_response_late.png}
      \caption{Positive Treatment Response ($k=11$)}\label{app_fig:id_set_binary_iv_bernstein_k_11_monotone_response}
  \end{subfigure}

  \begin{subfigure}[b]{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../../bld/figures/binary_iv/id_bernstein_2_mte_monotone_late.png}
      \caption{Decreasing MTE ($k=2$)}\label{app_fig:id_set_binary_iv_bernstein_k_2_mte_monotone}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../../bld/figures/binary_iv/id_bernstein_11_mte_monotone_late.png}
      \caption{Decreasing MTE ($k=11$)}\label{app_fig:id_set_binary_iv_bernstein_k_11_mte_monotone}
  \end{subfigure}


\end{figure}

\begin{figure}

  \caption{Identified Sets for the Binary-IV-Bernstein: Shape Restrictions (Sharp Set)}\label{app_fig:id_set_binary_iv_bernstein_shape_restrictions_sharp}

  \centering
  \begin{subfigure}[b]{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../../bld/figures/binary_iv/id_bernstein_2_monotone_response_sharp.png}
      \caption{Positive Treatment Response ($k=2$)}\label{app_fig:id_set_binary_iv_bernstein_k_2_monotone_response_sharp}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../../bld/figures/binary_iv/id_bernstein_11_monotone_response_sharp.png}
      \caption{Positive Treatment Response ($k=11$)}\label{app_fig:id_set_binary_iv_bernstein_k_11_monotone_response_sharp}
  \end{subfigure}

  \begin{subfigure}[b]{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../../bld/figures/binary_iv/id_bernstein_2_mte_monotone_sharp.png}
      \caption{Decreasing MTE ($k=2$)}\label{app_fig:id_set_binary_iv_bernstein_k_2_mte_monotone_sharp}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../../bld/figures/binary_iv/id_bernstein_11_mte_monotone_sharp.png}
      \caption{Decreasing MTE ($k=11$)}\label{app_fig:id_set_binary_iv_bernstein_k_11_mte_monotone_sharp}
  \end{subfigure}
\end{figure}

\clearpage
\newpage

\section{Additional Simulation Results}
\begin{figure}

  \caption{Simulation Results for the Binary-IV Model with Bernstein Polynomial MTRs: Means}\label{app_fig:simulation_binary_iv_bernstein_means}

  \centering
  \begin{subfigure}[b]{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../../bld/figures/pyvmte_sims/1n/sims_binary_iv_sharp_none_means_bootstrap.png}
      \caption{No Shape Restrictions}\label{app_fig:simulation_means_bootstrap_none}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../../bld/figures/pyvmte_sims/1n/sims_binary_iv_sharp_none_means_problematic_region_bootstrap.png}
      \caption{Problematic Region}\label{app_fig:simulation_means_bootstrap_none_problematic}
  \end{subfigure}

  \begin{subfigure}[b]{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../../bld/figures/pyvmte_sims/1n/sims_binary_iv_sharp_monotone_response_means_bootstrap.png}
      \caption{Monotone Response}\label{app_fig:simulation_means_bootstrap_monotone_response}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../../bld/figures/pyvmte_sims/1n/sims_binary_iv_sharp_monotone_response_means_problematic_region_bootstrap.png}
      \caption{Problematic Region}\label{app_fig:simulation_means_bootstrap_monotone_response_problematic}
  \end{subfigure}

  \begin{subfigure}[b]{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../../bld/figures/pyvmte_sims/1n/sims_binary_iv_sharp_mte_monotone_means_bootstrap.png}
      \caption{MTE Decreasing}\label{app_fig:simulation_means_bootstrap_mte_monotone}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../../bld/figures/pyvmte_sims/1n/sims_binary_iv_sharp_mte_monotone_means_problematic_region_bootstrap.png}
      \caption{Problematic Region}\label{app_fig:simulation_means_bootstrap_mte_monotone_problematic}
  \end{subfigure}
\end{figure}

\begin{figure}

  \caption{Simulation Results for the Binary-IV Model with Bernstein Polynomial MTRs: Subsampling Share and LP Tolerance $\kappa_n$}\label{app_fig:sims_add_subsample_share_tolerance}

  \centering
  \begin{subfigure}[b]{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../../bld/figures/pyvmte_sims/plot_by_subsample_share/sims_binary_iv_sharp_none_coverage_problematic_region_by_subsample_share.png}
      \caption{Coverage: Subsampling Share}\label{app_fig:sims_add_subsample_share_coverage}
  \end{subfigure}
  \hfil
  \begin{subfigure}[b]{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../../bld/figures/pyvmte_sims/plot_by_subsample_share/sims_binary_iv_sharp_none_means_problematic_region_subsampling_by_subsample_share.png}
      \caption{Means: Subsampling Share}\label{app_fig:sims_add_subsample_share_means}
  \end{subfigure}

  \begin{subfigure}[b]{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../../bld/figures/pyvmte_sims/plot_by_tolerance/sims_binary_iv_sharp_none_coverage_problematic_region_by_tolerance.png}
      \caption{Coverage: LP Tolerance $\kappa_n$}\label{app_fig:sims_add_tolerance_coverage}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../../bld/figures/pyvmte_sims/plot_by_tolerance/sims_binary_iv_sharp_none_means_problematic_region_bootstrap_by_tolerance.png}
      \caption{Means: LP Tolerance $\kappa_n$}\label{app_fig:sims_add_tolerance_means}
  \end{subfigure}

\end{figure}

\begin{figure}

  \caption{Simulation Results for the Binary-IV Model with Bernstein Polynomial MTRs --- Imbens-Manski Critical Values}\label{app_fig:sims_add_alpha_im_crit}
  \centering
  \includegraphics[width=\textwidth]{../../bld/figures/pyvmte_sims/plot_by_im_alpha_crit/sims_binary_iv_sharp_none_coverage_problematic_region_by_alpha.png}

\end{figure}

\clearpage
\newpage

\section{Linear Programs}\label{app_sec:linear_programs}
This section presents the exact formulation of the linear programs corresponding to how most solvers specify linear programs.

\subsection{Identification}

The program for the upper bound is formulated as follows in~\cite{mogstad2018using}:
\begin{align}
  \overline{\beta}^* \equiv \sup_{(\theta_0, \theta_1)\in\Theta} \sum_{k=1}^{K_0}\theta_{0k}\Gamma^*_0(b_{0k}) + \sum_{k=1}^{K_1}\theta_{1k}\Gamma^*_1(b_{1k}) \\
  \text{subject to} \qquad \sum_{k=1}^{K_0}\theta_{0k}\Gamma_{0s}(b_{0k}) + \sum_{k=1}^{K_1}\theta_{1k}\Gamma_{1s}(b_{1k}) = \beta_s \text{ for } s \in \mathcal{S}.
\end{align}

In words: We maximize the target parameter, where we maximize over a basis function approximation with coefficients in a set $\Theta$.
The first summation term corresponds to $m_0$, the second to $m_1$.
We can separately parametrize $m_0$ and $m_1$ as reflected by the potentially different number of basis coefficients $K_0$ and $K_1$.
$\Theta$ can implicitly incorporate other constraints, like shape restrictions on $m_0, m_1$ which will be made explicit below.
It will at least entail the restrictions that $m_0, m_1$ are bounded by the support of the outcome $Y$, which we take to be $[0,1]$ without loss of generality.

Explicitly stated in the second line are the restrictions that for each point-identified estimand $s\in\mathcal{S}$ the maximizers $(\theta_0, \theta_1)$ must recover the point-identified parameter.

\paragraph{Matrix Form}
Next, I explicitly define the matrices typically used to describe a linear program.
In the so-called \textit{standard form}, all linear programs are described by:
(1) A \textit{maximization} objective; (2) inequality constraints with upper bounds; (3) non-negativity constraints on the choice variables.
Hence, we have:

\begin{align}
  \max_x c'x \\
  \text{ subject to } Ax\leq b, x\geq0
\end{align}

To keep the notation lighter, I instead reformulate the program in terms of:
(1) A \textit{minimization} objective; (2) inequality constraints with upper bounds; (3) equality constraints; (4) lower and upper bounds on the choice variables.
In this case, the program is:

\begin{align}
  \min_x c'x \\
  \text{ subject to } A_{ub}x \leq b_{ub}, A_{eq} = b_{eq}, l\leq x\leq u.
\end{align}

This is, for example, how the linear program is inputted into the \texttt{SciPy} \texttt{linprog} solver.
The program can easily be transformed to standard form by: (1) taking the negative of the objective, (2) reformulating equalities as two inequalities, and (3) expressing variables without positivity constraints as differences of two non-negative variables.
\texttt{SciPy}, for example, performs these operations in addition to other pre-solve checks for potential simplifications in the background.
It then passes the standard form program to a solver and post-processes the result to a solution of the original problem.

For simplicity, in the following we take $K_0 = K_1 = K$. This is the case for nonparametric exact bounds using constant splines and also parametric cases there seems little reason to have different numbers of basis polynomials.
Further, it simplifies the formulation of shape restrictions.

The choice variables are the basis function coefficients
\begin{equation*}
  x =
  \begin{bmatrix}
     \theta_{01} & \cdots & \theta_{0K} & \theta_{11} & \cdots & \theta_{1K}
  \end{bmatrix}',
\end{equation*}
and the coefficients are the individual contributions to the target parameter
\begin{equation*}
  c =
  \begin{bmatrix}
     \Gamma_0^*(b_{01}) & \cdots & \Gamma_0^*(b_{0K}) & \Gamma_1^*(b_{11}) & \cdots & \Gamma_1^*(b_{1K})
  \end{bmatrix}'.
\end{equation*}
Both of these are vectors of length $2K$.

Next, we have an equality constraint for each point-identified estimand $s\in\mathcal{S}$.
\begin{equation*}
  a_{eq}^s =
  \begin{bmatrix}
     \Gamma_{0s}(b_{01}) & \cdots & \Gamma_{0s}(b_{0K}) & \Gamma_{1s}(b_{11}) & \cdots & \Gamma_{1s}(b_{1K})
  \end{bmatrix}.
\end{equation*}
Note that we changed the map from $\Gamma^*_d$ to $\Gamma_{ds}$. Stacking these row-vectors gives the equality constraint matrix:
\begin{equation*}
  A_{eq}^s =
  \begin{bmatrix}
    a_{eq}^1 \\
    \vdots \\
    a_{eq}^\mathcal{S} \\
  \end{bmatrix},
\end{equation*}
which is a matrix of dimension $(S\times 2K)$, where, for brevity, we denote the \textit{number} of identified estimands $|\mathcal{S}| = S$.
The constraint values are given by the point-identified estimands:
\begin{equation*}
  b_{eq} =
  \begin{bmatrix}
    \beta_1, \ldots, \beta_\mathcal{S}
  \end{bmatrix}'.
\end{equation*}


A set of restrictions to achieve a minimal degree of identification is $y_l \leq m_d \leq y_d$ for $d=0,1$ and $y_l, y_u$ the lower and upper bound of the support of $Y$.
Taking $y_l = 0, y_u=1$, for typical shape constraints --- constant splines or Bernstein polynomials --- this amounts to the restrictions $0\leq \theta_{dj} \leq 1$ for $d=0,1, j=1,\ldots,K$.
Hence, we have the restrictions
\begin{equation*}
  l = \mathbf{0} \leq x \leq \mathbf{1} = u,
\end{equation*}
where $l$ and $u$ are vectors of length $2K$ each.

Finally, a number of shape restrictions may be added in the form of inequality bounds $A_{ub} \leq b_{ub}$.
The following applies to constant splines and Bernstein polynomials, but might not hold for all possible types of basis functions.

\paragraph{MTR Monotonicity}
To enforce a \textit{decreasing} MTR function $m_d(u)$ for constant splines, it is immediate that $\theta_{d1} \geq \theta_{d2} \geq \cdots \geq \theta_{dK}$, given that the functions are ordered over the partition of $u$, which we always assume to be the case.
For Bernstein polynomials this also holds, since they obey a monotonicity property: Whenever the basis coefficients $\theta_{dk}$ are decreasing over $k$, so is the resulting Bernstein polynomial (and the same for increasing).

Hence, to enforce a decreasing MTR function for $d=0$ we add the following constraints:
% \begin{equation*}
%   A_{eq}^s =
%   \begin{bmatrix}
%     -1 & 1 & 0 & \cdots & 0 & 0\\
%     0 & -1 & 1 & \cdots & 0 & 0\\
%     \vdots & & & \vdots \\
%     0 & 0 & 0 & \cdots & -1 & 1\\
%   \end{bmatrix} + \mathbf{0}
% \end{equation*}

\begin{equation*}
  A_{ub} =
  \begin{bNiceArray}{ccccccc}
    -1 & 1 & 0 & \cdots & 0 & 0 & \Block{4-1}<\Large>{\mathbf{0}}\\
    0 & -1 & 1 & \cdots & 0 & 0\\
    \vdots & & & \vdots \\
    0 & 0 & 0 & \cdots & -1 & 1\\
  \end{bNiceArray} \leq \mathbf{0} = b_{ub}.
\end{equation*}
Here, the first block is of dimension $(K-1) \times K$ reflecting the inequalities on the basis functions for $m_0(u)$.
The second block are zeros of dimension $(K-1) \times K$ on the basis functions for $m_1(u)$.
To add a similar restriction on $m_1(u)$ we can switch the blocks.
To enforce \textit{increasing} MTR functions $A_{ub}$ above can be multiplied by $-1$ to flip the inequalities.
$b_{ub}$ has length $(K-1)$.

\paragraph{MTE Monotonicity}
To enforce a \textit{decreasing} MTE function (the difference $m_1(u) - m_0(u)$) we require
$\theta_{11} - \theta_{01} \geq \theta_{12} - \theta_{02} \geq \cdots \geq \theta_{1K} - \theta_{0K}$.
For both constant splines and Bernstein polynomials this is immediate, since the difference $m_1(u) - m_0(u)$ implies a set of basis functions with the above differences as coefficients, which, hence, obey the same monotonicity conditions.

\begin{equation*}
  A_{ub} =
  \begin{bmatrix}
    \ldots
  \end{bmatrix} \leq \mathbf{0} = b_{ub}
\end{equation*}

\paragraph{Monotone Treatment Response}
Monotone treatment response requires $m_1(u) - m_0(u) \geq 0$ (positive) or $m_1(u) - m_0(u) \leq 0$ (negative) for all $u$.
Hence, we require $\theta_{1j} - \theta_{0j} \geq 0$ for all $j=1,\ldots,K$ for positive responses and reversed for negative.
\begin{equation*}
  A_{ub} =
  \begin{bmatrix}
    \ldots
  \end{bmatrix} \leq \mathbf{0} = b_{ub}
\end{equation*}

\subsection{Estimation}
Estimation is more involved and features solving two linear programs.
~\cite{mogstad2018using} propose a two-step estimator.
The reason is, that in sample we might not always expect all equality constraints for the identified estimands to be satisfied.
To allow for some sampling uncertainty, in a first step we compute the minimal achievable deviations from all point-identified estimands (as measured by the $l_1$ norm).
In the second step, we then restrict the set of allowed MTR functions to maximize over to those achieving this minimal deviation plus some tolerance.
In this way, the program always has a solution, meaning our estimators always exists.

This approach is stated in equation (27) of~\cite{mogstad2018using}:
\begin{align}
  & \hat{\overline{\beta}^*} \equiv \sup_{m\in \mathcal{M}}\hat{\Gamma}^*(m) \\
  & \text{ subject to } \sum_{s\in\mathcal{S}}|\hat{\Gamma}_s(m) - \hat{\beta}_s| \leq \inf_{m'\in \mathcal{M}} \sum_{s\in\mathcal{S}}|\hat{\Gamma}_s(m') - \hat{\beta}_s| + \kappa_n
\end{align}

Note we have a single constraint for the $l_1$ norm.
The $\inf$ on the right-hand side is the first-step linear program, and we allow for tolerance $\kappa_n$ to enlarge the set of possible solutions.

\subsubsection{First Step Linear Program}
The first step program is given by
\begin{equation*}
  \inf_{m'\in \mathcal{M}} \sum_{s\in\mathcal{S}}|\hat{\Gamma}_s(m') - \hat{\beta}_s|
\end{equation*}

We again rewrite it in the matrix form stated above.
Note that $\mathcal{M}$ again implicitly encodes any bounds and other shape restrictions on the MTR functions.
Further, as pointed out in~\cite{mogstad2018using}, we need to introduce dummy variables to mimic the absolute values in the objective.

\paragraph{Absolute Values in the Objective}
With $S = 1$, that is a single point-identified estimand, we have an objective of the following form:
\begin{equation*}
  \inf_m |X_1|,
\end{equation*}

where $X_1 = \hat{\Gamma}_1(m) - \hat{\beta}_1$.
We now add a dummy variable $X_1'$ and two constraints to the model:
\begin{align*}
  X_1 \leq X_1', \\
  -X_1 \leq X_1'.
\end{align*}
In the objective we replace the absolute value with the dummy:
\begin{equation*}
  \inf X_1'.
\end{equation*}
By going through all possible cases ($X_1=0, X_1<0, X_1>0$) it is easy to show,
that $X_1'$ will behave like $|X_1|$.

Having $|S|>1$ is then simply a matter of adding another dummy and two constraints per point-identified estimand.

Plugging in the original expression for $X_1$ we get
\begin{align*}
  \inf X_1' \\
  \text{ subject to }\\
  \hat{\Gamma}_1(m) - \hat{\beta}_1 \leq X_1', \\
  -(\hat{\Gamma}_1(m) - \hat{\beta}_1) \leq X_1',
\end{align*}
where minimization is now over the basis coefficients (appearing in the constraints) and the dummy variables (appearing in the objective).
Hence, we now have $2K + S$ choice variables.

The vector of choice variables is given by
\begin{equation*}
  x^{fs} =
  \begin{bmatrix}
     \theta_{01} & \cdots & \theta_{0K} & \theta_{11} & \cdots & \theta_{1K} & X_1' & \cdots & X_{S}
  \end{bmatrix}',
\end{equation*}
with corresponding coefficients

\begin{equation*}
  c^{fs} =
  \begin{bmatrix}
     \mathbf{0}', \mathbf{0}', \mathbf{1}'
  \end{bmatrix}',
\end{equation*}
where the zeros on the basis functions are of length $K$ each and the $1$ on the dummies of length $S$.
Both of these are vectors of length $2K + S$.


To encode the first type of inequality for all $S$ we get the following inequality constraint matrix:
\begin{equation*}
  A_{ub, 1}^{fs} =
  \begin{bmatrix}
    \hat{\Gamma}_{01}(b_{01}) & \cdots & \hat{\Gamma}_{01}(b_{0K}) & \hat{\Gamma}_{11}(b_{11}) & \cdots & \hat{\Gamma}_{11}(b_{1K}) & -1 & 0 & \cdots & 0 \\
    \vdots & & \vdots & \vdots &  & \vdots & \\
    \hat{\Gamma}_{0S}(b_{01}) & \cdots & \hat{\Gamma}_{0S}(b_{0K}) & \hat{\Gamma}_{1S}(b_{11}) & \cdots & \hat{\Gamma}_{1S}(b_{1K}) & 0 & 0 & \cdots & -1 \\
  \end{bmatrix}
\end{equation*}
which is of dimension $S\times (2K + S)$. The upper bounds are the estimated point-identified estimands:

\begin{equation*}
  b_{ub, 1}^{fs} =
  \begin{bmatrix}
    \hat{\beta}_1 & \cdots & \hat{\beta}_S
  \end{bmatrix}'.
\end{equation*}

To encode the second type of constraint we multiply the first $2K$ columns of $A_{ub, 1}^{fs}$ as well as the upper bounds by $-1$.

In the first step program, $c$ is a constant, while $A_{ub}^{fs}$ and $b_{ub}^{fs}$ need to be estimated from the data.

\subsubsection{Second Step Linear Program}
Denote the minimal deviation from the first step program, that is the value function, by $\mu$ (or $\hat{\mu}$, to emphasize the dependence on the data).

The choice variables in the second-step program are again only the basis function coefficients:
\begin{equation*}
  x^{ss} =
  \begin{bmatrix}
     \theta_{01} & \cdots & \theta_{0K} & \theta_{11} & \cdots & \theta_{1K} & X_1' & \cdots & X_S'
  \end{bmatrix}',
\end{equation*}
and the coefficients are the estimated individual contributions to the target parameter
\begin{equation*}
  c^{ss} =
  \begin{bmatrix}
     \hat{\Gamma}_0^*(b_{01}) & \cdots & \hat{\Gamma}_0^*(b_{0K}) & \hat{\Gamma}_1^*(b_{11}) & \cdots & \hat{\Gamma}_1^*(b_{1K}) & \mathbf{0}
  \end{bmatrix}'.
\end{equation*}
Both of these are vectors of length $2K + S$.

Note that the problem again features the $l_1$ norm, this time in the constraints.
Because we have a sum over multiple absolute values, we again introduce a dummy variable and two constraints that behaves like the absolute value for each point-identified estimand.
The only difference to the first-step constraint matrix is then the additional presence of the (relaxed) constraint on the $l_1$ norm:

\begin{equation*}
  a_{ub}^{ss, 1} = \begin{bmatrix}
    \mathbf{0} & \mathbf{1}
  \end{bmatrix}
  \leq \hat{\mu} + \kappa_n.
\end{equation*}
Here the first vector of zeros is of length $2K$ (the basis functions) and the vector of ones of length $S$ (the dummy variables).

The full second-step upper bound matrix is then given by
\begin{equation*}
  A_{ub}^{ss} = \begin{bmatrix}
    a_{ub}^{ss, 1} \\
    \mathbf{A_{ub,1}^{fs}} \\
    \mathbf{A_{ub,2}^{fs}} \\
  \end{bmatrix}
  \leq
  \begin{bmatrix}
    \hat{\mu} + \kappa_n \\
    \mathbf{\hat{\beta}} \\
    - \mathbf{\hat{\beta}} \\
  \end{bmatrix}
  = b_{ub}^{ss}.
\end{equation*}

The full program is then
\begin{align}
  & \min_x \hat{c}^{ss'}x^{ss} \text{ subject to }\\
  & \hat{A}_{ub}^{ss} \leq \hat{b}_{ub}^{ss} \\
  & \mathbf{0} \leq x^{ss} \leq \mathbf{1},
\end{align}
where $\hat{\cdot}$ is used to emphasize, which parts of the problem are estimated from the data.

\subsubsection{Shape Constraints}
Shape constraints need to be added to both the first and second step program.
All shape constraints are similar to the identification part, except for additional zero columns in $A_{ub}$ in the first step program due to the dummy variables.
In particular, all of these constraints are non-random.

\clearpage
\newpage

\section{Pyvmte Implementation}\label{app_sec:pyvmte}
All simulations for the more complicated MTE model settings are performed using an implementation of both the identification and estimation procedures in Python.
The implementation in the form of the \textit{pyvmte} module can be found under \url{https://github.com/buddejul/pyvmte}.
This implementation grew out of a programming class at the University of Bonn.
The only other existing implementation is provided by~\cite{shea2023ivmte} in the form of an R-package called \textit{ivmte}.
Writing an implementation was partly motivated by the fact, that \textit{ivmte} was too slow to perform bootstrap simulations.
\footnote{For typical datasets the ivmte simulation would take up to $100\times$ longer.
After profiling the core function calls, however, most of this came down to an inefficient way of computing the point-identified estimands.
Hence, this should in principle be possible to fix.}
Another motivation was the availability of many open-source optimizer implementations in the \textit{SciPy} ecosystem, which avoids the needs for licenses such as Gurobi or similar.
After all, the linear programs in this context seem easy enough to solve such that commercial software is not required.


\paragraph{Functionality}
The key task of the package is to translate user input into the matrices required for the identification and/or estimation linear programs (cf. Appendix Section~\ref{app_sec:linear_programs}).
The \textit{pyvmte} module has two core functionalities:
The \textit{identification} function derives solutions given a user-specified DGP corresponding to the identification part.
The \textit{estimation} function estimates bounds given a user-provided dataset. Inference using the bootstrap or subsampling is supported.
The functionality of both parts is tested against known results, both from~\cite{mogstad2018using} and the calculations presented in this paper.

\clearpage
\newpage

\section{Relaxation MTE Problem}

\begin{figure}

  \caption{Convex Relaxation: MTE Problem with Degree 11 Bernstein Polynomial}\label{app_fig:convex_relax_mte_bernstein_11}

  \centering
  \begin{subfigure}[b]{0.95\textwidth}
      \centering
        \includegraphics[width=\textwidth]{../figures/relax/relaxation_mte_k_bernstein_11_scipy_slsqp.png}
        \caption{Approximation by $k$}\label{app_fig:convex_relax_mte_by_k}
      \end{subfigure}

      \begin{subfigure}[b]{0.95\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../figures/relax/relaxation_mte_k_bernstein_11_k_20_by_algorithm.png}
        \caption{Algorithms ($k=20$)}\label{app_fig:convex_relax_solmte_by_algorithm}
      \end{subfigure}


\end{figure}

\clearpage
\newpage

\section*{Statement of authorship}

I hereby confirm that the work presented has been performed and interpreted solely by myself except for where I explicitly identified the contrary.
I assure that this work has not been presented in any other form for the fulfillment of any other degree or qualification.
Ideas taken from other works in letter and in spirit are identified in every single case.\\[3cm]

\begingroup
    \setlength{\parindent}{0pt}
    \begin{singlespace}
        Julian Budde\\
        Bonn, 4.11.2024
    \end{singlespace}
\endgroup

\end{document}
