\documentclass[11pt, aspectratio=169]{beamer}
% \documentclass[11pt,handout]{beamer}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{textcomp}
\usepackage{float, afterpage, rotating, graphicx}
\usepackage{epstopdf}
\usepackage{longtable, booktabs, tabularx}
\usepackage{fancyvrb, moreverb, relsize}
\usepackage{eurosym, calc}
\usepackage{amsmath, amssymb, amsfonts, amsthm, bm}
\usepackage[
    natbib=true,
    bibencoding=inputenc,
    bibstyle=authoryear-ibid,
    citestyle=authoryear-comp,
    maxcitenames=3,
    maxbibnames=10,
    useprefix=false,
    sortcites=true,
    backend=biber
]{biblatex}
\AtBeginDocument{\toggletrue{blx@useprefix}}
\AtBeginBibliography{\togglefalse{blx@useprefix}}
\setlength{\bibitemsep}{1.5ex}
\addbibresource{../refs.bib}

\hypersetup{colorlinks=true, linkcolor=black, anchorcolor=black, citecolor=black,
filecolor=black, menucolor=black, runcolor=black, urlcolor=black}

\setbeamertemplate{footline}[frame number]
\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{frametitle}{\centering\vspace{1ex}\insertframetitle\par}

\newcommand{\indep}{\perp\!\!\!\!\perp}

\AtBeginSection[]{
  \begin{frame}
  \vfill
  \centering
  \begin{beamercolorbox}[sep=8pt,center,shadow=true,rounded=true]{title}
    \usebeamerfont{title}\insertsectionhead\par%
  \end{beamercolorbox}
  \vfill
  \end{frame}
}

\title{Inference in Partially Identified Marginal Treatment Effect Models}

\author[Julian Budde]{\bf Julian Budde}

\begin{document}



\begin{frame}
    \titlepage
    \note{~}
\end{frame}

\begin{frame}
    \frametitle{Motivation}

    \begin{itemize}
        \item \citet{mogstad2018using} propose partial identification approach for \textit{extrapolating} information from instrumental variable estimates
        \item Focus is on identification and estimation
        \item Less on \textit{inference}:
        \begin{itemize}
            \footnotesize
            \item Working paper version has an inference procedure \dots
            \item \dots but no simulation results and no implementation.
            \item Authors' \textit{R package} supports bootstrap and subsampling \dots
            \item \dots but ``There does not currently exist a solution for the MTE framework that is both theoretically
            satisfactory and computationally tractable\dots [implemented methods] are known to not be valid in general''
        \end{itemize}
        \item
    \end{itemize}

\end{frame}

\begin{frame}
    \frametitle{This Project}

    \begin{itemize}
        \item Show the standard bootstrap is invalid in simple setting.
        \begin{itemize}
            \item Suggest some alternative approaches for these settings.
        \end{itemize}
        \item Simulation results for coverage probabilities in empirically relevant setting.
        \item Look at inference in linear programs more generally.
    \end{itemize}

\end{frame}

\begin{frame}
    \tableofcontents
\end{frame}

\section{Background}

\begin{frame}
    \frametitle{Marginal Treatment Effect Model: Notation}

    Program evaluation setting:
    \begin{itemize}
        \item Outcome $Y$
        \item Binary Treatment $D$
        \item Potential Outcomes $Y = D Y_{i1} + (1-D) Y_{i0}$
        \item Binary Instrument $Z$
    \end{itemize}

    \vspace{0.5cm}

    \textbf{Treatment Choice Equation}:
    \begin{equation}
        D = I\{p(Z) - U \geq 0\}
    \end{equation}
    where $p(z)$ is the propensity score and $U\sim Unif[0,1]$.

    \vspace{0.5cm}

    $U$ is ``resistance'' to treatment: Small $u$ $\rightarrow$ always take treatment.

\end{frame}

\begin{frame}
    \frametitle{MTE Model: Assumptions}
    $(Y,D,Z)$ are observable, $(Y_1, Y_0, U)$ unobservables.


    \begin{itemize}
        \item $U \indep Z$
        \item $E[Y_d|Z,U] = E[Y_d|U]$ and $E[Y_d^2] < \infty$ for $d \in \{0,1\}$
        \item $U$ is uniform on $[0,1]$ conditional on $Z$.
    \end{itemize}

\end{frame}

\begin{frame}
    \frametitle{MTR Functions}

    Key to the paper: Define everything in terms of (unobservable) MTR functions.

    For $d\in\{0,1\}$:
    \begin{equation}
        m_d(u) \equiv E[Y_d | U=u].
    \end{equation}

    \vspace{0.5cm}

    \textbf{Extrapolation}: Combine
    \begin{itemize}
        \item \textit{Information} on point-identified estimands, with
        \item \textit{Assumptions} on MTR functions.
    \end{itemize}

    \vspace{0.5cm}

    Use basis functions $\Rightarrow$ linear program to get identified set

\end{frame}

\begin{frame}

\end{frame}

\begin{frame}
    {\begin{align*}
        \beta_s &= E\left[\int_0^1 m_0(u)\omega_{0s}(u)du\right] + E\left[\int_0^1 m_1(u)\omega_{1s}(u)du\right] \\
        & \text{ where } w_{0s} \equiv s(0,z)I\{u>p(z)\} \\
        & \text{ where } w_{1s} \equiv s(1,z)I\{u\leq p(z)\} \\
    \end{align*}
    }

    \only<2>{\begin{align*}
        \beta_s &= \Gamma_s(m) \\
    \end{align*}
    }
\end{frame}

\begin{frame}
    \frametitle{Linear Program}

    \begin{align*}
        \overline{\beta^*} & = \sup_{(\theta_0, \theta_1) \in \Theta}\sum_{k=1}^{K_0} \theta_{0k}\Gamma^*_0(b_{0k}) + \sum_{k=1}^{K_1} \theta_{1k}\Gamma_1^*(b_{1k}) \\
        & s.t. \sum_{k=1}^{K_0} \theta_{0k}\Gamma_{0s}(b_{0k}) + \sum_{k=1}^{K_1} \theta_{1k} \Gamma_{1s}(b_{1k}) = \beta_s \quad \text{ for all } s\in\mathcal{S}.
    \end{align*}

\end{frame}

\section{Simple Example}

\begin{frame}
    \frametitle{Simple Example: Setup}

    \begin{itemize}
        \item Outcome $Y \in [0,1]$, $D$ and $Z$ binary
        \item $0 \leq p(0) < p(1) \leq 1$.
    \end{itemize}

    \vspace{0.5cm}

    \textbf{Point-Identified LATE}: $\beta_0^s = E[Y_1 - Y_0 | u \in (p(0), p(1)]]$.

    \vspace{0.5cm}

    \textbf{Target Parameter}: $\beta^* = E[Y_1 - Y_0 | u \in (p(0), p(1) + \overline{u}]]$.

    \vspace{0.5cm}

    Constant spline basis functions (no parametric assumptions).

\end{frame}

\begin{frame}
    \frametitle{Solution}

    Let $\beta_{\overline{u}} \equiv E[Y(1) - Y(0) | u \in [p(1), p(1) + \overline{u}]]$.

    \vspace{0.5cm}

    Then $\beta^*$ is a weighted average of the two LATEs:\@

    \begin{equation*}
        \beta^* = \omega\beta_s + (1-\omega)\beta_{\overline{u}}
    \end{equation*}

    where $\omega = \frac{p(1) - p(0)}{\overline{u} + p(1) - p(0)}$ is the relative complier share.

    \vspace{0.5cm}

    Finding the identified set then amounts to finding bounds on $\beta_{\overline{u}}$, where different restrictions imply different bounds.

\end{frame}

\begin{frame}
    \frametitle{Solutions: Constant Splines 1}

    \textbf{Nonparametric Bounds}:
    \begin{equation*}
        \beta^* \in [\omega\beta_s - (1 - \omega), \omega\beta_s + (1 - \omega)]
    \end{equation*}

    \vspace{0.5cm}

    \textbf{Decreasing Marginal Treatment Effect}:
    \begin{equation*}
        \beta^* \in [\omega\beta_s - (1 - \omega), \beta_s]
    \end{equation*}

    \vspace{0.5cm}

    \textbf{Positive Treatment Response}:
    \begin{equation*}
        \beta^* \in \begin{cases}
            [\omega\beta_s, \beta_s + (1 - \omega)] & \beta_s \geq 0 \\
            \emptyset & \beta_s < 0
        \end{cases}
    \end{equation*}

\end{frame}

\begin{frame}
    \frametitle{Solutions: Constant Splines II}

    \textbf{Increasing Marginal Treatment Response}:

    \begin{equation}\label{eq:solution_cs_increasing_mtr_upper}
        \overline{\beta^*}(\beta_s, \omega)=
        \begin{cases}
            \omega \beta_s + (1 - \omega),& \quad \text{if } \beta_s \geq 0\\
            \beta_s + (1 - \omega),              & \quad \text{if } \beta_s < 0,
        \end{cases}
    \end{equation}
    and
    \begin{equation}\label{eq:solution_cs_increasing_mtr_lower}
        \underline{\beta^*}(\beta_s, \omega)=
        \begin{cases}
            \beta_s - (1 - \omega),& \quad \text{if } \beta_s \geq 0\\
            \omega \beta_s - (1 - \omega),              & \quad \text{if } \beta_s < 0.
        \end{cases}
    \end{equation}

\end{frame}

\begin{frame}{Solutions: Bernstein Polynomials ($k=2$)}

    \begin{figure}
        \centering
        \includegraphics[height=0.825\textheight]{../../bld/figures/binary_iv/id_bernstein_2_none_late.png}
    \end{figure}


\end{frame}

\begin{frame}{Solutions: Bernstein Polynomials ($k=11$)}

    \begin{figure}
        \centering
        \includegraphics[height=0.825\textheight]{../../bld/figures/binary_iv/id_bernstein_11_none_late.png}
    \end{figure}


\end{frame}

\begin{frame}
    \frametitle{Inference}

    The standard \textit{bootstrap fails} at the kinks (e.g.~\citet{fang2019infdirdiff}).

    \vspace{0.5cm}

    \textbf{Alternatives}: Based on \textit{directional} differentiability of $\phi$ we can use adjusted delta methods.
    \begin{itemize}
        \item \textit{Analytical} delta method, e.g.~\cite{fang2019infdirdiff}.
        \item \textit{Numerical} delta method, e.g.~\cite{hong2018numerical}.
        \item Subsampling.
    \end{itemize}

\end{frame}

\begin{frame}
    \frametitle{Analytical delta bootstrap}
    \begin{itemize}
        \item[1.] Bootstrap approximation to distribution of $\sqrt{n}(\hat{\beta^s} - \beta_0^s)$.
        \item[2.] \pause Suitable estimator $\hat{\phi'_n}$ for directional derivative.
        \item[3.] \pause Delta method:
        \begin{equation*}
            \hat{\phi'_n}(\sqrt{n}(\hat{\beta^s}^*_n - \hat{\beta^s}_n))
        \end{equation*}
    \end{itemize}

    \vspace{0.5cm}
    \pause
    In our case: Simple pre-test
    \begin{equation*}
        \hat{\phi'_n}(h)=
        \begin{cases}
            \omega h ,& \quad \text{if } \sqrt{n}\hat{\beta^s}/\hat{\sigma^s} > \kappa_n\\
            \pause \max\{\omega h, -h\},& \quad \text{if } |\sqrt{n}\hat{\beta^s}/\hat{\sigma^s}| \leq \kappa_n\\
            \pause h,& \quad \text{if } \sqrt{n}\hat{\beta^s}/\hat{\sigma^s} < -\kappa_n\\
        \end{cases}
    \end{equation*}
    where we require $\kappa_n \to \infty$ but more slowly than $\sqrt{n}$, i.e. $\kappa_n / \sqrt{n} \to 0$.
\end{frame}

\begin{frame}
    \frametitle{Numerical delta bootstrap}

    Alternative: numerical approximation.
    \begin{equation*}
        \hat{\phi'}_{n, s_n} \equiv \frac{1}{s_n}\{\phi(\hat{\beta^s} + s_n h) - \phi(\hat{\beta^s})\}
    \end{equation*}

    Combining this with a bootstrap approximation to $\sqrt{n}(\hat{\beta^s} - \beta_0^s)$ we get
    \begin{equation*}
        \hat{\phi'}_{n, s_n}(\sqrt{n}(\hat{\beta^s} - \beta_0^s)) \equiv \frac{1}{s_n}\{\phi(\hat{\beta^s} + s_n \sqrt{n}(\hat{\beta^s} - \beta_0^s)) - \phi(\hat{\beta^s})\},
    \end{equation*}
    the distribution of which we can use to construct confidence intervals.
    We require $s_n\to0$ but $s_n\sqrt{n} \to \infty$.
\end{frame}

\begin{frame}
    \frametitle{Monte Carlo Simulations}

    \begin{itemize}
        \item True parameter $\beta^*$ at upper bound of identified set.
        \item Compare: Standard bootstrap, analytical and numerical delta method.
    \end{itemize}

    \vspace{0.5cm}

    To construct $1-\alpha = 0.95$ confidence interval for \textit{true parameter}:
    \begin{itemize}
        \item Use asymptotic approximations from above.
        \item Construct one-sided $\alpha$ intervals for the upper and lower bound \citep{imbens2004confidence}.
    \end{itemize}

    \vspace{0.5cm}

    DGP\@: $p(0) = 0.4 < 0.6 = p(1)$, $P(Z=0)=0.5$, $Y(d) = \gamma_d^g + \epsilon$ with $\epsilon\sim N(0, \sqrt{0.1})$.

    \vspace{0.5cm}
    $s_n = 1 / \sqrt{n}$, $\kappa_n = \sqrt{\ln(n)}$.

\end{frame}

\begin{frame}
    \frametitle{Simple Monte Carlo: Coverage}

    \begin{figure}
        \includegraphics[height=0.75\textheight]{../../bld/simple_model/figures/coverage.png}
    \end{figure}

\end{frame}

\begin{frame}
    \frametitle{Simple Monte Carlo: Length}

    \begin{figure}
        \includegraphics[height=0.75\textheight]{../../bld/simple_model/figures/length.png}
    \end{figure}

\end{frame}

\section{Simulation Studies}

\begin{frame}
    Goal: Simulation study closer to empirical application.

    \vspace{0.5cm}
    \pause

    \begin{itemize}
        \item Sharp identified set.
        \item Flexible polynomial for MTR functions.
        \item Shape restrictions.
    \end{itemize}

    \vspace{0.5cm}

    (Minor) difficulty: Construct DGP \textit{consistent} with assumptions and with \textit{parameter at boundary} of ID set.
\end{frame}

\begin{frame}
    \frametitle{DGP\@: Construction}


    \textbf{Parameters}: $\gamma_d^g = E[Y_d|g(U)=g]$ with $d\in\{0,1\}$ and $g = \{at, c, nt\}$.

    \vspace{0.5cm}

    \footnotesize{Cannot just take $\gamma_1^{nt} - \gamma_0^{nt} = -1$.}

    \vspace{0.5cm}

    \normalsize

    \textbf{Step 1}: Solve population problem with point-identified $\gamma_1^{at}, \gamma_1^c, \gamma_0^c, \gamma_0^{nt}$, Bernstein basis functions and all constraints.

    \vspace{0.5cm}

    \textbf{Step 2}: Use the population solutions $m_d(u;\gamma)$ for the lower bound, to construct a DGP implying parameter at lower bound.

    \vspace{0.5cm}

    \footnotesize{Polynomials that imply $\gamma_1^{nt}, \gamma_0^{nt}$.}

    \vspace{0.5cm}

    Simulation: $Y_i = D_i m_1(u;\gamma) + (1-D_i)m_0(u;\gamma) + \epsilon_i$.
\end{frame}

\begin{frame}
    \frametitle{DGP\@: Parameter Values}

    View as function of LATE\@: $\gamma_1^c = \beta_s/2 + 0.5$, $\gamma_0^c = -\beta_s/2 + 0.5$.

    \vspace{0.5cm}

    Set $\gamma_1^{at} = 0.75$ and $\gamma_0^{nt} = 0.2$.

    \vspace{0.5cm}

    Consistent with \textit{decreasing treatment effect} (MTE function decreasing) and a \textit{positive monotone treatment response} (MTE function non-negative).

    \vspace{0.5cm}

    Results for the \textit{non-parametric bootstrap} and \textit{subsampling} ($B = 0.1\times N$).

\end{frame}

\begin{frame}
    % \frametitle{Result: Consistency}

    \begin{figure}
        \includegraphics[height=0.95\textheight]{../../bld/figures/pyvmte_sims/1n/sims_binary_iv_sharp_mte_monotone_means_bootstrap.png}
    \end{figure}

\end{frame}

% \begin{frame}
%     % \frametitle{Result: Consistency}

%     \begin{figure}
%         \includegraphics[height=0.95\textheight]{../../bld/figures/pyvmte_sims/1n/sims_binary_iv_sharp_mte_monotone_means_problematic_region_bootstrap.png}
%     \end{figure}

% \end{frame}

\begin{frame}

    \begin{figure}
        \includegraphics[height=0.95\textheight]{../../bld/figures/pyvmte_sims/1n/sims_binary_iv_sharp_mte_monotone_coverage.png}
    \end{figure}

\end{frame}

\begin{frame}
    \frametitle{Simulation Results: Variations}
    Undercoverage is also observed for
    \begin{itemize}
        \item different subsample size ($s\in\{0.05, 0.1, 0.2\}$);
        \item critical values adjusted for the length of the identified set;
        \item different linear program tolerances ($\kappa_n \in \{\frac{1}{n^2}, \frac{1}{n}, \frac{1}{\sqrt{n}}\})$.\footnote{
            Not quite true.
        }
    \end{itemize}

\end{frame}

\begin{frame}
    \frametitle{Summary and Outlook}

    \textbf{Summary}: In relevant applications, confidence intervals might be severely undercovering.

    \vspace{0.5cm}

    \textbf{Caveats}: Estimator implementation, DGP\@.

\end{frame}

\begin{frame}
    \frametitle{Outlook: Other Approaches}
    Inference on values of linear programs:
    \begin{itemize}
        \item~\citet{bhattacharya2009inferring} and~\citet{freyberger2015identification} suggest bootstrap approach after pretest.
        \item~\citet{cho2024simple} propose \textit{perturbation} approach.
        \item~\citet{gafarov2024simple} proposes \textit{regularization} approach.
        \item Projection approaches.
    \end{itemize}
    $\rightarrow$ Crux: \textbf{Tuning parameter choice}; less so, computational complexity.

    \vspace{0.5cm}
    This project:
    \begin{itemize}
        \item Unless MTE problem has further special structure, theory seems difficult.
        \item Systematic simulation study/literature review.
    \end{itemize}

\end{frame}


\begin{frame}[allowframebreaks]
    \frametitle{References}
    \renewcommand{\bibfont}{\normalfont\footnotesize}
    \printbibliography
\end{frame}

\end{document}
