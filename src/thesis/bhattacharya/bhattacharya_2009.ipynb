{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bhattacharya 2009 JASA\n",
    "\n",
    "## Problem\n",
    "\n",
    "Bhattacharya (2009) studies inference on the value of a linear program.\n",
    "The *population problem* is given by\n",
    "\n",
    "$$\n",
    "\\max_p \\gamma' p \\text{ subject to } Ap = \\gamma, p \\geq 0.\n",
    "$$\n",
    "Here, we have $m$ decision variables collected in the vector $p$.\n",
    "\n",
    "The analogous *sample problem* is given by \n",
    "$$\n",
    "\\min_p \\hat{\\gamma}' p \\text{ subject to } Ap = \\gamma, p \\geq 0.\n",
    "$$\n",
    "The only difference is that $\\gamma$ is treated as estimated by an estimator $\\hat{\\gamma}$.\n",
    "Importantly, all other constraints are nonrandom. That is, the polyhedron $P$ over which we maximize is non-random. \n",
    "\n",
    "In linear programs, if there is a solution, it is attained at an extreme point of the constraint set $P$.\n",
    "Informally, extreme points of $P$ are points in $P$, which are not attainable as a convex combination of any two other distinct points. \n",
    "For example, if $P=[0,1]^2$, the extreme points are the set $\\{(0,0), (1,0), (0,1), (1,1)\\}$.\n",
    "\n",
    "These extreme points correspond to the basic solutions of a linear program. \n",
    "Further, if the program has a solution it is attained at one of the *feasible* basic solutions.\n",
    "\n",
    "Denote the set of *feasible* basic solutions by $S = \\{z_1, \\ldots, z_{|S|}\\}$.\n",
    "We have $|S| \\leq {m \\choose M}$, where $m$ are the number of decision variables and $M$ is the number of equality constraints.\n",
    "\n",
    "<small>\n",
    "The bound follows from the requirement that at every basic solution m-M linearly independent constraint need to be active (and the symmetry of the binomial coefficient).\n",
    "</small>\n",
    "\n",
    "Hence, if we know $S$ we can simply think of maximizing over the finite set $S$.\n",
    "\n",
    "## Distribution Theory\n",
    "\n",
    "The main result is given in propositions 3 and 4.\n",
    "\n",
    "We assume \n",
    "$$\n",
    "\\sqrt{n}(\\hat{\\gamma} - \\gamma) \\to_d w = O_p(1)\n",
    "$$\n",
    "and denote the set of optimal solutions as \n",
    "$$\n",
    "\\Theta_0 = \\{z| z \\in S, \\gamma'z = v\\}\n",
    "$$\n",
    "which is finite and has $J$ elements, so $\\Theta_0 = \\{z_1, \\ldots, z_J\\}$.\n",
    "\n",
    "\n",
    "**Proposition 3** Assume $\\sqrt{n}(\\hat{\\gamma}-\\gamma)\\to_d w$ and that elements of $\\hat{\\gamma}$ are bounded with probability 1. Then\n",
    "$$\n",
    "\\sqrt{n}(\\hat{v} - v) = \\max_{z\\in\\Theta_0}\\{w'z\\} + o_p(1) = \\max_{1\\leq j\\leq J}\\{w'z_j\\} + o_p(1).\n",
    "$$\n",
    "\n",
    "Note this distribution is not pivotal: it depends on an unknown parameter, namely the set of optimal solutions $\\Theta_0$.\n",
    "Proposition 4 then states that we can instead use an estimator of this set given by\n",
    "$$\n",
    "\\hat{\\Theta}_n = \\{z^* \\in S: \\hat{\\gamma}'z^* \\geq \\max_{z\\in S} \\hat{\\gamma}'z - c_n\\}\n",
    "$$\n",
    "So these are all basic solutions which attain a similar sample value up to some tolerance.\n",
    "$c_n > 0$ is a tuning parameter and needs to be chosen by the researcher. \n",
    "The asymptotic theory requires $\\sqrt{n}c_n \\to \\infty$ and $c_n \\to 0$.\n",
    "\n",
    "If there is only a single optimal solution, so $|\\Theta_0| = 1$, then the asymptotic distribution is given by $\\sqrt{n}(\\hat{\\gamma} - \\gamma)z_o$.\n",
    "Hence, this is a linear combination of normal random variables with weights $z_0$. Generally, $z_0$ is unknown but can be conistently estimated.\n",
    "\n",
    "<small> Check the latter requirement is actually needed for proposition 4 (and maybe 3). </small>\n",
    "\n",
    "## Similarity to pretesting\n",
    "\n",
    "The approach above works by allowing for a slightly larger set of optimal solutions.\n",
    "The non-standard behavior arises from multiple optimal solutions.\n",
    "We can think of $\\hat{\\Theta}_n$ in terms of a pre-test whether there is a unique solution:\n",
    "Given some tolerance $c_n>0$\n",
    "- if the difference between the two values is too small, we cannot be sure that $\\hat{z}$ is the unique optimal solution;\n",
    "- if the difference is large enough, we can be fairly certain, that $\\hat{z}$ is the optimal solution.\n",
    "\n",
    "### Example in m = 2 dimensions\n",
    "\n",
    "### Example in m > 2 dimensions\n",
    "\n",
    "## Procedure\n",
    "\n",
    "- Step 1: Estimate set of optimal solutions $\\hat{\\Theta}_n \\equiv \\{z^* \\in S: \\hat{\\gamma}'z^* \\geq \\max_{s\\in S} \\hat{\\gamma}'z - c_n\\}$\n",
    "- Step 2: For each of $B$ draws $w$ from $\\sqrt{n}(\\hat{\\gamma} - \\gamma)$:\n",
    "  - Calculate $v_i := \\max_{z\\in \\hat{\\Theta}_n}\\{w'z\\}$.\n",
    "- Step 3: Determine the quantiles of $\\{v_i\\}_{i=1}^R$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Illustrate Bhattacharya 2009 in a simple example.\"\"\"\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from scipy.stats import gaussian_kde, norm\n",
    "\n",
    "from thesis.bhattacharya.cdd_funcs import (\n",
    "    find_extreme_points,\n",
    "    find_extreme_points_box,\n",
    "    mat_box_constraint,\n",
    ")\n",
    "from thesis.config import BLD, RNG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Illustration of cdd for finding extreme points of a box constraint\n",
    "\n",
    "# Get the matrix for the box constraint\n",
    "mat_for_cdd = mat_box_constraint(2)\n",
    "\n",
    "# V-representation: [t V]\n",
    "# The library will always output V-representations in this form, i.e. so that all\n",
    "# components of the first column are zero or one, and so that\n",
    "# L does not contain rows whose first component is one.\n",
    "# 1 in the first column means it is an extreme point.\n",
    "find_extreme_points(mat_for_cdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In lower dimensional problem, finding extreme points is fast, but increases\n",
    "# exponentially with dimension.\n",
    "# For example, [0,1]^16 has 2^16=65536 extreme points.\n",
    "\n",
    "times = {}\n",
    "\n",
    "for dim in [2, 4, 8, 16]:\n",
    "    times[dim] = %timeit -o find_extreme_points_box(dim)\n",
    "\n",
    "times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Simulation\n",
    "\n",
    "We will now consider the B2009 method in the simplest possible linear program.\n",
    "\n",
    "The population problem is given by \n",
    "$$\n",
    "\\min_{x\\in[0,1]^2} c_1 x_1 + c_2 x_2\n",
    "$$\n",
    "\n",
    "We will fix $c_2 > 0$ for the analysis.\n",
    "Hence, the solution has a simple form: $x_2=0$ is optimal and hence the only relevant extreme points are $\\{(1,0), (0,0)\\}$.\n",
    "\n",
    "- When $c_1 > 0$, $(0,0)$ is the unique optimal solution.\n",
    "- When $c_1 = 0$, both $(0, 0)$ and $(1,0)$ are optimal solutions.\n",
    "- When $c_1 < 0$, $(1,0)$ is the unique optimal solution.\n",
    "\n",
    "The optimal value viewed as a function of $c_1$ has the following form:\n",
    "$$\n",
    "v(c_1) = 0 + I\\{c_1 < 0\\}\\times c_1 = \\min\\{0, c_1\\}\n",
    "$$\n",
    "We can see, that the point of multiple solutions $c_1 = 0$ corresponds to the kink in the value function.\n",
    "\n",
    "Hence, the four extreme points are the corners of the box.\n",
    "We will treat $c_1$ unknown and $c_2$ as known.\n",
    "However, we have $n$ draws from a random variable $y$ with $E[y] = c_1$ and finite variance $\\sigma^2$. Hence, we can estimate $c_1$ by $\\hat{c}_1 = \\frac{1}{n}\\sum_{i=1}^n y_i$.\n",
    "\n",
    "The sample problem is thus given by\n",
    "$$\n",
    "\\min_{x\\in[0,1]^2} \\hat{c}_1 x_1 + c_2 x_2.\n",
    "$$\n",
    "\n",
    "B2009 applies since the polyhedron $[0,1]^d$ is not stochastic and $\\sqrt{n}(\\hat{c}_1 - c_1) \\to_d N(0, \\sigma^2)$.\n",
    "\n",
    "By the B2009 results, the asymptotic distribution of $\\sqrt{n}(\\hat{v} - v)$ is given by\n",
    "\n",
    "- $0$, whenever $c_1 > 0$\n",
    "- $\\min\\{N(0, \\sigma^2), 0\\}$, when $c_1=0$\n",
    "- $N(0, \\sigma^2)$, when $c_1 < 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we illustrate the distribution of \\hat{v} for different sample sizes and c_1.\n",
    "\n",
    "\n",
    "def draw_data(\n",
    "    num_obs: int,\n",
    "    c_1: float,\n",
    "    sigma: float,\n",
    "    rng: np.random.Generator,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Draw num_obs observations from N(c_1, sigma^2).\"\"\"\n",
    "    return rng.normal(c_1, sigma, num_obs)\n",
    "\n",
    "\n",
    "def v_hat(data: np.ndarray) -> float:\n",
    "    \"\"\"Calculate the Bhattacharya 2009 estimator for a given sample.\"\"\"\n",
    "    mean = np.mean(data)\n",
    "    return mean * (mean < 0)\n",
    "\n",
    "\n",
    "def sim_distribution_v_hat(\n",
    "    num_reps: int,\n",
    "    num_obs: int,\n",
    "    c_1: float,\n",
    "    sigma: float,\n",
    "    rng: np.random.Generator,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Simulate finite sample distribution of v_hat.\"\"\"\n",
    "    out = np.empty(num_reps)\n",
    "\n",
    "    for i in range(num_reps):\n",
    "        data = draw_data(num_obs, c_1, sigma, rng)\n",
    "        out[i] = v_hat(data)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scaled_distr(\n",
    "    num_obs: int,\n",
    "    num_reps: int,\n",
    "    num_grid: int,\n",
    "    c_1: float,\n",
    "    sigma: float,\n",
    "    rng: np.random.Generator,\n",
    ") -> go.Figure:\n",
    "    \"\"\"Plot the scaled distribution of v_hat.\"\"\"\n",
    "    grid = np.linspace(-3, 3, num_grid)\n",
    "\n",
    "    distr = sim_distribution_v_hat(\n",
    "        num_reps=num_reps,\n",
    "        num_obs=num_obs,\n",
    "        c_1=c_1,\n",
    "        sigma=sigma,\n",
    "        rng=rng,\n",
    "    )\n",
    "\n",
    "    v = np.min(c_1, 0)\n",
    "\n",
    "    scaled_distr = np.sqrt(num_obs) * (distr - v)\n",
    "\n",
    "    try:\n",
    "        kde = gaussian_kde(scaled_distr).evaluate(grid)\n",
    "    except np.linalg.LinAlgError:\n",
    "        kde = np.zeros(num_grid)\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=grid,\n",
    "            y=kde,\n",
    "            mode=\"lines\",\n",
    "            name=\"Finite Sample (KDE)\",\n",
    "            line={\"dash\": \"solid\", \"color\": \"blue\"},\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Add N(0,sigma^2) distribution\n",
    "\n",
    "    sigma_asympt = sigma if c_1 <= 0 else 0\n",
    "\n",
    "    if c_1 < 0:\n",
    "        asym_distr = norm.pdf(grid, loc=0, scale=sigma_asympt)\n",
    "    if c_1 == 0:\n",
    "        # Minimum of norm.pdf(grid, loc=0, scale=sigma_asympt) and 0\n",
    "        asym_distr = np.where(grid <= 0, norm.pdf(grid, loc=0, scale=sigma_asympt), 0)\n",
    "        # Add zero to grid and set asym_distr to 0.5 at that point\n",
    "        grid = np.concatenate(([0], grid))\n",
    "        asym_distr = np.concatenate(([0.5], asym_distr))\n",
    "\n",
    "        idx = np.argsort(grid)\n",
    "        grid = grid[idx]\n",
    "        asym_distr = asym_distr[idx]\n",
    "    if c_1 > 0:\n",
    "        asym_distr = np.zeros(num_grid)\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=grid,\n",
    "            y=asym_distr,\n",
    "            mode=\"lines\",\n",
    "            name=\"Asymptotic Distribution\",\n",
    "            line={\"dash\": \"solid\", \"color\": \"red\"},\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Add mean(scaled_distr) and alpha/2 and 1-alpha/2 quantiles\n",
    "    alpha = 0.05\n",
    "\n",
    "    mean = np.mean(scaled_distr)\n",
    "    q1_fs = np.quantile(scaled_distr, alpha / 2)\n",
    "    q2_fs = np.quantile(scaled_distr, 1 - alpha / 2)\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[mean, mean],\n",
    "            y=[0, 0.5],\n",
    "            mode=\"lines\",\n",
    "            name=f\"FS: Mean = {mean:.2f}\",\n",
    "            line={\"dash\": \"dash\", \"color\": \"orange\"},\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[q1_fs, q1_fs],\n",
    "            y=[0, 0.5],\n",
    "            mode=\"lines\",\n",
    "            name=f\"FS: {alpha/2} quantile = {q1_fs:.2f}\",\n",
    "            line={\"dash\": \"dash\", \"color\": \"green\"},\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[q2_fs, q2_fs],\n",
    "            y=[0, 0.5],\n",
    "            mode=\"lines\",\n",
    "            name=f\"FS: {1-alpha/2} quantile = {q2_fs:.2f}\",\n",
    "            line={\"dash\": \"dash\", \"color\": \"green\"},\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Add quantiles of the asymptotic distribution\n",
    "    q1_asym = norm.ppf(alpha / 2, loc=0, scale=sigma_asympt) if c_1 <= 0 else 0\n",
    "\n",
    "    q2_asym = norm.ppf(1 - alpha / 2, loc=0, scale=sigma_asympt) if c_1 < 0 else 0\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[q1_asym, q1_asym],\n",
    "            y=[0, 0.5],\n",
    "            mode=\"lines\",\n",
    "            name=f\"Asym.: {alpha/2} quantile = {q1_asym:.2f}\",\n",
    "            line={\"dash\": \"dot\", \"color\": \"black\"},\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[q2_asym, q2_asym],\n",
    "            y=[0, 0.5],\n",
    "            mode=\"lines\",\n",
    "            name=f\"Asym.: {1-alpha/2} quantile = {q2_asym:.2f}\",\n",
    "            line={\"dash\": \"dot\", \"color\": \"black\"},\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f\"N = {num_obs}, c_1 = {c_1}, Sigma = {sigma}, Simulations = {num_reps}\",\n",
    "        xaxis_title=\"sqrt(n) * (v_hat - v)\",\n",
    "        yaxis_title=\"Density\",\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "plot = partial(plot_scaled_distr, num_reps=25_000, num_grid=1000, sigma=1, rng=RNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_1_for_plot = [-0.2, -0.05, -0.01, 0, 0.05]\n",
    "num_obs_for_plot = [100, 250, 1000, 10_000]\n",
    "\n",
    "res_plots = {\n",
    "    c_1: {num_obs: plot(num_obs=num_obs, c_1=c_1) for num_obs in num_obs_for_plot}\n",
    "    for c_1 in c_1_for_plot\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_plots = BLD / \"bhattacharya\" / \"plots\"\n",
    "\n",
    "for c_1, plot_dict in res_plots.items():\n",
    "    for num_obs, plot in plot_dict.items():\n",
    "        plot.write_image(path_to_plots / f\"dist_v_hat_{c_1}_num_obs_{num_obs}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bhatta_confidence_interval(\n",
    "    data: np.ndarray,\n",
    "    c_n: float,\n",
    "    n_reps: int,\n",
    "    alpha: float,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Calculate the Bhattacharya 2009 confidence interval for a given sample.\"\"\"\n",
    "    num_obs = len(data)\n",
    "\n",
    "    # Step 1: Construct set of optimal solutions up to some tolerance\n",
    "    gamma_1_hat = np.mean(data)\n",
    "    gamma_2_hat = 0.5\n",
    "    gamma = np.array([gamma_1_hat, gamma_2_hat])\n",
    "\n",
    "    basic_feasible_solutions = find_extreme_points_box(2)[:, 1:]\n",
    "\n",
    "    values = basic_feasible_solutions @ gamma.T\n",
    "\n",
    "    v_hat = np.min(values)\n",
    "\n",
    "    idx = np.where(v_hat + c_n >= values)[0]\n",
    "\n",
    "    num_solutions = len(idx)\n",
    "\n",
    "    estimated_optimal_solutions = basic_feasible_solutions[idx, :]\n",
    "\n",
    "    # Step 2: For each of B draws w from N(0, sigma^2_hat) calculate the minimum\n",
    "    # value over the estimated optimal solutions\n",
    "    sigma_hat = np.std(data)\n",
    "\n",
    "    asym_distr = np.empty(n_reps)\n",
    "\n",
    "    for i in range(n_reps):\n",
    "        w1 = RNG.normal(0, sigma_hat)\n",
    "        w2 = 0\n",
    "        w = np.array([w1, w2])\n",
    "\n",
    "        if len(idx) == 1:\n",
    "            asym_distr[i] = np.dot(estimated_optimal_solutions, w)[0]\n",
    "        else:\n",
    "            _values = estimated_optimal_solutions @ w\n",
    "\n",
    "            asym_distr[i] = np.min(_values)\n",
    "\n",
    "    z_hi = np.quantile(asym_distr, 1 - alpha / 2)\n",
    "    z_lo = np.quantile(asym_distr, alpha / 2)\n",
    "\n",
    "    ci_lo = v_hat - z_hi / np.sqrt(num_obs)\n",
    "    ci_hi = v_hat - z_lo / np.sqrt(num_obs)\n",
    "\n",
    "    return np.array([ci_lo, ci_hi, z_lo, z_hi, v_hat, num_solutions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a small simulation to see how to confidence intervals look on average\n",
    "# Choosing c_n large amounts to correctly choosing both solutions when c_1 = 0\n",
    "# Choosing c_n = 0 amounts to choosing (0, 0) whenever c_1_hat > 0 and (1, 0)\n",
    "# whenever c_1_hat < 0\n",
    "\n",
    "\n",
    "def sim_confidence_interval(\n",
    "    num_obs: int,\n",
    "    c_1: float,\n",
    "    c_n: float,\n",
    "    sigma: float,\n",
    "    num_sims: int,\n",
    "    alpha: float,\n",
    "    rng: np.random.Generator,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Simulate finite sample distribution of v_hat.\"\"\"\n",
    "    res = np.zeros((num_sims, 5))\n",
    "\n",
    "    for i in range(num_sims):\n",
    "        data = draw_data(num_obs, c_1, sigma, rng)\n",
    "        res[i, :] = bhatta_confidence_interval(\n",
    "            data=data,\n",
    "            c_n=c_n,\n",
    "            n_reps=1000,\n",
    "            alpha=alpha,\n",
    "        )\n",
    "\n",
    "    cols = [\"ci_lo\", \"ci_hi\", \"z_lo\", \"z_hi\", \"v_hat\"]\n",
    "\n",
    "    out = pd.DataFrame(res, columns=cols)\n",
    "\n",
    "    out[\"true\"] = np.min([c_1, 0])\n",
    "\n",
    "    out[\"covers_hi\"] = out[\"ci_hi\"] >= out[\"true\"]\n",
    "    out[\"covers_lo\"] = out[\"ci_lo\"] <= out[\"true\"]\n",
    "\n",
    "    out[\"covers\"] = out[\"covers_hi\"] & out[\"covers_lo\"]\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1_grid = np.sort(np.concatenate((np.linspace(-0.1, 0.1, 10), np.zeros(1))))\n",
    "\n",
    "sigma = 0.5\n",
    "\n",
    "alpha = 0.05\n",
    "\n",
    "num_obs = 1000\n",
    "\n",
    "num_sims = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_n_for_simulation = {\n",
    "    # \"paper_0.01\": 0.01 * np.log(num_obs) / np.sqrt(num_obs),\n",
    "    # \"paper_0.1\": 0.1 * np.log(num_obs) / np.sqrt(num_obs),\n",
    "    \"normal_alpha_over_1\": sigma * norm.ppf(1 - alpha) / (np.sqrt(num_obs)),\n",
    "    \"normal_alpha_over_2\": sigma * norm.ppf(1 - alpha / 2) / (np.sqrt(num_obs)),\n",
    "    \"normal_alpha_over_4\": sigma * norm.ppf(1 - alpha / 4) / (np.sqrt(num_obs)),\n",
    "}\n",
    "\n",
    "c_n_for_simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {\n",
    "    c_n_key: {\n",
    "        c_1: sim_confidence_interval(\n",
    "            num_obs=num_obs,\n",
    "            c_1=c_1,\n",
    "            c_n=c_n_val,\n",
    "            sigma=sigma,\n",
    "            num_sims=num_sims,\n",
    "            alpha=alpha,\n",
    "            rng=RNG,\n",
    "        )\n",
    "        for c_1 in c1_grid\n",
    "    }\n",
    "    for c_n_key, c_n_val in c_n_for_simulation.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate results in the dictionaries\n",
    "df_res = pd.concat(\n",
    "    [\n",
    "        pd.concat([res[c_n_key][c_1].assign(c_n=c_n_key, c_1=c_1) for c_1 in c1_grid])\n",
    "        for c_n_key in c_n_for_simulation\n",
    "    ],\n",
    ")\n",
    "\n",
    "df_res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_plot = df_res.groupby([\"c_1\", \"c_n\"]).mean()\n",
    "\n",
    "data_for_plot = data_for_plot.reset_index()\n",
    "\n",
    "c_n_to_color = {\n",
    "    \"inf\": \"blue\",\n",
    "    \"zero\": \"red\",\n",
    "    \"paper_0.01\": \"green\",\n",
    "    \"paper_0.1\": \"purple\",\n",
    "    \"paper_1\": \"orange\",\n",
    "    \"normal_alpha_over_1\": \"blue\",\n",
    "    \"normal_alpha_over_2\": \"red\",\n",
    "    \"normal_alpha_over_4\": \"green\",\n",
    "}\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for c_n in c_n_for_simulation:\n",
    "    data = data_for_plot.query(f\"c_n == '{c_n}'\")\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=data[\"c_1\"],\n",
    "            y=data[\"covers\"],\n",
    "            mode=\"lines\",\n",
    "            name=f\"c_n={c_n} ({c_n_for_simulation[c_n]:.3f})\",\n",
    "            line={\"color\": c_n_to_color[c_n]},\n",
    "        ),\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=(\n",
    "        f\"Coverage of Confidence Intervals (N = {num_obs}, Simulations = {num_sims},\"\n",
    "        f\" Nominal Coverage = {1 - alpha})\"\n",
    "    ),\n",
    "    xaxis_title=\"True Value\",\n",
    "    yaxis_title=\"Coverage\",\n",
    ")\n",
    "\n",
    "# Get x range of plo\n",
    "\n",
    "# Add a line at 0.95\n",
    "fig.add_shape(\n",
    "    type=\"line\",\n",
    "    x0=np.min(c1_grid),\n",
    "    y0=0.95,\n",
    "    x1=np.max(c1_grid),\n",
    "    y1=0.95,\n",
    "    line={\"color\": \"black\", \"width\": 1},\n",
    ")\n",
    "\n",
    "fig.write_image(path_to_plots / f\"coverage_{num_obs}.png\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Case 1: $c_n=\\infty$.**\n",
    "\n",
    "Clearly, in this case we have undercoverage whenever $c_1 \\leq 0$.\n",
    "When $c_n=\\infty$, our estimator of the set of optimal solutions includes all feasible basic solutions.\n",
    "That is, $\\hat{\\Theta}_n = S$.\n",
    "\n",
    "Consider some $c_1 < 0$. As $n\\to \\infty$, we have $\\hat{c_1} \\to_p c_1$.\n",
    "The correct asymptotic distribution would be $\\sqrt{n}(\\hat{v} - v) = \\sqrt{n}(\\min(\\hat{c}_1, 0) - v) \\sim N(0, \\sigma^2)$.\n",
    "Hence, the correct quantiles for a two-sided CI are $\\Phi^{-1}(\\alpha/2)$ and $\\Phi^{-1}(1-\\alpha/2)$.\n",
    "\n",
    "Now note that the plot showing the average CI bounds indicates, that undercoverage comes from the lower bound being too large.\n",
    "That is, $z_{1-\\alpha/2} < \\Phi^{-1}(1-\\alpha/2)$.\n",
    "\n",
    "The reason is simple: When we simulate, we simulate $\\min_{z\\in\\hat{\\Theta}_n}\\{w'z\\}$.\n",
    "But if $\\hat{\\Theta}_n = S$ it always contains $(0,0)$, which will be optimal, whenever $w > 0$. Remember $\\sqrt{n}(v-\\hat{v}) \\to_d w$.\n",
    "Since $w =_d N(0,\\sigma^2)$, we have $Pr(w > 0) = 0.5$ and hence our simulated distribution has a point-mass of 0.5 at 0.\n",
    "Hence, asymptotically, for any $\\alpha \\leq 0.5$ we have $z_{1-\\alpha/2} = 0 < \\Phi^{-1}(1-\\alpha/2)$, so the confidence interval is too short. \n",
    "\n",
    "This is not a problem in terms of coverage when $c_1 \\geq 0$. In this case, the optimal value is $0$, hence a confidence interval with lower bound $0$ covers the true parameter. \n",
    "\n",
    "<small> That is at least asymptotically true. I think in finite sample we might run into situations where close to the right of $0$ the confidence interval might not contain $0$ in finite samples. </small>\n",
    "\n",
    "**Case 2: $c_n=0$.**\n",
    "\n",
    "In this case we have correct coverage for $c_1 <<0$ and conservative coverage for $c_1 >> 0$.\n",
    "The latter is expected since the asymptotic distribution is degenerate with all mass at $0$.\n",
    "\n",
    "When $c_n=0$, we only consider the sample optimal solution, meaninig $\\hat{\\Theta}_n = \\hat{z}$.\n",
    "\n",
    "<small> While this should be unique with probability 1 in sample, we can define it to be unique by picking a random element. </small>\n",
    "\n",
    "Why is there undercoverage at 0? **No**, there is undercoverage for $c_1 = 0 - \\epsilon$!\n",
    "And the reason is simple: When $\\hat{c}_1 > 0$ is observed, the optimal sample solution is $(0,0)$.\n",
    "Hence the simulated quantiles will be 0 and the CI is given by the point $\\hat{v} = 0$ which leads to coverage of zero.\n",
    "Since $P(\\hat{c}_1) > 0$ in finite sample, we have undercoverage, mostly resulting from this case.\n",
    "\n",
    "Note this is not an issue at $c_1 = 0$! In this case $\\hat{v} = 0$ covers the true parameter $v=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison to pre-testing\n",
    "\n",
    "## Pre-test estimator\n",
    "Note that the approach in B2009 is equivalent to a pre-testing procedure.\n",
    "\n",
    "A pre-test would determine whether we are at $c_1=0$ and then choose the approximating distribution accordingly.\n",
    "In particular, we have for the asymptotic distribution $G$\n",
    "\n",
    "$$\n",
    "G(c_1) = \\begin{cases}\n",
    "    0 & \\text{if } c_1 > 0 \\\\ % & is your \"\\tab\"-like command (it's a tab alignment character)\n",
    "    \\min\\{0, Z\\} & \\text{if } c_1 = 0 \\\\\n",
    "    Z & \\text{if } c_1 < 0,\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where $Z\\sim N(0,\\sigma^2)$.\n",
    "\n",
    "Hence, an estimator of the asymptotic distribution incorporating a pre-test might look like\n",
    "\n",
    "$$\n",
    "\\hat{G}(c_1) = \\begin{cases}\n",
    "    0 & \\text{if } \\frac{\\sqrt{n}(\\hat{c}_1 - 0)}{\\hat{\\sigma}} > \\kappa_n \\\\ % & is your \"\\tab\"-like command (it's a tab alignment character)\n",
    "    \\min\\{0, Z\\} & \\text{if } |\\frac{\\sqrt{n}(\\hat{c}_1 - 0)}{\\hat{\\sigma}}| \\leq \\kappa_n \\\\\n",
    "    Z & \\text{if } \\frac{\\sqrt{n}(\\hat{c}_1 - 0)}{\\hat{\\sigma}} < -\\kappa_n.\n",
    "\\end{cases}\n",
    "$$\n",
    "Here, $\\kappa_n$ has to satisfy $\\kappa_n \\to \\infty$ but more slowly than $\\sqrt{n}$, so $\\frac{\\kappa_n}{\\sqrt{n}}\\to0$.\n",
    "\n",
    "We can for example choose $\\kappa_n = \\Phi(1 - \\frac{\\alpha}{2r_n})$ where $r_n\\to \\infty, \\frac{r_n}{\\sqrt{n}}\\to 0$.\n",
    "Then the type I error (wrongly rejecting $c_1=0$) is approximately $\\kappa_n = \\Phi(1 - \\frac{\\alpha}{2r_n})$ in a large sample.\n",
    "\n",
    "## B2009 estimator\n",
    "\n",
    "Now the estimator of B2009 is essentially a pre-test.\n",
    "Note that an event equivalent to $c_1=0$ is $|\\Theta_0| > 1$. That is, we have two optimal basic feasible basis solutions if and only if $c_1=0$ (given that $c_2 > 0$).\n",
    "More explicitly, we want to test $\\Theta_0 = \\{(1, 0), (0,0)\\}$.\n",
    "\n",
    "By using the preliminary estimator $\\hat{\\Theta}_n = \\{z \\in S: \\hat{\\gamma}'z \\leq \\hat{z} + c_n\\}$ we are conducting this pretest.\n",
    "We can work through the cases for type I errors, to motivate a tuning parameter choice similarly to the pre-test above.\n",
    "\n",
    "In particular, we can conduct a similar test by choosing\n",
    "$$\n",
    "c_n = \\frac{\\sigma \\Phi^{-1}(1 - \\frac{\\alpha}{2 r_n})}{\\sqrt{n}},\n",
    "$$\n",
    "\n",
    "with $r_n \\to \\infty$ and $\\frac{r_n}{\\sqrt{n}} \\to 0$. \n",
    "Note this satisfies $c_n \\to 0$ and $\\sqrt{n}c_n \\to \\infty$ as required by the theory in B2009.\n",
    "\n",
    "The pre-test has similar consequences:\n",
    "- If $\\hat{\\Theta}_n = \\{(0,0)\\}$, we use $0$ as the asymptotic approximation.\n",
    "- If $\\hat{\\Theta}_n = \\{(1,0)\\}$, we use $Z\\sim N(0, \\sigma^2)$ as the asymptotic approximation.\n",
    "- If $\\hat{\\Theta}_n = \\{(1,0), (0,0)\\}$, we use $\\min\\{Z, 0\\}$ as the asymptotic approximation.\n",
    "\n",
    "Issues again arise when we are smaller, but close to $c_1=0$.\n",
    "\n",
    "<small> All assuming we are actually correctly centering the interval. </small>\n",
    "\n",
    "- When $\\hat{\\Theta}_n = \\{(0,0)\\}$ we have zero coverage, since the CI collapses to a point. Closer to zero, this happens with probability $\\approx \\alpha/(2r_n)$.\n",
    "- When $\\hat{\\Theta}_n = \\{(1,0),(0,0)\\}$ we will also have below nominal level coverage. In particular, in this case the sample critical value for the lower CI bound is $\\hat{z}_{1-\\alpha/2} = 0$, since there is a point-mass at zero in the approximation. For $\\hat{z}_{alpha/2}$ we get a Normal critical value.\n",
    "- When $\\hat{\\Theta}_n = \\{(1,0), (0,0)\\}$ we will have correct coverage since \n",
    "\n",
    "Is the main problem **finite sample bias**?\n",
    "That is: If we are close to $c_1$ where the asymptotic distribution does not have mean zero, we will be *biased in finite samples*.\n",
    "This is essentially due to the point mass at zero that comes form being close to $(0,0)$ being optimal.\n",
    "\n",
    "But wouldn't this be taken into account by the construction of the confidence intervals?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undercoverage close to 0\n",
    "\n",
    "The key question is: Why is there undercoverage close to zero?\n",
    "\n",
    "Remember: For $v_1 < 0$ close to zero, the true distribution $\\sqrt{n}(\\hat{v} - v_1)$ will *not* have $z_{1-\\alpha/2} = 0$.\n",
    "$\\sqrt{n}(\\hat{v})$ will have a point-mass at zero in finite sample, but the centered distribution will have this point mass at $-\\sqrt{n}v_1 > 0$. Hence, using $\\hat{v} - 0$ for constructing the lower (one-sided) CI will lead to undercoverage. \n",
    "\n",
    "Evidence:\n",
    "- A one-sided *upper* confidence interval has nominally (close to) correct coverage.\n",
    "  - The critical value we use is $z_{\\alpha/2}$ and the interval is $(-\\infty, \\hat{v} - z_{\\alpha/2}]$\n",
    "  - Note that $z_{\\alpha/2} = \\Phi^{-1}(\\alpha/2)$ since we are in the left tail of the distribution which is normal.\n",
    "- A one-sided *lower* confidence interval has coverage (close to) 50%.\n",
    "  - The lower CI is given by $[\\hat{v} - z_{1-\\alpha/2}, \\infty)$. \n",
    "  - About half-the time, we include $(0,0)$ in $\\hat{\\Theta}_n$. In this case, $z_{1-\\alpha/2} = 0$ but the correct critical value (see above) is $-\\sqrt{n}v_1 > 0$. \n",
    "  - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's illustrate this discrepancy in the quantiles.\n",
    "# First, we plot the true finite sample distribution.\n",
    "\n",
    "plot_scaled_distr(\n",
    "    num_obs=10000,\n",
    "    num_reps=25_000,\n",
    "    num_grid=1_000,\n",
    "    c_1=-0.01,\n",
    "    sigma=1,\n",
    "    rng=RNG,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
